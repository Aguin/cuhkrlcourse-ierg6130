{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IERG 6130 Assignment 3: Policy Gradient\n",
    "\n",
    "*2019-2020 2nd term, IERG 6130: Reinforcement Learning and Beyond. Department of Information Engineering, The Chinese University of Hong Kong. Course Instructor: Professor ZHOU Bolei. Assignment author: PENG Zhenghao.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Student Name | Student ID |\n",
    "| :----: | :----: |\n",
    "| TYPE_YOUR_NAME_HERE | TYPE_YOUR_STUDENT_ID_HERE |\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welecome to the assignment 3 of our RL course. \n",
    "\n",
    "You need to go through this self-contained notebook, which contains many TODOs in part of the cells and has special `[TODO]` signs. You need to finish all TODOs. Some of them may be easy such as uncommenting a line, some of them may be difficult such as implementing a function. You can find them by searching the `[TODO]` symbol. However, we suggest you to go through the notebook step by step, which would give you a better understanding of the content.\n",
    "\n",
    "You are encouraged to add more code on extra cells at the end of the each section to investigate the problems you think interesting. At the end of the file, we left a place for you to optionaly write comments.\n",
    "\n",
    "Please report any code bugs to us via cuhkrlcourse@googlegroups.com or via github issue. Before you submission, remember to check the submit instruction at the directory of this assignment and make sure the required contents are included in your submission.\n",
    "\n",
    "Before you get start, remember to follow the instruction at https://github.com/cuhkrlcourse/ierg6130 to setup your environment.\n",
    "\n",
    "We will cover the following knowledege in this assignment:\n",
    "\n",
    "1. Basic policy gradient method\n",
    "2. Policy gradient with baseline\n",
    "3. Actor-critic framework\n",
    "\n",
    "You need to install some packages.\n",
    "1. Install `yaml` package via `pip install pyyaml` to print training information.\n",
    "2. Install `pandas` via `pip install pandas` to organize the learning progress.\n",
    "3. Install `matplotlib` via `pip install matplotlib` to visualize the learning result.\n",
    "4. Install `seaborn` via `pip install seaborn` to visualize the learning result (beautiful than matplotlib)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Section 1: Basic reinforcement learning pipeline\n",
    "\n",
    "(5 / 100 points)\n",
    "\n",
    "In this section, we will prepare several functions for evaulation, training RL algorithms. We will also build an `AbstractTrainer` class used as a general framework which left blanks for policy gradient methods.\n",
    "\n",
    "A essential difference of this assignment compared to the previous is that: the sampling and the optimization are splited into two phase. In each training iteration, the agent first collects a batch of samples followed by conducting the policy optimization. This modification allows us to have a clear view on the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    \"\"\"Run the trainer and report progress, agnostic to the class of trainer\n",
    "    :param trainer_cls: A trainer class \n",
    "    :param config: A dict\n",
    "    :param reward_threshold: the reward threshold to break the training\n",
    "    :return: The trained trainer and a dataframe containing learning progress\n",
    "    \"\"\"\n",
    "    assert inspect.isclass(trainer_cls)\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    trainer = trainer_cls(config)\n",
    "    config = trainer.config\n",
    "    start = now = time.time()\n",
    "    stats = []\n",
    "    print(\"=== {} {} Training Start ===\".format(trainer_cls.__name__, config[\"env_name\"]))\n",
    "    pretty_print({\"Config\": config})\n",
    "    for i in range(config['max_iteration'] + 1):\n",
    "        stat = trainer.train()\n",
    "        stats.append(stat or {})\n",
    "        if i % config['evaluate_interval'] == 0 or \\\n",
    "                i == config[\"max_iteration\"]:\n",
    "            reward = trainer.evaluate(config[\"evaluate_num_episodes\"])\n",
    "            stat[\"evaluate_reward\"] = reward\n",
    "            print(\"=== {} {} Iteration {} ===\".format(\n",
    "                trainer_cls.__name__, config[\"env_name\"], i)\n",
    "            )\n",
    "            pretty_print({\"Training Progress\": stat})\n",
    "            now = time.time()\n",
    "        if reward_threshold is not None and reward > reward_threshold:\n",
    "            print(\"In {} iteration, current mean episode reward {:.3f} is \"\n",
    "                  \"greater than reward threshold {}. Congratulation! Now we \"\n",
    "                  \"exit the training process.\".format(\n",
    "                i, reward, reward_threshold))\n",
    "            break\n",
    "    stats = pd.DataFrame(stats)\n",
    "    return trainer, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "default_config = dict(\n",
    "    env_name=\"CartPole-v0\",\n",
    "    \n",
    "    max_iteration=1000,\n",
    "    max_episode_length=1000,\n",
    "    \n",
    "    train_batch_size=1000,\n",
    "    gamma=0.99,\n",
    "    learning_rate=0.01,\n",
    "    seed=0,\n",
    "    \n",
    "    evaluate_interval=10,\n",
    "    evaluate_num_episodes=50\n",
    ")\n",
    "\n",
    "\n",
    "class AbstractTrainer:\n",
    "    \"\"\"This is the abstract class for value-based RL trainer. We will inherent\n",
    "    the specify algorithm's trainer from this abstract class, so that we can\n",
    "    reuse the codes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        self.config = check_and_merge_config(config or {}, default_config)\n",
    "\n",
    "        # Create the environment\n",
    "        self.env_name = self.config['env_name']\n",
    "        self.env = gym.make(self.env_name)\n",
    "        if \"ram\" in self.env_name:  # Special process for Atari games\n",
    "            self.env = wrap_deepmind_ram(self.env)\n",
    "\n",
    "        # Apply the random seed\n",
    "        self.seed = self.config[\"seed\"]\n",
    "        seed_everything(self.seed)\n",
    "        self.env.seed(self.seed)\n",
    "\n",
    "        assert isinstance(self.env.observation_space,\n",
    "                          gym.spaces.Box), self.env.observation_space\n",
    "        self.obs_dim = self.env.observation_space.shape[0]\n",
    "        self.discrete_obs = True\n",
    "\n",
    "        assert isinstance(self.env.action_space, gym.spaces.discrete.Discrete)\n",
    "        self.act_dim = self.env.action_space.n\n",
    "\n",
    "        self.iteration = 0\n",
    "        self.start_time = time.time()\n",
    "        self.iteration_time = self.start_time\n",
    "        self.total_timesteps = 0\n",
    "        self.total_episodes = 0\n",
    "\n",
    "        # build the model\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # ===== Helper functions =====\n",
    "    def evaluate(self, num_episodes=10, render=False):\n",
    "        \"\"\"Evaluate the agents for num_episodes episodes.\"\"\"\n",
    "        return evaluate_agent(self, self.env, num_episodes, render)\n",
    "\n",
    "    def to_tensor(self, array):\n",
    "        \"\"\"Preprocess the observation.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_action(self, observation):\n",
    "        \"\"\"Return the action, which can feed to self.env.step immediately.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_values(self, observation):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_log_probs(self, observation):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # ===== Training-related functions =====\n",
    "    def collect_samples(self):\n",
    "        \"\"\"Here we define the pipeline to collect sample even though\n",
    "        any specify functions are not implemented yet.\n",
    "        \"\"\"\n",
    "        iter_timesteps = 0\n",
    "        iter_episodes = 0\n",
    "        episode_lens = []\n",
    "        episode_rewards = []\n",
    "        episode_obs_list = []\n",
    "        episode_act_list = []\n",
    "        episode_reward_list = []\n",
    "        while iter_timesteps <= self.config[\"train_batch_size\"]:\n",
    "            obs_list, act_list, reward_list = [], [], []\n",
    "            obs = self.env.reset()\n",
    "            steps = 0\n",
    "            episode_reward = 0\n",
    "            while True:\n",
    "                act = self.compute_action(obs)\n",
    "                next_obs, reward, done, _ = self.env.step(act)\n",
    "\n",
    "                obs_list.append(obs)\n",
    "                act_list.append(act)\n",
    "                reward_list.append(reward)\n",
    "\n",
    "                obs = next_obs.copy()\n",
    "                steps += 1\n",
    "                episode_reward += reward\n",
    "                if done or steps > self.config[\"max_episode_length\"]:\n",
    "                    break\n",
    "            iter_timesteps += steps\n",
    "            iter_episodes += 1\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_lens.append(steps)\n",
    "            episode_obs_list.append(np.array(obs_list, dtype=np.float32))\n",
    "            episode_act_list.append(np.array(act_list, dtype=np.float32))\n",
    "            episode_reward_list.append(np.array(reward_list, dtype=np.float32))\n",
    "        \n",
    "        # [TODO] Uncomment everything below and understand the data structure:\n",
    "        # The return `samples` is a dict that contains several fields.\n",
    "        # Each field (key-value pair) contains a list.\n",
    "        # Each element in list is a list represent the data in one trajectory (episode).\n",
    "        # Each episode list contains the data of that field of all time steps in that episode.\n",
    "        # The return `sample_info` is a dict contains logging item name and its value.\n",
    "        samples = {\n",
    "            \"obs\": episode_obs_list,\n",
    "            \"act\": episode_act_list,\n",
    "            \"reward\": episode_reward_list\n",
    "        }\n",
    "        \n",
    "        sample_info = {\n",
    "            \"iter_timesteps\": iter_timesteps,\n",
    "            \"iter_episodes\": iter_episodes,\n",
    "            \"performance\": np.mean(episode_rewards),  # help drawing figures\n",
    "            \"training_episode_length\": {\n",
    "                \"mean\": float(np.mean(episode_lens)),\n",
    "                \"max\": float(np.max(episode_lens)),\n",
    "                \"min\": float(np.min(episode_lens))\n",
    "            },\n",
    "            \"training_episode_reward\": {\n",
    "                \"mean\": float(np.mean(episode_rewards)),\n",
    "                \"max\": float(np.max(episode_rewards)),\n",
    "                \"min\": float(np.min(episode_rewards))\n",
    "            }\n",
    "        }\n",
    "        return samples, sample_info\n",
    "\n",
    "    def process_samples(self, samples):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_model(self, processed_samples):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # ===== Training iteration =====\n",
    "    def train(self):\n",
    "        \"\"\"Here we defined the training pipeline using the abstract\n",
    "        functions.\"\"\"\n",
    "        info = dict(iteration=self.iteration)\n",
    "\n",
    "        # [TODO] Uncomment the following block and go through the learning \n",
    "        # pipeline.\n",
    "        # Collect samples\n",
    "        samples, sample_info = self.collect_samples()\n",
    "        info.update(sample_info)\n",
    "\n",
    "        # Process samples\n",
    "        processed_samples, processed_info = self.process_samples(samples)\n",
    "        info.update(processed_info)\n",
    "\n",
    "        # Update the model\n",
    "        update_info = self.update_model(processed_samples)\n",
    "        info.update(update_info)\n",
    "\n",
    "        now = time.time()\n",
    "        self.iteration += 1\n",
    "        self.total_timesteps += info[\"iter_timesteps\"]\n",
    "        self.total_episodes += info[\"iter_episodes\"]\n",
    "        info[\"iter_time\"] = now - self.iteration_time\n",
    "        info[\"total_time\"] = now - self.start_time\n",
    "        info[\"total_episodes\"] = self.total_episodes\n",
    "        info[\"total_timesteps\"] = self.total_timesteps\n",
    "        self.iteration_time = now\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "class TestAbstractTrainer(AbstractTrainer):\n",
    "    def build_model(self):\n",
    "        pass\n",
    "    def compute_action(self, _):\n",
    "        return np.random.choice(self.act_dim)\n",
    "test_trainer = TestAbstractTrainer()\n",
    "s, s_info = test_trainer.collect_samples()\n",
    "assert len(s[\"obs\"]) == s_info[\"iter_episodes\"]\n",
    "for i in range(len(s[\"obs\"])):\n",
    "    assert s[\"obs\"][i].shape[0] <= default_config[\"max_episode_length\"]\n",
    "    assert len(s[\"act\"][i]) == len(s[\"obs\"][i]) == len(s[\"reward\"][i])\n",
    "assert abs(s_info[\"training_episode_length\"][\"mean\"] * s_info[\"iter_episodes\"]\n",
    "           - s_info[\"iter_timesteps\"]) < 1\n",
    "print(\"Test Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an abstract trainer class here. We have abstracted the learning process into three phases: Collect samples, process samples and update the model. In the following sections, we will continue to fulfill the blank functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Basic policy gradient method\n",
    "\n",
    "(40 / 100 points)\n",
    "\n",
    "Unlike supervised learning, in RL setting the signal: reward and the output of neural network: the action distribution parameter (like the logits in discrete case or the mean in continous case) have no direct connection in terms of gradient. That is, you can't compute the gradient from the environment to your network output. \n",
    "\n",
    "To leverage the powerful tool of gradient descent, scientists innovated a technique called policy gradient, which is computed on top of the rewards and policy network's output. Scientists prove that: the policy gradient is the gradient of expected return w.r.t. the output. So that by applying gradient descent to gradient descent algorithm you can optimize your policy network and maximize the expected return (or minimize the negative of expected return).\n",
    "\n",
    "Concretely, if we consider the expected return, the value we want to maximize, is:\n",
    "\n",
    "$$Q = \\mathbb E_{\\text{possible trajectories}} \\sum_t r(a_t, s_t) = \\sum_{s_0, a_0,..} p(s_0, a_0, ..., s_t, a_t) r(s_0, a_0, ..., s_t, a_t) = \\sum_{\\tau} p(\\tau)r(\\tau)$$ \n",
    "\n",
    "wherein $\\sum_t r(a_t, s_t) = r(s_0, a_0, ..., s_t, a_t) = r(\\tau)$ is the trajectory return. We use $\\tau$ to represent a trajectory $s_0, a_0, ..., s_t, a_t$. Note that we remove the discount factor for simplicity.\n",
    "\n",
    "Since we want to maximize Q, we can simply compute the gradient from Q to parameter $\\theta$ (which is included in the action probability $p(a_t)$):\n",
    "\n",
    "$$\\nabla_\\theta Q = \\nabla_\\theta \\sum_{\\tau} p(\\tau)r(\\tau) = \\sum_{\\tau} r(\\tau) \\nabla_\\theta p(\\tau)$$\n",
    "\n",
    "Scientists use a famous trick to deal with the gradient of probability of trajectory: $\\nabla_\\theta p(\\tau) = p(\\tau)\\cfrac{\\nabla_\\theta p(\\tau)}{p(\\tau)} = p(\\tau)\\nabla_\\theta \\log p(\\tau)$. \n",
    "\n",
    "The reason for this trick is that the probability of a trajectory is the product of all related probabilities of states and actions. So introducing a log term can change the production to summation: $p_\\theta(\\tau) = p(s_0, a_0, ...) = p(s_0) \\prod_t \\pi_\\theta (a_t|s_t) p(s_{t+1}|s_t, a_t)$ Now we can expand the log of product above to sum of log:\n",
    "\n",
    "$$\\log p_\\theta (\\tau) = \\log p(s_1) + \\sum_t \\log \\pi_\\theta(a_t|s_t) + \\sum_t \\log p(s_{t+1}|s_t, a_t)$$\n",
    "\n",
    "You will find that the first and third term are not related to the parameter of policy $\\pi_\\theta(\\cdot)$. So when we back to $\\nabla_\\theta Q$, we find \n",
    "\n",
    "$$\\nabla_\\theta Q = \\sum_{\\tau} r(\\tau) \\nabla_\\theta p(\\tau) =  \\sum p_\\theta(\\tau) ( \\sum_t  \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) ) r(\\tau) d\\tau$$\n",
    "\n",
    "The probability of trajectory is included when we sampled sufficient data from environment (Yes, so REINFORCE is a Monte Carlo algorithm) so the final form of policy gradient is:\n",
    "\n",
    "$$\\nabla_\\theta Q =\\cfrac{1}{N}\\sum_{i=1}^N [( \\sum_t  \\nabla_\\theta \\log \\pi_\\theta(a_{i,t}|s_{i,t}) (\\sum_{t'=t} \\gamma^{t'-t} r(s_{i,t'}, a_{i,t'}) )]$$\n",
    "\n",
    "This algorithm is called REINFORCE algorithm, which is a Monte Carlo Policy Gradient algorithm having long history. In this section, we will implement the it using pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy network is composed by two parts: a basic MLP serve as function approximator which predict the probabilities of actions given the observation; and a distribution layer building upon the MLP to wrap the raw logits output of neural network to a distribution and provides API for action sampling and log probability retrieving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_units=128):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(obs_dim, hidden_units)\n",
    "        self.fc1 = nn.Linear(hidden_units, act_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.type_as(self.fc0.bias)\n",
    "        x = F.relu(self.fc0(x))\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CategoricalPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_units=128):\n",
    "        super(CategoricalPolicy, self).__init__()\n",
    "        self.network = Net(obs_dim, act_dim, hidden_units)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        logit = self.network(obs)\n",
    "        \n",
    "        # [TODO] Create an object of the class \"Categorical\" in torch \n",
    "        # package \"distributions\". Then sample an action from it.\n",
    "        m = torch.distributions.Categorical(logits=logit)\n",
    "        action = m.sample()\n",
    "        return action\n",
    "\n",
    "    def log_prob(self, obs, act):\n",
    "        logits = self.network(obs)\n",
    "        \n",
    "        # [TODO] Create an object of the class \"Categorical\" in torch \n",
    "        # package \"distributions\". Then collect the log probability of\n",
    "        # the action in this distribution.\n",
    "        m = torch.distributions.Categorical(logits=logits)\n",
    "        log_prob = m.log_prob(act)\n",
    "        return log_prob\n",
    "\n",
    "# Not that we do not implement GaussianPolicy here. So we can't\n",
    "# apply our algorithm to the environment with continous action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "pg_default_config = merge_config(dict(\n",
    "    normalize_advantage=True,\n",
    "    clip_gradient=10.0,\n",
    "    hidden_units=64,\n",
    "), default_config)\n",
    "\n",
    "\n",
    "class PGTrainer(AbstractTrainer):\n",
    "    def __init__(self, config=None):\n",
    "        config = check_and_merge_config(config or {}, pg_default_config)\n",
    "        super().__init__(config)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build the policy network and related optimizer\"\"\"\n",
    "        # Detect whether you have GPU or not. Remember to call X.to(self.device)\n",
    "        # if necessary.\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        \n",
    "        # [TODO] Build the policy network using CategoricalPolicy\n",
    "        # Hint: Remember to pass config[\"hidden_units\"], and set policy network\n",
    "        #  to the device you are using.\n",
    "        self.policy = CategoricalPolicy(self.obs_dim, self.act_dim, self.config[\"hidden_units\"]).to(self.device)\n",
    "        \n",
    "        # Build the Adam optimizer.\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.policy.parameters(),\n",
    "            lr=self.config[\"learning_rate\"]\n",
    "        )\n",
    "\n",
    "    def to_tensor(self, array):\n",
    "        \"\"\"Transform a numpy array to a pytorch tensor\"\"\"\n",
    "        return torch.from_numpy(array).to(self.device)\n",
    "\n",
    "    def to_array(self, tensor):\n",
    "        \"\"\"Transform a pytorch tensor to a numpy array\"\"\"\n",
    "        return tensor.cpu().detach().numpy()\n",
    "\n",
    "    def compute_action(self, observation):\n",
    "        \"\"\"Compute the action for single observation\"\"\"\n",
    "        assert observation.ndim == 1\n",
    "        # [TODO] Sample an action from action distribution given by the policy\n",
    "        # Hint: The input of policy network is a batch of data, so you need to\n",
    "        #  expand the first dimension of observation before feeding it to policy network.\n",
    "        obs = self.to_tensor(np.expand_dims(observation, 0))\n",
    "        action = self.to_array(self.policy(obs))[0]\n",
    "        return action\n",
    "\n",
    "    def compute_log_probs(self, observation, action):\n",
    "        \"\"\"Compute the log probabilities of a batch of state-action pair\"\"\"\n",
    "        # [TODO] Using the function of policy network to get log probs.\n",
    "        # Hint: Remember to transform the data into tensor before feeding it.\n",
    "        obs = self.to_tensor(observation)\n",
    "        act = self.to_tensor(action)\n",
    "        return self.policy.log_prob(obs, act)\n",
    "\n",
    "    def process_samples(self, samples):\n",
    "        \"\"\"Process samples and add advantages in it\"\"\"\n",
    "        values = []\n",
    "        for reward_list in samples[\"reward\"]:\n",
    "            # reward_list contains rewards in one episode\n",
    "            returns = np.zeros_like(reward_list, dtype=np.float32)\n",
    "            Q = 0\n",
    "            # [TODO] Scan the episode in a reverse order and compute the\n",
    "            # discounted return at each time step. Fill the array `returns`\n",
    "            # Hint: returns[n-1] = r_n-1\n",
    "            #       returns[n-2] = r_n-2 + gamma * r_n-1\n",
    "            #       ...\n",
    "            #       returns[0] = r_0 + gamma * r_1 + gamma^2 * r_2 + ...\n",
    "            #  wherein n is len(reward_list)\n",
    "            for i in range(len(reward_list) - 1, -1, -1):\n",
    "                Q = Q * self.config[\"gamma\"] + reward_list[i]\n",
    "                returns[i] = Q\n",
    "            values.append(returns)\n",
    "\n",
    "        # We call the values advantage here.\n",
    "        advantages = np.concatenate(values)\n",
    "\n",
    "        if self.config[\"normalize_advantage\"]:\n",
    "            # [TODO] normalize the advantage so that it's mean is\n",
    "            # almost 0 and the its standard deviation is almost 1.\n",
    "            advantages = (advantages - advantages.mean()) / max(advantages.std(), 1e-6) \n",
    "        \n",
    "        samples[\"advantages\"] = advantages\n",
    "        return samples, {}\n",
    "\n",
    "    def update_model(self, processed_samples):\n",
    "        \"\"\"A wrapper function, call self.update_policy\"\"\"\n",
    "        return self.update_policy(processed_samples)\n",
    "\n",
    "    def update_policy(self, processed_samples):\n",
    "        \"\"\"Update the policy network\"\"\"\n",
    "        advantages = self.to_tensor(processed_samples[\"advantages\"])\n",
    "        flat_obs = np.concatenate(processed_samples[\"obs\"])\n",
    "        flat_act = np.concatenate(processed_samples[\"act\"])\n",
    "\n",
    "        self.policy.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        log_probs = self.compute_log_probs(flat_obs, flat_act)\n",
    "\n",
    "        assert log_probs.shape == advantages.shape, \"log_probs shape {} is not \" \\\n",
    "            \"compatible with advantages {}\".format(log_probs.shape, advantages.shape)\n",
    "        \n",
    "        # [TODO] Compute the loss using log probabilities and advantages.\n",
    "        loss = -(log_probs * advantages).sum(dim=-1).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradient\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            self.policy.parameters(), self.config[\"clip_gradient\"]\n",
    "        )\n",
    "\n",
    "        self.optimizer.step()\n",
    "        self.policy.eval()\n",
    "        update_info = {\n",
    "            \"policy_loss\": loss.item(),\n",
    "            \"mean_log_prob\": torch.mean(log_probs).item(),\n",
    "            \"mean_advantage\": torch.mean(advantages).item()\n",
    "        }\n",
    "        return update_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Test advantage computing\n",
    "test_trainer = PGTrainer({\"normalize_advantage\": False})\n",
    "test_trainer.train()\n",
    "fake_sample = {\"reward\": [[2, 2, 2, 2, 2]]}\n",
    "assert_almost_equal(\n",
    "    test_trainer.process_samples(fake_sample)[0][\"reward\"][0],\n",
    "    fake_sample[\"reward\"][0]\n",
    ")\n",
    "assert_almost_equal(\n",
    "    test_trainer.process_samples(fake_sample)[0][\"advantages\"],\n",
    "    np.array([9.80199, 7.880798, 5.9402, 3.98, 2.], dtype=np.float32)\n",
    ")\n",
    "\n",
    "# Test advantage normalization\n",
    "test_trainer = PGTrainer(\n",
    "    {\"normalize_advantage\": True, \"env_name\": \"LunarLander-v2\"})\n",
    "test_adv = test_trainer.process_samples(fake_sample)[0][\"advantages\"]\n",
    "assert_almost_equal(test_adv.mean(), 0.0)\n",
    "assert_almost_equal(test_adv.std(), 1.0)\n",
    "\n",
    "# Test the shape of functions' returns\n",
    "fake_observation = np.array([\n",
    "    test_trainer.env.observation_space.sample() for i in range(10)\n",
    "])\n",
    "fake_action = np.array([\n",
    "    test_trainer.env.action_space.sample() for i in range(10)\n",
    "])\n",
    "assert test_trainer.to_tensor(fake_observation).shape == torch.Size([10, 8])\n",
    "assert np.array(test_trainer.compute_action(fake_observation[0])).shape == ()\n",
    "assert test_trainer.compute_log_probs(fake_observation, fake_action).shape == \\\n",
    "       torch.Size([10])\n",
    "\n",
    "print(\"Test Passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PGTrainer CartPole-v0 Training Start ===\n",
      "Config:\n",
      "  checked: true\n",
      "  clip_gradient: 10.0\n",
      "  env_name: CartPole-v0\n",
      "  evaluate_interval: 10\n",
      "  evaluate_num_episodes: 50\n",
      "  gamma: 0.99\n",
      "  hidden_units: 64\n",
      "  learning_rate: 0.01\n",
      "  max_episode_length: 200\n",
      "  max_iteration: 1000\n",
      "  normalize_advantage: false\n",
      "  seed: 0\n",
      "  train_batch_size: 200\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 0 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 23.18\n",
      "  iter_episodes: 10\n",
      "  iter_time: 0.09841632843017578\n",
      "  iter_timesteps: 217\n",
      "  iteration: 0\n",
      "  mean_advantage: 11.939602851867676\n",
      "  mean_log_prob: -0.6919979453086853\n",
      "  performance: 21.7\n",
      "  policy_loss: 1793.763427734375\n",
      "  total_episodes: 10\n",
      "  total_time: 0.09841632843017578\n",
      "  total_timesteps: 217\n",
      "  training_episode_length:\n",
      "    max: 32.0\n",
      "    mean: 21.7\n",
      "    min: 9.0\n",
      "  training_episode_reward:\n",
      "    max: 32.0\n",
      "    mean: 21.7\n",
      "    min: 9.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 10 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 58.24\n",
      "  iter_episodes: 3\n",
      "  iter_time: 0.09149575233459473\n",
      "  iter_timesteps: 246\n",
      "  iteration: 10\n",
      "  mean_advantage: 34.35287094116211\n",
      "  mean_log_prob: -0.6174343228340149\n",
      "  performance: 82.0\n",
      "  policy_loss: 5194.11279296875\n",
      "  total_episodes: 63\n",
      "  total_time: 1.395097017288208\n",
      "  total_timesteps: 2549\n",
      "  training_episode_length:\n",
      "    max: 123.0\n",
      "    mean: 82.0\n",
      "    min: 59.0\n",
      "  training_episode_reward:\n",
      "    max: 123.0\n",
      "    mean: 82.0\n",
      "    min: 59.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 20 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 100.46\n",
      "  iter_episodes: 3\n",
      "  iter_time: 0.09033656120300293\n",
      "  iter_timesteps: 245\n",
      "  iteration: 20\n",
      "  mean_advantage: 32.579036712646484\n",
      "  mean_log_prob: -0.6084086298942566\n",
      "  performance: 81.66666666666667\n",
      "  policy_loss: 4873.6044921875\n",
      "  total_episodes: 96\n",
      "  total_time: 3.350127935409546\n",
      "  total_timesteps: 5005\n",
      "  training_episode_length:\n",
      "    max: 94.0\n",
      "    mean: 81.66666666666667\n",
      "    min: 64.0\n",
      "  training_episode_reward:\n",
      "    max: 94.0\n",
      "    mean: 81.66666666666667\n",
      "    min: 64.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 30 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 138.96\n",
      "  iter_episodes: 3\n",
      "  iter_time: 0.09751296043395996\n",
      "  iter_timesteps: 272\n",
      "  iteration: 30\n",
      "  mean_advantage: 46.58760452270508\n",
      "  mean_log_prob: -0.603961169719696\n",
      "  performance: 90.66666666666667\n",
      "  policy_loss: 7558.8349609375\n",
      "  total_episodes: 120\n",
      "  total_time: 6.111550331115723\n",
      "  total_timesteps: 7715\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 90.66666666666667\n",
      "    min: 27.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 90.66666666666667\n",
      "    min: 27.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 40 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 175.12\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.13622546195983887\n",
      "  iter_timesteps: 371\n",
      "  iteration: 40\n",
      "  mean_advantage: 54.99089813232422\n",
      "  mean_log_prob: -0.5928560495376587\n",
      "  performance: 185.5\n",
      "  policy_loss: 12080.8759765625\n",
      "  total_episodes: 140\n",
      "  total_time: 9.784844875335693\n",
      "  total_timesteps: 10924\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 185.5\n",
      "    min: 171.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 185.5\n",
      "    min: 171.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 50 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 161.72\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.14862847328186035\n",
      "  iter_timesteps: 396\n",
      "  iteration: 50\n",
      "  mean_advantage: 56.83638000488281\n",
      "  mean_log_prob: -0.5828325748443604\n",
      "  performance: 198.0\n",
      "  policy_loss: 13075.9453125\n",
      "  total_episodes: 161\n",
      "  total_time: 14.153083086013794\n",
      "  total_timesteps: 14241\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 198.0\n",
      "    min: 196.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 198.0\n",
      "    min: 196.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 60 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 173.6\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.1451733112335205\n",
      "  iter_timesteps: 400\n",
      "  iteration: 60\n",
      "  mean_advantage: 57.131980895996094\n",
      "  mean_log_prob: -0.5676713585853577\n",
      "  performance: 200.0\n",
      "  policy_loss: 13028.9970703125\n",
      "  total_episodes: 182\n",
      "  total_time: 18.291285276412964\n",
      "  total_timesteps: 17720\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 200.0\n",
      "    min: 200.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 200.0\n",
      "    min: 200.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 70 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 195.84\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.14654827117919922\n",
      "  iter_timesteps: 400\n",
      "  iteration: 70\n",
      "  mean_advantage: 57.131980895996094\n",
      "  mean_log_prob: -0.5589467287063599\n",
      "  performance: 200.0\n",
      "  policy_loss: 12739.4853515625\n",
      "  total_episodes: 202\n",
      "  total_time: 22.715216636657715\n",
      "  total_timesteps: 21213\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 200.0\n",
      "    min: 200.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 200.0\n",
      "    min: 200.0\n",
      "\n",
      "In 70 iteration, current mean episode reward 195.840 is greater than reward threshold 195.0. Congratulation! Now we exit the training process.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pg_trainer_no_na, pg_result_no_na = run(PGTrainer, dict(\n",
    "    learning_rate=0.01,\n",
    "    max_episode_length=200,\n",
    "    train_batch_size=200,\n",
    "    normalize_advantage=False,  # <<== Here!\n",
    "), 195.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PGTrainer CartPole-v0 Training Start ===\n",
      "Config:\n",
      "  checked: true\n",
      "  clip_gradient: 10.0\n",
      "  env_name: CartPole-v0\n",
      "  evaluate_interval: 10\n",
      "  evaluate_num_episodes: 50\n",
      "  gamma: 0.99\n",
      "  hidden_units: 64\n",
      "  learning_rate: 0.01\n",
      "  max_episode_length: 200\n",
      "  max_iteration: 1000\n",
      "  normalize_advantage: true\n",
      "  seed: 0\n",
      "  train_batch_size: 200\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 0 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 27.24\n",
      "  iter_episodes: 10\n",
      "  iter_time: 0.08732461929321289\n",
      "  iter_timesteps: 217\n",
      "  iteration: 0\n",
      "  mean_advantage: -1.5381843354589364e-08\n",
      "  mean_log_prob: -0.6919979453086853\n",
      "  performance: 21.7\n",
      "  policy_loss: 0.11937570571899414\n",
      "  total_episodes: 10\n",
      "  total_time: 0.08732461929321289\n",
      "  total_timesteps: 217\n",
      "  training_episode_length:\n",
      "    max: 32.0\n",
      "    mean: 21.7\n",
      "    min: 9.0\n",
      "  training_episode_reward:\n",
      "    max: 32.0\n",
      "    mean: 21.7\n",
      "    min: 9.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 10 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 67.16\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.09068632125854492\n",
      "  iter_timesteps: 248\n",
      "  iteration: 10\n",
      "  mean_advantage: -1.509343405814434e-07\n",
      "  mean_log_prob: -0.6186385750770569\n",
      "  performance: 124.0\n",
      "  policy_loss: -0.666816771030426\n",
      "  total_episodes: 67\n",
      "  total_time: 1.4770653247833252\n",
      "  total_timesteps: 2526\n",
      "  training_episode_length:\n",
      "    max: 169.0\n",
      "    mean: 124.0\n",
      "    min: 79.0\n",
      "  training_episode_reward:\n",
      "    max: 169.0\n",
      "    mean: 124.0\n",
      "    min: 79.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 20 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 123.32\n",
      "  iter_episodes: 3\n",
      "  iter_time: 0.09261488914489746\n",
      "  iter_timesteps: 244\n",
      "  iteration: 20\n",
      "  mean_advantage: 1.9395938011257385e-07\n",
      "  mean_log_prob: -0.6076608300209045\n",
      "  performance: 81.33333333333333\n",
      "  policy_loss: 2.991849422454834\n",
      "  total_episodes: 100\n",
      "  total_time: 3.6299726963043213\n",
      "  total_timesteps: 5084\n",
      "  training_episode_length:\n",
      "    max: 115.0\n",
      "    mean: 81.33333333333333\n",
      "    min: 64.0\n",
      "  training_episode_reward:\n",
      "    max: 115.0\n",
      "    mean: 81.33333333333333\n",
      "    min: 64.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 30 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 176.64\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.1456165313720703\n",
      "  iter_timesteps: 385\n",
      "  iteration: 30\n",
      "  mean_advantage: 1.7958801379336364e-07\n",
      "  mean_log_prob: -0.5720318555831909\n",
      "  performance: 192.5\n",
      "  policy_loss: -8.292258262634277\n",
      "  total_episodes: 123\n",
      "  total_time: 6.927541017532349\n",
      "  total_timesteps: 7925\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 192.5\n",
      "    min: 185.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 192.5\n",
      "    min: 185.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 40 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 194.28\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.1369764804840088\n",
      "  iter_timesteps: 386\n",
      "  iteration: 40\n",
      "  mean_advantage: 1.5935749786422093e-07\n",
      "  mean_log_prob: -0.5526421666145325\n",
      "  performance: 193.0\n",
      "  policy_loss: -3.0765557289123535\n",
      "  total_episodes: 143\n",
      "  total_time: 11.427533626556396\n",
      "  total_timesteps: 11716\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 193.0\n",
      "    min: 186.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 193.0\n",
      "    min: 186.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 50 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 194.46\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.15274667739868164\n",
      "  iter_timesteps: 400\n",
      "  iteration: 50\n",
      "  mean_advantage: 7.212162245195941e-08\n",
      "  mean_log_prob: -0.5184469819068909\n",
      "  performance: 200.0\n",
      "  policy_loss: -0.6939629316329956\n",
      "  total_episodes: 163\n",
      "  total_time: 16.28590989112854\n",
      "  total_timesteps: 15541\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 200.0\n",
      "    min: 200.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 200.0\n",
      "    min: 200.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 60 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 179.88\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.12105727195739746\n",
      "  iter_timesteps: 328\n",
      "  iteration: 60\n",
      "  mean_advantage: -2.4714120527846717e-08\n",
      "  mean_log_prob: -0.5553181171417236\n",
      "  performance: 164.0\n",
      "  policy_loss: 6.247684478759766\n",
      "  total_episodes: 183\n",
      "  total_time: 21.150541305541992\n",
      "  total_timesteps: 19422\n",
      "  training_episode_length:\n",
      "    max: 177.0\n",
      "    mean: 164.0\n",
      "    min: 151.0\n",
      "  training_episode_reward:\n",
      "    max: 177.0\n",
      "    mean: 164.0\n",
      "    min: 151.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 70 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 141.16\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.09981584548950195\n",
      "  iter_timesteps: 268\n",
      "  iteration: 70\n",
      "  mean_advantage: 1.8237241761198675e-07\n",
      "  mean_log_prob: -0.5572788119316101\n",
      "  performance: 134.0\n",
      "  policy_loss: 0.5613934993743896\n",
      "  total_episodes: 203\n",
      "  total_time: 25.51922631263733\n",
      "  total_timesteps: 22759\n",
      "  training_episode_length:\n",
      "    max: 148.0\n",
      "    mean: 134.0\n",
      "    min: 120.0\n",
      "  training_episode_reward:\n",
      "    max: 148.0\n",
      "    mean: 134.0\n",
      "    min: 120.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 80 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 161.64\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.13757848739624023\n",
      "  iter_timesteps: 365\n",
      "  iteration: 80\n",
      "  mean_advantage: 7.05457736671633e-08\n",
      "  mean_log_prob: -0.5335915684700012\n",
      "  performance: 182.5\n",
      "  policy_loss: 4.533784866333008\n",
      "  total_episodes: 223\n",
      "  total_time: 29.15142583847046\n",
      "  total_timesteps: 25790\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 182.5\n",
      "    min: 165.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 182.5\n",
      "    min: 165.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 90 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 199.06\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.14116978645324707\n",
      "  iter_timesteps: 391\n",
      "  iteration: 90\n",
      "  mean_advantage: 9.573329151635335e-08\n",
      "  mean_log_prob: -0.5454728007316589\n",
      "  performance: 195.5\n",
      "  policy_loss: 0.5286332368850708\n",
      "  total_episodes: 243\n",
      "  total_time: 33.438331842422485\n",
      "  total_timesteps: 29472\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 195.5\n",
      "    min: 191.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 195.5\n",
      "    min: 191.0\n",
      "\n",
      "In 90 iteration, current mean episode reward 199.060 is greater than reward threshold 195.0. Congratulation! Now we exit the training process.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pg_trainer_na, pg_result_na = run(PGTrainer, dict(\n",
    "    learning_rate=0.01,\n",
    "    max_episode_length=200,\n",
    "    train_batch_size=200,\n",
    "    normalize_advantage=True,  # <<== Here!\n",
    "), 195.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Policy Gradient')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pg_result_no_na[\"normalize_advantage\"] = False\n",
    "pg_result_na[\"normalize_advantage\"] = True\n",
    "pg_result = pd.concat([pg_result_no_na, pg_result_na])\n",
    "ax = sns.lineplot(\n",
    "    x=\"total_timesteps\", \n",
    "    y=\"performance\", \n",
    "    data=pg_result, hue=\"normalize_advantage\",\n",
    ")\n",
    "ax.set_title(\"Policy Gradient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Policy gradient with baseline\n",
    "\n",
    "(25 / 100 points)\n",
    "\n",
    "Recall the REINFORCE algorithm above. We compute the gradient of $Q = \\mathop{\\mathbb E} \\sum_t r(a_t, s_t)$ w.r.t. the parameter to update the policy. A natural question is that, when you take an inferior action that lead to positive expected return, the policy gradient is also positive and you will update your network toward this action. At the same time a potential better action is ignored.\n",
    "\n",
    "To tackle this problem, we introduce \"baseline\" when computing the policy gradient. The insight behind this is that we want to optimize the policy toward an action that are better than the average performance when taking all action into consideration at a given state.\n",
    "\n",
    "We introduce $b_{t} = \\mathbb E_{a_t} \\sum_{t'}{\\gamma^{t'-t} r(s_{t'}, a_{t'})}$ as the baseline. It average all possible actions at state $s_t$. So that the profit achieved by action $a_t$ can be evaluated via $\\sum_{t'=t} \\gamma^{t' -t}r(a_{t'}, s_{t'}) - b_t$\n",
    "\n",
    "Therefore, the policy gradient becomes:\n",
    "\n",
    "$$\\nabla_\\theta Q =\\cfrac{1}{N}\\sum_{i=1}^N [( \\sum_t  \\nabla_\\theta \\log \\pi_\\theta(a_{i,t}|s_{i,t}) (\\sum_{t'} \\gamma^{t'-t} r(s_{i,{t’}}, a_{i,t‘}) - b_{i, t})]$$\n",
    "\n",
    "In our implementation, we estimate the baseline via an extra network `self.baseline`, which has same structure of policy network but output only a scalar value. We use the output of this network to serve as the baseline, while this network is updated by fitting the true value of expected return of current state: $\\mathbb E_{a_t} \\sum_{t'}{\\gamma^{t'-t} r(s_{t'}, a_{t'})}$\n",
    "\n",
    "In implementation, we use a trick to match the distribution of baseline and values. During training, we first collect a batch of target values: $\\{t_i= \\mathbb E_{a_t} \\sum_{t'}{\\gamma^{t'-t} r(s_{t'}, a_{t'})}\\}_i$. Then we normalzie all targets to standard distribution with mean = 0 and std = 1. Then we ask the baseline network to fit such normalized targets.\n",
    "\n",
    "When computing the advantages, instead of using the output of baseline network as the baseline $b$, we firstly match the baseline distribution with action values. The transformed baselines $b' = f(b)$ should has the same means of action values and same STD of action values too. Then we compute the advantage of current action: $adv_{i,t} = \\sum_{t'} \\gamma^{t'-t} r(s_{i,{t'}}, a_{i,t'}) - b'_{i, t}$\n",
    "\n",
    "By doing this, we mitigate the instability of training baseline.\n",
    "\n",
    "Hint: We suggest to normalize an array via: `(x - x.mean()) / max(x.std(), 1e-6)`. The max term can avoid potential risk of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_with_baseline_default_config = merge_config(dict(), pg_default_config)\n",
    "\n",
    "class PolicyGradientWithBaselineTrainer(PGTrainer):\n",
    "    def __init__(self, config=None):\n",
    "        config = check_and_merge_config(config or {}, pg_with_baseline_default_config)\n",
    "        super().__init__(config)\n",
    "\n",
    "    def build_model(self):\n",
    "        # Build the actor in name of self.policy\n",
    "        super().build_model()\n",
    "        \n",
    "        # [TODO] Build the baseline network using Net class.\n",
    "        # Remember that you need to set the output dimension to 1.\n",
    "        # Remember using model.to(device) function to move your model to GPU (if applicable)\n",
    "        self.baseline = Net(self.obs_dim, 1).to(self.device)\n",
    "        \n",
    "        self.baseline_loss = nn.MSELoss()\n",
    "            \n",
    "        self.baseline_optimizer = torch.optim.Adam(\n",
    "            self.baseline.parameters(),\n",
    "            lr=self.config[\"learning_rate\"]\n",
    "        )\n",
    "\n",
    "    def process_samples(self, samples):\n",
    "        # Call the original process_samples function to get advantages\n",
    "        tmp_samples, _ = super().process_samples(samples)\n",
    "        values = tmp_samples[\"advantages\"]\n",
    "        samples[\"values\"] = values  # We add q_values into samples\n",
    "\n",
    "        # [TODO] flatten the observations in all trajectories (still a numpy array)\n",
    "        obs = np.concatenate(tmp_samples[\"obs\"])\n",
    "        \n",
    "        assert obs.ndim == 2\n",
    "        assert obs.shape[1] == self.obs_dim\n",
    "        \n",
    "        obs = self.to_tensor(obs)\n",
    "        samples[\"flat_obs\"] = obs\n",
    "\n",
    "        # [TODO] Compute the baseline by feeding observation to baseline network\n",
    "        # Hint: baselines turns out to be a numpy array with the same shape of `values`,\n",
    "        #  that is: (batch size, )\n",
    "        baselines = self.to_array(self.baseline(obs).view(-1))\n",
    "        \n",
    "        assert baselines.shape == values.shape\n",
    "        \n",
    "        # [TODO] Match the distribution of baselines to the values.\n",
    "        # Hint: We expect to see baselines.std almost equals to values.std, \n",
    "        #  and baselines.mean almost equals to values.mean\n",
    "        baselines = (baselines - baselines.mean() + values.mean()) / max(baselines.std(), 1e-6) * max(values.std(), 1e-6)\n",
    "\n",
    "        # Compute the advantage\n",
    "        advantages = values - baselines\n",
    "        samples[\"advantages\"] = advantages\n",
    "        process_info = {\"mean_baseline\": float(np.mean(baselines))}\n",
    "        return samples, process_info\n",
    "\n",
    "    def update_model(self, processed_samples):\n",
    "        update_info = {}\n",
    "        update_info.update(self.update_baseline(processed_samples))\n",
    "        update_info.update(self.update_policy(processed_samples))\n",
    "        return update_info\n",
    "\n",
    "    def update_baseline(self, processed_samples):\n",
    "        self.baseline.train()\n",
    "        obs = processed_samples[\"flat_obs\"]\n",
    "\n",
    "        # [TODO] Normalize the values to mean=0, std=1.\n",
    "        values = processed_samples[\"values\"]\n",
    "        values = (values - values.mean()) / max(values.std(), 1e-6)\n",
    "        \n",
    "        values = self.to_tensor(values[:, np.newaxis])\n",
    "        \n",
    "        baselines = self.baseline(obs)\n",
    "\n",
    "        self.baseline_optimizer.zero_grad()\n",
    "        loss = self.baseline_loss(input=baselines, target=values)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradient\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            self.baseline.parameters(), self.config[\"clip_gradient\"]\n",
    "        )\n",
    "        \n",
    "        self.baseline_optimizer.step()\n",
    "        self.baseline.eval()\n",
    "        return dict(baseline_loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PolicyGradientWithBaselineTrainer CartPole-v0 Training Start ===\n",
      "Config:\n",
      "  checked: true\n",
      "  clip_gradient: 10.0\n",
      "  env_name: CartPole-v0\n",
      "  evaluate_interval: 10\n",
      "  evaluate_num_episodes: 50\n",
      "  gamma: 0.99\n",
      "  hidden_units: 64\n",
      "  learning_rate: 0.01\n",
      "  max_episode_length: 200\n",
      "  max_iteration: 1000\n",
      "  normalize_advantage: true\n",
      "  seed: 0\n",
      "  train_batch_size: 200\n",
      "\n",
      "=== PolicyGradientWithBaselineTrainer CartPole-v0 Iteration 0 ===\n",
      "Training Progress:\n",
      "  baseline_loss: 1.065671443939209\n",
      "  evaluate_reward: 23.92\n",
      "  iter_episodes: 12\n",
      "  iter_time: 0.0966947078704834\n",
      "  iter_timesteps: 219\n",
      "  iteration: 0\n",
      "  mean_advantage: -1.0886693324607677e-08\n",
      "  mean_baseline: 1.9596047451386767e-08\n",
      "  mean_log_prob: -0.6843988299369812\n",
      "  performance: 18.25\n",
      "  policy_loss: 1.5359973907470703\n",
      "  total_episodes: 12\n",
      "  total_time: 0.0966947078704834\n",
      "  total_timesteps: 219\n",
      "  training_episode_length:\n",
      "    max: 36.0\n",
      "    mean: 18.25\n",
      "    min: 11.0\n",
      "  training_episode_reward:\n",
      "    max: 36.0\n",
      "    mean: 18.25\n",
      "    min: 11.0\n",
      "\n",
      "=== PolicyGradientWithBaselineTrainer CartPole-v0 Iteration 10 ===\n",
      "Training Progress:\n",
      "  baseline_loss: 0.7732068300247192\n",
      "  evaluate_reward: 37.5\n",
      "  iter_episodes: 6\n",
      "  iter_time: 0.0840001106262207\n",
      "  iter_timesteps: 214\n",
      "  iteration: 10\n",
      "  mean_advantage: 8.801433182270557e-08\n",
      "  mean_baseline: -1.3369266582685668e-07\n",
      "  mean_log_prob: -0.6704781651496887\n",
      "  performance: 35.666666666666664\n",
      "  policy_loss: -7.636831283569336\n",
      "  total_episodes: 94\n",
      "  total_time: 1.386291742324829\n",
      "  total_timesteps: 2377\n",
      "  training_episode_length:\n",
      "    max: 50.0\n",
      "    mean: 35.666666666666664\n",
      "    min: 21.0\n",
      "  training_episode_reward:\n",
      "    max: 50.0\n",
      "    mean: 35.666666666666664\n",
      "    min: 21.0\n",
      "\n",
      "=== PolicyGradientWithBaselineTrainer CartPole-v0 Iteration 20 ===\n",
      "Training Progress:\n",
      "  baseline_loss: 0.7765956521034241\n",
      "  evaluate_reward: 105.36\n",
      "  iter_episodes: 3\n",
      "  iter_time: 0.11183977127075195\n",
      "  iter_timesteps: 278\n",
      "  iteration: 20\n",
      "  mean_advantage: -1.1577880876245672e-08\n",
      "  mean_baseline: 2.7443865846521476e-08\n",
      "  mean_log_prob: -0.6075342893600464\n",
      "  performance: 92.66666666666667\n",
      "  policy_loss: -16.61269187927246\n",
      "  total_episodes: 135\n",
      "  total_time: 3.0263025760650635\n",
      "  total_timesteps: 4823\n",
      "  training_episode_length:\n",
      "    max: 139.0\n",
      "    mean: 92.66666666666667\n",
      "    min: 64.0\n",
      "  training_episode_reward:\n",
      "    max: 139.0\n",
      "    mean: 92.66666666666667\n",
      "    min: 64.0\n",
      "\n",
      "=== PolicyGradientWithBaselineTrainer CartPole-v0 Iteration 30 ===\n",
      "Training Progress:\n",
      "  baseline_loss: 0.5110098719596863\n",
      "  evaluate_reward: 130.72\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.1508009433746338\n",
      "  iter_timesteps: 388\n",
      "  iteration: 30\n",
      "  mean_advantage: -3.748333199382614e-08\n",
      "  mean_baseline: 7.865354945124636e-08\n",
      "  mean_log_prob: -0.546448290348053\n",
      "  performance: 194.0\n",
      "  policy_loss: -14.068710327148438\n",
      "  total_episodes: 158\n",
      "  total_time: 6.094776153564453\n",
      "  total_timesteps: 7595\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 194.0\n",
      "    min: 188.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 194.0\n",
      "    min: 188.0\n",
      "\n",
      "=== PolicyGradientWithBaselineTrainer CartPole-v0 Iteration 40 ===\n",
      "Training Progress:\n",
      "  baseline_loss: 0.6151655912399292\n",
      "  evaluate_reward: 191.8\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.12653207778930664\n",
      "  iter_timesteps: 316\n",
      "  iteration: 40\n",
      "  mean_advantage: -3.244303314886565e-08\n",
      "  mean_baseline: 0.0\n",
      "  mean_log_prob: -0.5494881868362427\n",
      "  performance: 158.0\n",
      "  policy_loss: -12.118521690368652\n",
      "  total_episodes: 179\n",
      "  total_time: 9.65967607498169\n",
      "  total_timesteps: 10667\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 158.0\n",
      "    min: 116.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 158.0\n",
      "    min: 116.0\n",
      "\n",
      "=== PolicyGradientWithBaselineTrainer CartPole-v0 Iteration 50 ===\n",
      "Training Progress:\n",
      "  baseline_loss: 0.5924707651138306\n",
      "  evaluate_reward: 195.42\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.13965821266174316\n",
      "  iter_timesteps: 302\n",
      "  iteration: 50\n",
      "  mean_advantage: 3.1578619719141443e-09\n",
      "  mean_baseline: 0.0\n",
      "  mean_log_prob: -0.5591593980789185\n",
      "  performance: 151.0\n",
      "  policy_loss: -13.115449905395508\n",
      "  total_episodes: 199\n",
      "  total_time: 14.473676443099976\n",
      "  total_timesteps: 14412\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 151.0\n",
      "    min: 102.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 151.0\n",
      "    min: 102.0\n",
      "\n",
      "In 50 iteration, current mean episode reward 195.420 is greater than reward threshold 195.0. Congratulation! Now we exit the training process.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pg_trainer_wb, pgwb_result_wb = run(PolicyGradientWithBaselineTrainer, dict(\n",
    "    learning_rate=0.01,\n",
    "    max_episode_length=200,\n",
    "    train_batch_size=200,\n",
    "), 195.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Policy Gradient')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pg_result_no_na[\"normalize_advantage\"] = False\n",
    "pg_result_na[\"normalize_advantage\"] = True\n",
    "pgwb_result_wb[\"normalize_advantage\"] = \"True with Baseline\"\n",
    "pg_result = pd.concat([pg_result_no_na, pg_result_na, pgwb_result_wb])\n",
    "ax = sns.lineplot(\n",
    "    x=\"total_timesteps\", \n",
    "    y=\"performance\", \n",
    "    data=pg_result, hue=\"normalize_advantage\",\n",
    ")\n",
    "ax.set_title(\"Policy Gradient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward for your Policy Gradient agent in CartPole-v0:  193.0\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# You should see a pop up window which display the movement of the cart and pole.\n",
    "print(\"Average episode reward for your Policy Gradient agent in CartPole-v0: \",\n",
    "      pg_trainer_wb.evaluate(1, render=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Actor-critic Method\n",
    "\n",
    "(30 / 100 points totally)\n",
    "\n",
    "### Section 4.1: Implement ActorCriticTrainer\n",
    "\n",
    "(20 / 100 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As point out in the textbook Page 331, the difference of Actor-critic methods and policy gradient with baseline method is whether the critic (baseline) participates in the bootstrapping. In `PolicyGradientWithBaselineTrainer`, we update the baseline network by fitting the ground-truth Q values. However, in actor-critic trainer presented below, we update the critic via fitting a bootstrapped target: \n",
    "\n",
    "$$\\delta = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "wherein $V$ is the values predicted by critic. In `ActorCriticTrainer`, in order to reuse the codes, we use `self.baseline` to represent the critic network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "ac_default_config = merge_config(dict(\n",
    "    normalize_advantage=True,\n",
    "    num_critic_updates=10,\n",
    "    num_critic_update_steps=10\n",
    "), pg_with_baseline_default_config)\n",
    "\n",
    "class ActorCriticTrainer(PolicyGradientWithBaselineTrainer):\n",
    "    def __init__(self, config=None):\n",
    "        config = check_and_merge_config(config or {}, ac_default_config)\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def process_samples(self, samples):\n",
    "        \"\"\"\n",
    "        In this function, we will compute the target of value function\n",
    "        and also the advantage used for updating policy.\n",
    "        \n",
    "        First, we need to prepare data to compute the values.\n",
    "        \n",
    "        Consider you only collect two trajectories in samples.\n",
    "        Then the data structure should be:\n",
    "            samples[\"obs\"] = [\n",
    "                [\n",
    "                    obs_t1_0,  <<= t=0\n",
    "                    obs_t1_1,  <<= t=1\n",
    "                    obs_t1_n1-1, <<= t=n1-1 \n",
    "                ],  (the traj t1 has length = n1)\n",
    "                [\n",
    "                    obs_t2_0,\n",
    "                    ...\n",
    "                    obs_t2_n2-1 \n",
    "                ]   (the traj t2 has length = n2)\n",
    "            ]\n",
    "        the whole training batch size is N = (n1 + n2).\n",
    "        Then what you should do is to take store\n",
    "        [\n",
    "            obs_t1_1, ..., obs_t1_n1-1, ZERO, \n",
    "            obs_t2_1, ... obs_t2_n2-1, ZERO\n",
    "        ]\n",
    "        into `next_obs` variable, wherein ZERO is a zero array has\n",
    "        same shape as any other observation.\n",
    "        \n",
    "        Besides, you also need to prepare a boolean array `masks`\n",
    "        who has length N. It should be like:\n",
    "        [\n",
    "            False(0), ..., False(n1-2), True(n1-1),\n",
    "            False(0), ..., False(n2-2), True(n2-1)\n",
    "        ].\n",
    "        It represents whether a transition is terminal transition\n",
    "        and whether a observation in next_obs should be masked.\n",
    "        We will used it when computing the bootstrapped values:\n",
    "        \n",
    "            value_t = reward_t + gamma * value_t+1 * (1 - masks)\n",
    "            \n",
    "        So that we will not consider the impact of value_t+1\n",
    "        in terminal state.\n",
    "        \n",
    "        Then, we need to compute the values and advantage based on \n",
    "        the data collected and form a processed_samples for the \n",
    "        consecetive .\n",
    "        \"\"\"\n",
    "        # Define the sum of all trajectory length as N (train batch size)\n",
    "        N = sum([len(traj) for traj in samples[\"obs\"]])\n",
    "        n = len(samples[\"obs\"][0])  # the first traj length\n",
    "        \n",
    "        # [TODO] Create an array named next_obs.\n",
    "        # In each trajectory, take the observations from second\n",
    "        # transitions to the end. Fill an extra all zero observation\n",
    "        # at the end of trajectory to align next_obs with others.\n",
    "        next_obs = []\n",
    "        for obs_list in samples[\"obs\"]:\n",
    "            for i in range(1, len(obs_list)):\n",
    "                next_obs.append(obs_list[i])\n",
    "            next_obs.append(np.zeros_like(obs_list[0]))\n",
    "            \n",
    "        next_obs = np.array(next_obs)\n",
    "        \n",
    "        assert next_obs.shape == (N, self.obs_dim)\n",
    "        assert np.all(next_obs[n - 1] == 0.0)\n",
    "        \n",
    "        # [TODO] Scan all trajectories and create a boolean array `masks`.\n",
    "        # It should be N length, N is the sum of the trajectories lengths.\n",
    "        # You should loop over each trajectory and store n-1 False into masks\n",
    "        # while storing one True at the end of each episode. n is the length\n",
    "        # of current trajectory.\n",
    "        # Finally, you should transform the list to an float32 array\n",
    "        # with shape (N,)\n",
    "        masks = []\n",
    "        for obs_list in samples[\"obs\"]:\n",
    "            masks += [False] * (len(obs_list) - 1) + [True]\n",
    "            \n",
    "        masks = np.array(masks, dtype=np.float32)\n",
    "        \n",
    "        assert masks.shape == (N,)\n",
    "        assert masks[n - 1] == True\n",
    "        \n",
    "        # flatten rewards\n",
    "        rewards = np.concatenate(samples[\"reward\"])\n",
    "        tensor_rewards = self.to_tensor(rewards)\n",
    "        \n",
    "        # flatten observations\n",
    "        obs = np.concatenate(samples[\"obs\"])\n",
    "        \n",
    "        # [TODO] Compute the bootstrapped values\n",
    "        # Hint: value_t = reward_t + gamma * value_t+1 * (1 - masks)\n",
    "        #  You need to ask self.baseline using next_obs to get \n",
    "        #  the values in next state, then can you compute the \n",
    "        #  value_t.\n",
    "        tensor_next_obs = self.to_tensor(next_obs)\n",
    "        next_values = self.to_array(self.baseline(tensor_next_obs).view(-1))\n",
    "        values = rewards + self.config[\"gamma\"] * next_values * (1 - masks)\n",
    "        \n",
    "        assert isinstance(values, np.ndarray)\n",
    "        assert values.shape == (N,)\n",
    "        \n",
    "        # [TODO] Compute the baseline by feeding obs to critic\n",
    "        tensor_obs = self.to_tensor(obs)\n",
    "        baselines = self.to_array(self.baseline(tensor_obs).view(-1))\n",
    "        \n",
    "        assert isinstance(baselines, np.ndarray)\n",
    "        assert baselines.shape == (N,)\n",
    "        \n",
    "        # [TODO] Compute the advantages using values and baselines\n",
    "        advantages = values - baselines\n",
    "        \n",
    "        if self.config[\"normalize_advantage\"]:\n",
    "            # [TODO] normalize the advantages\n",
    "            advantages = (advantages - advantages.mean()) / max(advantages.std(), 1e-6)\n",
    "        \n",
    "        # We passed part of numpy array and part of torch tensor\n",
    "        # to the following modules.\n",
    "        processed_samples = samples\n",
    "        processed_samples[\"advantages\"] = advantages\n",
    "        processed_samples[\"tensor_next_obs\"] = tensor_next_obs\n",
    "        processed_samples[\"tensor_obs\"] = tensor_obs\n",
    "        processed_samples[\"tensor_rewards\"] = tensor_rewards\n",
    "        processed_samples[\"tensor_masks\"] = self.to_tensor(masks)\n",
    "        process_info = {\n",
    "            \"mean_baselines\": float(np.mean(baselines)),\n",
    "            \"advantages_std\": float(advantages.std())\n",
    "        }  # You can add value here so that it will be printed automatically.\n",
    "        return processed_samples, process_info\n",
    "        \n",
    "    def update_baseline(self, processed_samples):\n",
    "        # Use a bootstrapped target values and update the critic.\n",
    "        # Compute the target values r(s, a) + gamma * V(s') by calling\n",
    "        # the critic to compute V(s').\n",
    "        reward = processed_samples[\"tensor_rewards\"]\n",
    "        obs = processed_samples[\"tensor_obs\"]\n",
    "        next_obs = processed_samples[\"tensor_next_obs\"]\n",
    "        masks = processed_samples[\"tensor_masks\"]\n",
    "        \n",
    "        assert masks.shape == reward.shape\n",
    "        assert next_obs.shape == obs.shape\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        for _ in range(self.config[\"num_critic_updates\"]):  # Train critic for 10 iteration\n",
    "            self.baseline.eval()\n",
    "            \n",
    "            # Predict the values of next state\n",
    "            values_next = self.baseline(next_obs).reshape(reward.shape)  # a tensor\n",
    "            \n",
    "            assert values_next.shape == reward.shape  # tensor shape should equal\n",
    "            \n",
    "            # [TODO] Compute the target of critic using reward, masks\n",
    "            # and values_next (don't forget gamma)\n",
    "            # Hint: the masks are all False (that is, zero) when the state is not\n",
    "            #  terminal state, and are True (one) when is. So we can use (1-masks)\n",
    "            #  as a multiplier to apply to values_next when computing the target.\n",
    "            target = reward + self.config[\"gamma\"] * values_next * (1 - masks)\n",
    "            \n",
    "            target = target.detach()\n",
    "            self.baseline.train()\n",
    "\n",
    "            for _ in range(self.config[\"num_critic_update_steps\"]):\n",
    "                # [TODO] Uncomment the whole section\n",
    "                baselines = self.baseline(obs).reshape(reward.shape)\n",
    "                loss = self.baseline_loss(input=baselines, target=target)\n",
    "                self.baseline_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.baseline.parameters(), self.config[\"clip_gradient\"]\n",
    "                )\n",
    "                self.baseline_optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "            \n",
    "        self.baseline.eval()\n",
    "        return {\"critic_loss\": np.mean(losses)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ActorCriticTrainer CartPole-v0 Training Start ===\n",
      "Config:\n",
      "  checked: true\n",
      "  clip_gradient: 10.0\n",
      "  env_name: CartPole-v0\n",
      "  evaluate_interval: 10\n",
      "  evaluate_num_episodes: 50\n",
      "  gamma: 0.99\n",
      "  hidden_units: 64\n",
      "  learning_rate: 0.01\n",
      "  max_episode_length: 200\n",
      "  max_iteration: 1000\n",
      "  normalize_advantage: true\n",
      "  num_critic_update_steps: 10\n",
      "  num_critic_updates: 10\n",
      "  seed: 0\n",
      "  train_batch_size: 200\n",
      "\n",
      "=== ActorCriticTrainer CartPole-v0 Iteration 0 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999999403953552\n",
      "  critic_loss: 1.2177751067280769\n",
      "  evaluate_reward: 22.28\n",
      "  iter_episodes: 12\n",
      "  iter_time: 0.2574889659881592\n",
      "  iter_timesteps: 219\n",
      "  iteration: 0\n",
      "  mean_advantage: 3.3639881280578265e-07\n",
      "  mean_baselines: -0.3343355059623718\n",
      "  mean_log_prob: -0.6843988299369812\n",
      "  performance: 18.25\n",
      "  policy_loss: -4.012794494628906\n",
      "  total_episodes: 12\n",
      "  total_time: 0.2574889659881592\n",
      "  total_timesteps: 219\n",
      "  training_episode_length:\n",
      "    max: 36.0\n",
      "    mean: 18.25\n",
      "    min: 11.0\n",
      "  training_episode_reward:\n",
      "    max: 36.0\n",
      "    mean: 18.25\n",
      "    min: 11.0\n",
      "\n",
      "=== ActorCriticTrainer CartPole-v0 Iteration 10 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 3.5304459643363955\n",
      "  evaluate_reward: 47.12\n",
      "  iter_episodes: 5\n",
      "  iter_time: 0.28453803062438965\n",
      "  iter_timesteps: 241\n",
      "  iteration: 10\n",
      "  mean_advantage: -1.97857752404218e-09\n",
      "  mean_baselines: 21.83340072631836\n",
      "  mean_log_prob: -0.6339228749275208\n",
      "  performance: 48.2\n",
      "  policy_loss: -32.871665954589844\n",
      "  total_episodes: 80\n",
      "  total_time: 3.606191396713257\n",
      "  total_timesteps: 2477\n",
      "  training_episode_length:\n",
      "    max: 76.0\n",
      "    mean: 48.2\n",
      "    min: 31.0\n",
      "  training_episode_reward:\n",
      "    max: 76.0\n",
      "    mean: 48.2\n",
      "    min: 31.0\n",
      "\n",
      "=== ActorCriticTrainer CartPole-v0 Iteration 20 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999999403953552\n",
      "  critic_loss: 1.58562162399292\n",
      "  evaluate_reward: 70.68\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.2977569103240967\n",
      "  iter_timesteps: 244\n",
      "  iteration: 20\n",
      "  mean_advantage: 3.224513633881543e-08\n",
      "  mean_baselines: 32.40815353393555\n",
      "  mean_log_prob: -0.5764136910438538\n",
      "  performance: 122.0\n",
      "  policy_loss: -27.044662475585938\n",
      "  total_episodes: 122\n",
      "  total_time: 8.189496994018555\n",
      "  total_timesteps: 4844\n",
      "  training_episode_length:\n",
      "    max: 136.0\n",
      "    mean: 122.0\n",
      "    min: 108.0\n",
      "  training_episode_reward:\n",
      "    max: 136.0\n",
      "    mean: 122.0\n",
      "    min: 108.0\n",
      "\n",
      "=== ActorCriticTrainer CartPole-v0 Iteration 30 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 8.549580850601195\n",
      "  evaluate_reward: 88.84\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.2543766498565674\n",
      "  iter_timesteps: 213\n",
      "  iteration: 30\n",
      "  mean_advantage: 0.0\n",
      "  mean_baselines: 47.99197769165039\n",
      "  mean_log_prob: -0.49410921335220337\n",
      "  performance: 106.5\n",
      "  policy_loss: -3.5335586071014404\n",
      "  total_episodes: 156\n",
      "  total_time: 12.710586071014404\n",
      "  total_timesteps: 7244\n",
      "  training_episode_length:\n",
      "    max: 130.0\n",
      "    mean: 106.5\n",
      "    min: 83.0\n",
      "  training_episode_reward:\n",
      "    max: 130.0\n",
      "    mean: 106.5\n",
      "    min: 83.0\n",
      "\n",
      "=== ActorCriticTrainer CartPole-v0 Iteration 40 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 1.4630210620164872\n",
      "  evaluate_reward: 84.7\n",
      "  iter_episodes: 3\n",
      "  iter_time: 0.5632724761962891\n",
      "  iter_timesteps: 264\n",
      "  iteration: 40\n",
      "  mean_advantage: -3.612402821318028e-09\n",
      "  mean_baselines: 38.759002685546875\n",
      "  mean_log_prob: -0.4824106991291046\n",
      "  performance: 88.0\n",
      "  policy_loss: 1.8029084205627441\n",
      "  total_episodes: 185\n",
      "  total_time: 18.52391242980957\n",
      "  total_timesteps: 9808\n",
      "  training_episode_length:\n",
      "    max: 95.0\n",
      "    mean: 88.0\n",
      "    min: 75.0\n",
      "  training_episode_reward:\n",
      "    max: 95.0\n",
      "    mean: 88.0\n",
      "    min: 75.0\n",
      "\n",
      "=== ActorCriticTrainer CartPole-v0 Iteration 50 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.27845499411225316\n",
      "  evaluate_reward: 89.68\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.24955105781555176\n",
      "  iter_timesteps: 206\n",
      "  iteration: 50\n",
      "  mean_advantage: 0.0\n",
      "  mean_baselines: 41.80498123168945\n",
      "  mean_log_prob: -0.44830822944641113\n",
      "  performance: 103.0\n",
      "  policy_loss: 16.288541793823242\n",
      "  total_episodes: 211\n",
      "  total_time: 24.812755823135376\n",
      "  total_timesteps: 12411\n",
      "  training_episode_length:\n",
      "    max: 140.0\n",
      "    mean: 103.0\n",
      "    min: 66.0\n",
      "  training_episode_reward:\n",
      "    max: 140.0\n",
      "    mean: 103.0\n",
      "    min: 66.0\n",
      "\n",
      "=== ActorCriticTrainer CartPole-v0 Iteration 60 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.21431506007909776\n",
      "  evaluate_reward: 142.48\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.2345285415649414\n",
      "  iter_timesteps: 201\n",
      "  iteration: 60\n",
      "  mean_advantage: -2.3723241149298246e-09\n",
      "  mean_baselines: 30.112810134887695\n",
      "  mean_log_prob: -0.45264503359794617\n",
      "  performance: 100.5\n",
      "  policy_loss: -16.582427978515625\n",
      "  total_episodes: 236\n",
      "  total_time: 29.54921817779541\n",
      "  total_timesteps: 14798\n",
      "  training_episode_length:\n",
      "    max: 114.0\n",
      "    mean: 100.5\n",
      "    min: 87.0\n",
      "  training_episode_reward:\n",
      "    max: 114.0\n",
      "    mean: 100.5\n",
      "    min: 87.0\n",
      "\n",
      "=== ActorCriticTrainer CartPole-v0 Iteration 70 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 9.965487680435182\n",
      "  evaluate_reward: 200.0\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.6153864860534668\n",
      "  iter_timesteps: 400\n",
      "  iteration: 70\n",
      "  mean_advantage: 0.0\n",
      "  mean_baselines: 66.41399383544922\n",
      "  mean_log_prob: -0.4918277859687805\n",
      "  performance: 200.0\n",
      "  policy_loss: -16.534536361694336\n",
      "  total_episodes: 256\n",
      "  total_time: 37.88404178619385\n",
      "  total_timesteps: 18520\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 200.0\n",
      "    min: 200.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 200.0\n",
      "    min: 200.0\n",
      "\n",
      "In 70 iteration, current mean episode reward 200.000 is greater than reward threshold 195.0. Congratulation! Now we exit the training process.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "ac_trainer, ac_result = run(ActorCriticTrainer, dict(\n",
    "    learning_rate=0.01,\n",
    "    max_episode_length=200,\n",
    "    train_batch_size=200,\n",
    "), 195.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Policy Gradient')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEXCAYAAABcRGizAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOydd5xcZ33uv+/0vr1otSutqiXZxk2muMUYh05C7ART4uCQxJAAoeSGcEnuDSkQQkK5IYVQQqgOxfSOsS3AxsiSq2T1ru27s7vT+3v/eM+ZOTNzZnZWu7Pacp7PZz/aPXPOmTOamfc5z/NrQkqJBQsWLFiwUA+2i30BFixYsGBh+cMiCwsWLFiwMCcssrBgwYIFC3PCIgsLFixYsDAnLLKwYMGCBQtzwiILCxYsWLAwJyyysLCqIYQ4LYS4Vfv9PUKIT13sa7oQCCH+Wwjx99rvNwohjlzsa7KwtmCRhYUVAW3RTwohYkKIMW3xDMznHFLK90sp/7AJ17ZbCPFdIcS0EGJGCPGMEOJ9Qoi2xX4uACnlz6WUlyzGuYxkasFCPVhkYWEl4RVSygBwNbAb+KuLfD0IIa4DHgQeAnZIKVuBFwM54IoaxziW7AItWFgkWGRhYcVBSjkE/AC4DEAI8RtCiIPaXf2DQoidZscJId4rhPiC4e8bhBAPa8edE0LcJYS4VlMudsN+twkhnqxxOR8EPiOl/Acp5Zh2fWellH8tpXxQO/4uIcRDQoiPCCGmgPcKIbYIIe4XQkwJISaFEF8UQrQanvMqIcRjQoioEOLLgMfw2M1CiPOGv/uEEPcKISaEEKeEEH9a8Zq/IoT4nHaug0KI3dpjnwc2AN/RFNu7GnwLLKxBWGRhYcVBCDEAvBR4XAixHbgHeDvQBXwftfi55jjHRhThfEw77krgCSnlo8AU8ELD7ncCnzM5hx94HnBvA5f9HOAk0AO8DxDAPwB9wE5gAHivdl4X8E3g80A78FXg9hqvwwZ8B3gSWA+8AHi7EOJFht1+A/gfoBX4NvCvAFLKO4GzaIpNSvnBBl6HhTUKiywsrCR8UwgxA/wC2AO8H7gD+J6U8idSyizwz4AXuG6Oc70WuE9KeY+UMiulnJJSPqE99lngdwGEEO3Ai4AvmZyjDfUdGtU3CCE+qCmVuBDCaJMNSyk/JqXMSSmTUsrj2jWnpZQTwIeBX9P2fS7gBD6qXdvXgEdrvI5rgS4p5d9KKTNSypPAJ4FXG/b5hZTy+1LKPIqATO0xCxbqwfJOLawkvFJKeZ9xgxCiDzij/y2lLAghzqHusuthADhR47EvAIc05fAq4OdSyhGT/aaBArAOOKw9/7uAd2l2l/H7da7iunuA/wfcCARRpDOtPdwHDMnyLp9nMMdGoE8jUR124OeGv0cNvycAjxDCIaXM1TinBQtVsJSFhZWOYdSCCYAQQqCIYGiO484BW8we0GIivwRuQ1lQn6+xXxz4lbbfXKhs7/x+bdvlUsoQSskI7bERYL32WnRsqHHec8ApKWWr4ScopXxpA9dkdl0WLJjCIgsLKx1fAV4mhHiBEMIJ/BmQBh6e47gvArcKIV4lhHAIITqEEFcaHv8c8C7gcuDrdc7zLuANQoh3CyG6AYQQ/cCmOZ4/CMSAWSHEeuDPDY/9EpVN9adCCKcQ4jbg2TXOsxeICiH+QgjhFULYhRCXCSGuneP5dYwBmxvc18IahkUWFlY0pJRHUHflHwMmgVegAraZOY47iwqS/xkQBp6g3Mv/BkqxfENKmahznl8AtwA3AUc1O+iHqHTaj9W5hL9BpQDPAt/DQEjatd8G3KVd2x3UICwtDvFyVID+FOr/4FNAS53nNuIfgL/S4iz/q8FjLKxBCGv4kQUL5hBCnADeWBknsWBhLcJSFhYsmEAIcTvKz7//Yl+LBQvLAVY2lAULFRBCPAjsAu6UUhYu8uVYsLAsYNlQFixYsGBhTlg2lAULFixYmBMr2obq7OyUg4ODF/syLFiwYGFFYf/+/ZNSyq75HLOiyWJwcJB9+/Zd7MuwYMGChRUFIUStjgA1YdlQFixYsGBhTlhkYcGCBQsW5oRFFhYsWLBgYU5YZGHBggULFuaERRYWLFiwYGFONI0shBADQogHtOH1B4UQb9O2twshfiKEOKb926ZtF0KIfxFCHBdCPCWEuLpZ12bBggULFuaHZiqLHPBnUspdqMlfbxZC7ALeDfxUSrkN+Kn2N8BLgG3az93AfzTx2ixYsGDBwjzQtDoLbbLYiPZ7VAhxCDW97DeBm7XdPotq5fwX2vbPadPBHhFCtAoh1tWYUGZhmWH/mTDPDEfKtl21oY3L1pt3yv75sQmuHWzH47RXPTYWSfHjZ8YIRU8ghZ2ofyNdQQ8vvqy3tFM2CU9/DXIpZlM5ZhNZNrR7TZ8rk5c8fX6WXKHAcOf1xHz9BD1OXvasdTjtNpASDn4dEuHiMUfHY0zH63Y5p337c9l21a/V3QfglyemOD4erdp+LnGQycxpAHpDHjZ0+PE7/fQH+1kfWE+ntxObsPHomSH2nDzCbHaMeH6ahc4rCjq62BqoHo9x/dZONncFqrZLKbn/8DjPv6Qbm01UPV6JVDbPIyenuPmS7gVd51rGDw+MMhFNlTYIwa07u1nXYv4ZXwosSVGeEGIQuAo1VazHQACjqAH2oIjEOHryvLatjCyEEHejlAcbNtQaHmZhKTEZS3Pnp/eSyOTLtu9cF+IHb7uxav+hmSR3fnovH7jtcl797Or38N8fOM5nf3mGe11/TUx6eXtWic+H3n0L61u1L8vRH8K33wKowQ31hje4gGu037+Zv47/k1XHPXl+hr9+xaUQPglfe0PZMdvnfNVw9GA/3xI/5jevrD/B9Y2f30ckVT3B1L/t77E5YuqPceB4+eNuuxuX3UU0U000C0Eutp3kOX/V9lt39vCp1++u2v6rU2H+4LP7+M87r+FFl/ZWPV6JLzxyhr//3iHue+evsbW7mnws1Mf+M9O86Qv7q7Z//EEv33rL9XQG3BfhqpaALIQQAeBe4O1SyohxUqSUUgoh5nWbJKX8BPAJgN27d1tdEJcBPv7gCVLZPN968/Wsb1OL+Qd+cJgHDo+b7j80nQRgLJI2f3wmxbbuAFeSRrpdfOLma7j78/s5NREvkUVEu4d4y35+754jHBya5WWXr+Nvf/OysnMdHY/y2k8+wh27B3jHyJ/zkpYgN/zWrfzr/cf5zEOnefZgOy9p08ZX3/Ypjgau4a7P7OXSdS28//bLsGF+J+198K/pOvJTXvg/TzAdz3DX9eaD8QoFSSSV4+6bNnP3TaWBdPlCjlu/8W7u2HYntsjz+eTPT3L///o1pC3BUHSIoZj6SWSTfPGhKDcMbuf3n3MNnZ4ubGJh7rHT5iTgCpZt+9N7HmfceCdrwHnt/frVyXBDZPHQ8UkADgzNWmRxAfjEz07Q4nXy/bfdiNuh3usT4zFe/5m93P25fXzpj55rqsibjaaShTbm8l7gi1JKfdLXmG4vCSHWoe6pQM1MHjAc3s/cc5QtXGSMzqb43CNnuO3qfq4YaC1uX9/qJZzIkMsXcNjLF7eRWbX4TMXNyWI8mqKv1Yt9NAwUuLxf6YYz4Tg30Kl2io+DzYls38zjkyfIOtv4/IEEr7nFzc51oeK5/u7LJ8l6OvmjlzwHxxf9OMjiDrh5z0t38vi5Gd71tae46pWSXiDm6ebue8+Q83TyvjtvoCvoqf3C29bhtSV50aU9vPc7zxCOZ3jHr2+nfGw2JLNKbXUGXGV3hJNJpRa2tA/Q3jLAJ/LjRGIerhjoZXNLiVROTMT49Lf28JLnX8H1A/21r2eB6G3x8KuTYdPHRrX369HT5o8bkc0X2HtK7ffMSIRXXlVfdVkox8mJGD9+Zow337y1dGMEdAbcfORVV/LHX3yMv7j3KT56x5VVn7Vmo5nZUAL4NHBISvlhw0PfBl6v/f564FuG7b+nZUU9F5i14hXLH//6wDGklLztBdvKtncG3UgJ4US17z86q+5gp2rEBEZnU6wPAJkYJMP0BD24HDbOTBmmm8bGIdDNeCxDNJ3jzTdvJeh28E8/OlLcZc/RCX5+bJK33rKVVp8LHB7IKYJyOWz822uvwmYTfOK+pwH48J5hzk8n+ffXXU13PaIAcLcgcin+7Y7LuGP3AP9y/3H+8YdHqnaLp5X95HeX35eFU2pBbfe0s6lTWUKnp+JVxx8bU6Syrae5d+hdATcTsTRmIwtGI+r9Ojg8SyxdbacZ8dT5GeKZPDah9rcwP3zqF6dw2m28/rrBqsdecvk6/vxFl/CtJ4b52P3Hqw9uMpqZDXU9cCdwixDiCe3npcAHgF8XQhwDbtX+Bvg+cBLl3H4S+JMmXpuFRcC5cIL/2XuOO64dYKDdV/ZYV8AFwGS0mhBGdLKIVSuLXL7AZCzNoFd7LJvAlk+zsd3H6UnDYhobg0A3x8eV53/Nxjb++Oat3H94nL2nwuQLkvd/7xAb2n3c+byN6hiHG3Ilq6W/zceHX3UF4Wm1cN9/Ms57XrqT3YPtc794j1I7jmyMD9x+Obfs6OYbj5+v2k1fXP2ucrKYTk0Diiw2tPsQAk5NmpGFen3NtnM6Ai4yuQJREzIYnU1hE1CQyk+vh4ePTyEE/PquHg4OR0zJZ61BSsmnf3GKMyY3A0ZMxtJ8bf95br96PV1B87jEn9y8hduuWs+Hf3KU7zw53IzLrYmmkYWU8hdSSiGlfJaU8krt5/tSyikp5QuklNuklLdKKcPa/lJK+WYp5RYp5eVSSqud7DLH//vpMew2wVtv2Vb1mG65TJgQgq4swibKYjKWoSChz50sbUyG2djh42zYqCzGINDDiYnSYnrXdYP0hNx84AeH+Oq+cxwZi/IXL96B26H5uw53UVnoeMHOHl6yXS3Ez9s5yO9fP9jYi/doVldqFiEE/W1eMrnqoXp60L+esvA47fS1eM3JYjxGf5sXn6u54UX9/ZqKmZP7tYPt2G2CR0/Vt6IePjHFrnUhbtjayUwiy/CseRxkLWHfmWn+7rvP8PE9J+vu97mHT5PNF/jDGzfX3EcIwT/cfjm7N7YVY0NLBauC20JjOHYf/OdNkFd3nsfHY3z9sfPc+dyN9ISqLRt98ZmMVpPFSERXFtUL05j22DqnYeFMTrOxw8+ZqUTpTjU2Af4ujo/HCLoddAXdeF123n7rdh47O8N7v3OQqze08tLLDQFZE7IAuHWLsoH+723PbtwHdpfIAsDtsJE2IYuSsigPSOpk0eZpA2BTp79cOWk4OhZl2xIEiYvvlwm5j0VSbO4KcFlfiL114hapbJ79Z6e5bksHu/qU8qpMp16LuOdXZwG4//BYTaWVyOT43CNnuHVnD1tM0peNcDvsfPYNz+Yfbrt80a+1HiyysNAYhh+HkSchrb78H73vKF6nnT++eYvp7p3B2ouPHjCdTmTIF8q/PLo/3mWLlTYmwgx2+Ehm80xE01DIQ3yiqCy2dAeKi/zvXNPP5i4/qWyBv3zZrvLF3xCzMMKejQMCrz9Y9VhNaDaU/v/hdthNySKRMY9ZTKemEQhaXOo8mzr9nJqMly0muXyBk5NxtvXM47ouEB2abVhpDaZzeSZjGda1eLh2sJ0nzs2QzuXNTsH+M9NkcgWu29LJznVBhBW3YCaR4btPj9ATcjMWSXOwBnl+dd95ZhJZ3nhTbVVhhN/tWD0BbgurDFntrjeXJpXN872nR3jtczbQUSPn2++y43Haqsgimy8wHk3T4nVSkOrLZMS4RhZtwvClSobZ0KEHgROqeE7mIdDD8fFY2Z2Yw27j3193NR/6nSu4ZmNb+UXZXWUxiyIyMXAFYD5fPk+1ssgXJLl8OWHE0roNVa0s2jxt2G1q+2Cnn0gqV2bNnZtOkskVlkRZdBVtw8r3Q71/vSEP125qJ5Mr8NR5cwJ4+MQkDpvg2k3t+FwONnX6ay6OawVff2yITK7AP//OFQgB95ukk+fyBT71i5NcvaG1sXjZRYJFFhYaQ0aLF+RSjMymkJKyFNVKCCHoDLiZrFh8JqJppITL1qtjK+MWo5EUdpvAnzMsSMlpBjtUAP30VFylzQIJdztjkXRV8HdHb4jbrzFJM3V4IG+SrpuJgXueC3LRhtKUhVN9lSrVRa1sqOnUNG3uEplt6jS8Pg2lTKjmK4t2v56QUP7/oyu9Xk1ZAMXU2Eo8fGKKKwZaCWiv9dK+ljVtQ0kpuWfvWa4YaOXGbV1c0d/KT03I4gcHRjkXTnL3TeYqfbnAIgsLjSGjLWL5TLFOorelfnqpIovyxUfPhLpU87QryWQskqY76MaWDINDyzNPhFnf6sVhE5ydSqjgNjCUVQv2lq7qamRT1IhZkI6Bq8Fz6DCxoaBxsginwrR7S3eRmzoVWZ2aLAXxj40vTSYUKEXW5nNW1b7o79e6Fg/tfhfbugOm9RbRVJanzs9y3ZaO4rZL+0IMzSSr1ONawf4z0xwbj/HaZ6vysRfs6ObJczPKStUgpeTje06wucvPC3f11DrVsoBFFhYaQ9GGShWzmebqU9MZcJd9MaCUCXVpn1roKxensUiK7pAHElMQ6lNqIBnGYbexvs2r7rxj6u7sREIt8A0vpg6PsqEqg4y6DTUfuLW7fYMNBVT5+XHNhvI5TWwog7Lob/NitwlOTZZiNcfGoqxv9Rbv1JuNzoC7KtV5THu/erQbg2s3tbP/9HRVrElPV35eBVnA2g1yf2nvWQJuB6+4og+AW3aqXlkPHCmpi18cn+TgcIQ33rS5ob5bFxMWWVhoDEUbKl282+w1yYIyoivoqlIOuirRlUWlDTUWSdEbciuy8HWAtx2SKrd/Y4dfpc9qZPFMxI3TLthQUeNREw5ltZDPVry2eGnxbxQ2O7iC1TZUtlxZJDI5PE5bVRX7dHq6mAkF4LTbGGjzcrpCWSxlu4xaStDvshPUCOs5m9qJpnMcGikngIdPTOF22Lh6Q+k17dJsyrUYt5hNZPneUyO88qq+YtrzrnUhekMe7j9UIouP7zlBT8i9IirdLbKw0BgypQD36GyKVp8Tr6t+f5rOgJtwPF12Fzo6m8LrtDPYoQrRKslkdDalUnETYUUWvnZIaGTR7lMZQ7ExcHg5FJYMdvirFuKacGjkVhnkTkfnb0OBsqLmsKFi6VxVQV62kGU2PUuHp6Ns+6CWEQWQL0iOj8eWJLitoyPgqqqqH40k6WnxFDNv9LhFpRX18Ikpdg+2lfUs6gi46Q151mRG1NcfP086V+A1hkaZQghu2dnNz49NkM7lefr8LA8dn+IN128q1QItY1hkYaExGLKhRmZTDbVK7gy4KUiVIqtjJJJiXYtH88hdhA02VDKTJ5LKaWShK4s2SKqFaWOHj2gqR2ZmFALdnJiIz+/Ou0gWFXGLC7GhQGVEzWlD5ariFbNpdYxRWYBWazGl0mfPTydI5wpNb/NhhLKhqm3DdYbYVF+rl/Wt3rIg91QszaGRCNdt6aw656V9oTWnLIyBbV1B63jBjm7imTx7T4X5+J4TBD0OXvucldE92yILC43BkA01GkmWLSC1YFboNTqbKgbG2/2ussI8vSCvN6jZUH6NLLQ5E4Na+mxmdpSCv5sz4cScBUxlsOs2VCVZxOefDQUqI6pIFjUC3Jl83eptIzZ1+klk8oxH08U2H0uRCaWjK+gmms6RypYIb3Q2RW+o/MbgOZva2XsqzHeeHOY7Tw7zyZ+fAiiLV+i4tC/EiYlY2TlXO85MJTg6FuO3r662lq7b0onbYeMzD53mBwdG+N3nbiTocV6Eq5w/liZyZmHlo5gNlWZkJsWz+lvr74/qtApafyitkHp0NsVzNqtFssNfbnsUq7d9BbWg+zrUYlyMWajYhIyNkejYTL4gF0dZpC9UWbRAbBSoHbOIp3NzVm/r0Mnw1GSco9qwpKWMWXRo6bNT8QzrW73kC5LxaJrelvJamhu3d/L1x4d46z2Plx37LJNBV7v6WihIODwa5cqBuT8zqwFHtJTny02+I16Xneu3dnL/4XFcDlvj7WWWASyysNAYNBsqk04yFfewbo7gNlRXcecLkrFIii2+BHz7rfT6XsOB8RJZ6Dn9fS6NmHwdiiiSYZCSAa3hnjMxwUSHGmc0L2Xh0BY9Y8yiUFCv7UJtqMmjQB0bKpOn1Vt+52hsImiE3n321GSc42MxekMeQkt412ls0bK+1ctULE2uIOmtsBxfeeV6rhpoI1coEWOH320aO9Izog4Oz64ZsijWx9Qg+lt2dHP/4XFuv7p/7u7GywiWDWWhMWg2VDSmFvK5aiyg2obSF58r0/vhsc+x03a+LBtKVxaddi19VM+GKuQgHcXjtLM+6MCdm2U0r+yZLd3zCEybkYUei2mWDZXOmVZvQzVZ9LV6cdltnJ6Mc2w8tqTxCjC0/NDiSLWy3oQQDHb62dodLP60aaqkEv1tXkIex4qMW+TyBT7wg8NV6d9z4ehYjPWt3ir7UcfLLl/HSy/v5S23bF2My1wyWGRhYW7kc0WfPxpTC3kjAe6Qx4HLbit2ntUXnx45BUC7WzKdyBZbZIxF0vhc9lL1tp4NBUUr6vK2LDYkp9MB1rfOsxtrkSwMGT9pjZgWkg0lZf0At6s6ZmETNlrc5baN3SbY0OHjxERcy4RaungFGJWF+v/RlV4j8alaEEKwqy+0ImstjoxF+fieE3z3qfm1Aj86FmV7HaJv87v499ddUzbcaCXAIgsLcyNbakERTzSuLFTLD1dx8dHJoi0/of71qJTa6YSqexiNqLRZkVBkUlQWUMyI2hlUdRrHEn42N1q5rcMsdTajk8UFLMyekFI92WT9mIVJq49Wd6vpeNRNnX5+dWqKZDa/5Mqisq28XkDZyHtdD1u6AuXt5VcIpuPqc3l0rPEZ6Ll8gZMTcbYvYWLCUsEiCwtzI1P6oscT6vdG7zY7g6VCL73bbDCtipJanYosdNtjPJKiRy/IA6UqvFoQWMuI2uxVz//ktGv+wd+isjDYCjpZXKgNBZCaxaX59RlDI0EppZYNVW1DVVpQOjZ1+ommVIuQpayxABV89bvsxQy1kdkUTrug3WduMTWKoMdJLFV/wt5yhD7l8cho42RxJpwgky8saRbbUsEiCwtzI1NSFqlEnJDHUdOPrYSxKngkksJlt+GMK1kfcqmFNRwr2R7FGgthB3dLlQ21waW+uMP50PyC2wB2jSyMqbMLtaEA0hHcWjGaUVmkcwXyBWneRLAiE0qHnhEFLLkNBeXkPqa9HwttQxH0OMjkCysufXZai6cdHYs1PPFPD27Xs6FWKiyysDA3DDZUKpVoKF6hozPgMigLVWMhIhpZONXCOhnPIKVkLJJWwdTElCIJm81gQymy6LGpeMakbLkAZWGSOlu0oS4wdRYgFTGNWcRrjFSdS1kAdAfdtPiWPv++jNxnk3O2dGkEIY96/XPN715u0JMvYulcwxP/ji7RGNyLgaaRhRDiv4QQ40KIA4ZtXzbM4z4thHhC2z4ohEgaHvt4s67LwgXAYENl06l5edidATdTsQyFgmRkJsWGEMX4Q1Aji6lYmplElkyuUGoi6NMKvLxauqVmQ7XJWaLSSwr3AmwoY8xCz4a6gLt4gw3lsAlsojwbKp6uPVLV2ETQCJ0sljpeoaPDUChpLKBcCAIaWURXmBVl7DxwtEEr6uhYlIH25o/BvRhoprL4b+DFxg1Syjv0edzAvcDXDQ+fMMzqflMTr8vCfGGwobLpOtXbUsID74fR4v0BnQE3uYJkNpllJJJkh6/UVdVny2MT6g5uLGpI09T7QgHYnWpR1gjGlZwgLFpp8TqLRWQNwyxmkdYWgQXZUGoOd+W0vHimeqRqtpAlkonUVBY9ITftfheXmRS4LQV0G0pKyWgktaBMKB1Bt1JI0VR2jj2XF6YT2WJh6ZEGg9zHxmJsvwj24VKgafQnpfyZEGLQ7DGhupK9CrilWc9vYRFhsKHI1bnbnDkDe/5R/d57GVAqzJuIpRmbTbN5w0xxd1s+Q7tfdabVM2+KAe7ObaXzetuKNhSxcRKuDnZ0BOc/VrJegPtCi/KgrPNsOmtiQxmUxUxKvf5aZCGE4LtvvYHWi2BBgdb8MZEhHM+QyhZM56vPF0HdhlppyiKeYUO7D4fN1pCyyOYLnJyM8fwd3UtwdUuPixWzuBEYk1IeM2zbJIR4XAixRwhxY60DhRB3CyH2CSH2TUxMNP9KLRRtKGn34CRb+25zaL/617AYF+/MRqNk8gUG7NOl/fNp2v2qmaA+vrPURNDQlM7QH4r4OIMbB/nIHVfO/3WYps5qRHghysJgQ4Gq4i5XFtUjVWu1+jCib771I4uIzoALKeHQiFoc5xOfqgXdhoosI7L4+mPnGZpJ1t0nHFc3M9t7gxxugCxOT8bJ5uWqDG7DxSOL1wD3GP4eATZIKa8C3gl8SQhhOrNTSvkJKeVuKeXurq6uJbhUC/rdd9bdiptsVfuHIs5rZJEveb36bOenh9SC2sNUaf9chg6/imnoBWDdQWe5DQUq2K3ZUMTG8Lb10XchBU3FRoLGorwoOH1qPsV84fKrrC1Dm/LymEW1sqhVvb1c0Fnxfi1GzEJvWbJcbKjZZJZ3fuVJPvfw6br7TScytPlcXNIT4PhErGq+eiX04PZqrLGAi0AWQggHcBvwZX2blDItpSrrlVLuB04A25f62izUQFYpi6SjBTdZ+moqi33q3zJloS0+59Xi056f1DKcBOTTdARcKmYRSdHud+HOxUDmy8nC264IJJtSd/GBC5T5Qqj02cqivAuxoPTzeUIlG8phmzMbqlZfqOUC/f06MLx4ZBFcZtlQ57QCwRMT8Zr7SCmLyuKS3hCZXIEzcxQWHh2LIsQ8+5WtIFwMZXErcFhKeV7fIIToEkLYtd83A9uAkxfh2iyYQbOh4rYALpEzX0DyWRh5Uv1uIIsWrxOHTRQXn2B6FFrWF+dhd/hVau1YJEV30F2ym6qUxTTEtQlj/gV4wg5PRcziAtuT6zD2h3LayuoszJTFdFqRRT0b6mJC7w91cGgWIVQK70Khv/7lkg2lk8XJiVjNfZLZPOlcgTa/i0s0pTBX3OLYeJQN7b45h4KtVDQzdfYe4O0WEukAACAASURBVJfAJUKI80KIP9AeejXlFhTATcBTWirt14A3SSmrp8JbuDjIxMDpI15w4RNZ8/77YwdLd+yGojebTdARcBFN5XDYBK7EKIT61R1+PkNHwE0kleP8dFKRkLHVhw5vm1qQo6odOIEFDLZ3uMqVRTp2YfEKHRXT8uaKWUwlp0z7Qi0X6Mri9FSCzoAbZ6NTCOvAabfhddqXjQ11blqRxdlwgmwNa0mvsWj3qU4BQsydEXV0bOn7eS0lmpkN9Zoa2+8y2XYvKpXWwnJENqHIIm8jaK9RhatbUK5g1djSzoCbsUha9X2KDMHAc7RFO017m7qTPT4e44r+VkgMqYN8BpvG2w5ImDii/r5QGwo0ZWGIWWRiF9YXSoenpa4N5bCJYisQUMqiVl+o5QC9+WMmX1iUtFkdAY9jGdlQKrCdK0jOTCVM63X0vlBtfhdel52N7b66PaIyuQKnJ+O8cNcCbmSWOZbnJ9bC8kImAS4/kZwDn63GF37oMZXB1LG5fDGmdLeqCvKmIdRXVBZ6tlSuIOmppSx04pg4rP5dEFmYxCwWy4aqzIbSmggaU3ynU9PLNl4BKnVXt6IWo3pbR9DjWDbZUOemE0UCr2VF6X2h2v1KRW/vCdbtEXVqMk6uIFdtcBsssrDQCDLKqolmbXhEDSvh/D7o362RQHn/f50sdvq0L1tLf0lZ+EueeE/IDfFJ9UelDQUlZeFfQBZcZcxisW2obLkNFTCp3l7OZAGl92sxgts6zJoJJjI5Pr7nxJxZRouNs+EEz96k3oOTk+ZBbr0vVJvWRPGS3iCnpxI1+1vpquNiVd4vBSyysDA3sgmk08ds1oYLE7JIzaqJceuvKQaujegMqi/cZrc2p6KoLNLFu1ig1BfK7i5fwL0GZeFtKxXXXQjsrnIyW0g2FJRnQzmrbShfRbCzXhPB5YKislhMsnA7qmIW9x8e5wM/OMwT52ZqHLX4KBQk56eT7FwXpCvo5sR4DWWhxyy0LgHbe4LkC5KTNTKojo1Fsa3iTCiwyMJCI8jEydi8pKQTpzQhi+HHAVmTLPRai2JBXmi9piwyZS07eoytPozV2T5tcZ09t7BMKKiRDbUA68AdUsqiUKiyoWImsyymUlMrR1kssg1VmQ01phViGqclNhsTsTSZXIEN7T62dPlrK4tEBpso1Yhc0qtlRNWIWxwdi7Gxw4/HuTozocAiCwuNIJMgiZsMTuwFkxGT57Xg9vqrqxdjDIuP0BLcDMoi5FGptWCs3u4oO75oQ8HC4hVQHrOQsmixXTA8LYCETLQqGypRMcsiW8gSzUSXvbJojg1VHeAe1woxjQ37mg19CFN/u4/NXQFO1IpZxFVBnt6efbDDj9MuamZEHR2PLvn8kaWGRRYW5kY2Tly6SePEXsioRdaIocegY6ta1CttHpSEd9gEvXJSEYHTqy3aGWw2QZvfhcMmlMrQ25Mb4W4BPXtoIWmzUK58sgmQhYXbUFBsU17ZG8pYkFfsC+Ve7spi8QPcAbfTRFkosgjHly6lVq+xGGjzsbnTz0wia6psphOZsrniLoeNzZ0B9p0OUyiUf/7vPzzGyYk4Vwy0NvfiLzIssrBgigNDs7z9fx5Xw+ozcaIFN2mp1VcY22VIqdJm1+9Wf5vYULv6Qhz4mxfRkp1QqgLKSKXD76I76FZ3cYkp8HeWHa/mWmh344uiLLTrK7YnX2A2FEBqVotZlHedNW314V3eZPGSy9fx1lu2lg1iWih0ZZE3LLQlG8pErTYJetpsf5uXLZoSMMuImo5naato5viqawd49PQ0//ijw8VtJydivO2eJ7i0L8Qf3LCpiVd+8bH6mq5bWDDOTMW56zN7mYxliKXzfDKTYDbnpKD3VsqlSkHm2fMQG1OZUGBKFoDyciND0DJg2E+RzoZ2X7Gdt6kNBVrLj6lFIAtPyYYqtidfiLIwTMtzdJErSHL5Ag67jXg6b95EsMYsi+WC9a1e/uyFlyzqOfWWH/FMrhgH0NvSL6mymE7QE3LjcdrZ0qne9xMTMXYPlhP4dEJ1nDXiDdcPcnoyzn/uOUlvyMPv7B7gjZ/fj8Mu+M87r1nV8QqwyMJCBSaiae789F7yBcld1w3y3w+fAk+ccM6Fx+uDNOV1FHqn2fVXq39NUmeL0AvyoExZ/OPtz6IgJeRzkJqpQRa6sligDWV3lZTRQtqT6yizodS1ZTSyqAxwrxRl0QwEDQOQdLLQOw0vdcxioE2RwPo2Ly6HzTTDKRzPcGWFrSSE4L2/cSljkRR/+91n+PpjQ5ycjPP5Nzyb/jZf1TlWGywbykIR0VSWuz6zl4lomv+661r+78t3cf1gAEGBc1HwebUvhLGobWifIoiey9XfNZQFmYQqyGtZX7Vfm99FR8BdmllhRhZ6HGNRsqG0619Ie3IdetuO1GxptGq2QDZfIJMrmDcRXOYxi2YgWNF5NpbOFQPeU0uYDXU+nGBAUwx2m2BTh7+qoaCUsipmocNuE/zLa67i6g1tPD00y/9+yQ6u29pZtd9qhKUsLBTx/u8f5sholE++fjdXbVB38h98xRb4JAwlbGzr9sMM5WQweQw6t6tUWCiRgJTl6a/a3G1CGlkY7/B1FKu3TRZT/W58MWMWaU1ZLCR11mhDaTZEOlfAZjJSNZwKYxd2Qm7T7vurGpUDkPRMKKddFAvgmo1MrsBIJFUkC4DNXf6qyuxYOkc2L2n3mU9i9DjtfOb3r2X/6WluvmTtjEmwlIWFIk5PxrlqQyvPv6S0IK/3q4BkAjdBv3YHbrSZEuHyxd3uBqTqQmtERGsyHKpWFqVzmbT60LFYNpQxtTezGDELQ4BbVxa5vOlI1XAqvKz7QjUTgYrOs3pwe0tXYMnIYngmiZQw0FaahbKlK1DVUNDYF6oWQh4nz9/RPf9pjSsYa+9Ta6Emktk83soJbZpV84rd27juEm2hN9pQyQqy0APflXGLorLQs6HcdZSFCVn07ILgOvPH5gOHW83LyOcWx4ZyuNVrSUdwO0rKwrQ9+Qqo3m4WdBsqotlQetrsznUhoukcmVzzW37oNRaVykJvKKijsi+UBQWLLNYC0lGIjs25Wyqbx1eZ0aHNsrjp0o10tWp30cYAdyJcsojAMOe6gghmtW6yRWXhmp+yuOp34Z2HwH5hzmkmn2EkNmK4vpTBhlpgMZWnpSpmofvxgQobqsOzQLJboagcgKSTxQ6tMnopgtx6a/INZWRRnT5b2RfKgoJFFmsBP/07+PxvzblbIpOvHtySNdx9V86wLhS07CUzsihvU05kSCvI085hd0Mhq85RvACtiWCtbKEFSP6vHPkKt337NvLF9N/04mRDQbE/lNtZsqES2iwLY2+o6fRaVhbVNpTfZS8u3PNt+fHIySk+et/ReR1zLpzEaReqU4CGzV1KVRrbflT2hbKgYJHFWsDseYgOz7lbMpuvzhXXrRqnrxTELgaIZ1UFtLEdh72WDTVUUhVQOle+QqW4AiVCWUSMxEeIZWOk9HhBXiMLuxvsC7QbtP5QRhsqVmP+9lolC6/Tjt0mitlQY9EUPSFPMS4w37jFFx45w0fvO0Yi03jb83PhBOtbvdhtpZuOkMdZ1VBQVzn1YhZrERZZrAWkIyXLpQ5SmXxVl9QyX19XFjoR6CNQTW0ok5iFkSzMSMWs1cciIa4ppJRNe326DbVQCwqqbahcvriI6WSRzau+UMu9iWCzIIQg4HaUZUN1h9zFu/fwPG2oQyOq0+/xGl1jzXBuOlEWr9CxudNfpSwcNkHQbSWLGmGRxVpAalZZPpVxBAOklCSyebyVyiKrBf5c/tICrxNBUmstbWpDVZDF7PlSjUXZfkZlUaN6exEQ1TKfUvpdpW5DLdSCgmobKlsgli4fqTqbUe3ZW92ru39QPRg7z+qTE/W4wHxsqFQ2zyltca83kKgS58I1yKIrUB6z0Gos1lKmUyNo5gzu/xJCjAshDhi2vVcIMSSEeEL7eanhsf8thDguhDgihHhRs65rTUIbzlP06E2QzUvyBVkdsyizoSriEUldWZjZUMbRpXEV29AzoUDVWYCJsmhOgVMsq157ShjJIr44ZKHZUPr0tbJsKC27LKK9B8t19vZSIOhxEknlkFIyFtFsKK3/0nzI4shoFL3F1LEGlUU0lWU6kS1WbxuxpcvPdCJbtMLC8UzNGou1jGYqi/8GXmyy/SNSyiu1n+8DCCF2Aa8GLtWO+XchxOputLKU0IbzFHshmSCpdUutUhZmNlSuERvKEOCOjat/A70m+1WSRXOURUwjyjJlkY4urg1VLMrLk0jnEKIU4I5k1HsQXMi87xWOoNtBLJ0lksyRzhXoDrpx2G20eJ3zilnoFlSrz1k1X+KHB0b4/c/sRVZ0RtYbCA60e6lEKcitPiPT8SxtVtpsFZpGFlLKnwHhBnf/TeB/pJRpKeUp4Djw7GZd25qClAZlYT7oBSCpZe9UZ0MlwOZQSqAywK0rC1MbqkJZQHmltL3iXFBd4LeIiGY1G0rfkEstng214Xlw5Wtx2xURqQB3Hr+rNH9bJ4uQa+1Vb+vQbSi9gaCeldTud82r5cehkQh+l52btnVxtMKG+vKj53jgyATj0XIb1CxtVsfmYkNB9TkNJzJW2qwJLkbM4i1CiKc0m0r3L9YD5wz7nNe2VUEIcbcQYp8QYt/ExESzr3XlI5uAgpYxUseGqqssnH6VtlqZOpucBkSp5QWYKwuz4rfK4r18Vl2ftznZQnHtGlK6DZ3PaDbUIrTh3vlyeNmHSsoiWyCRKR+papGFgSwi5WTR5nPOq87i0EiUHetCXNIbZHg2VcywyuULPHpa9d+qHJdqnGNRif42L067KDYUnKnRF2qtY6nJ4j+ALcCVwAjwofmeQEr5CSnlbinl7q6utdOX5YKhW1BQ34bSlIVp6qxL+4LZK9JdE2FFFDbDMWZZTsV6Bn/1froC0YPlnuYEgEvKQrMnitlQi2cL6amzmbxKnTUW5Okxi7XYF0pHQJtpobf66Ampz0C7391wm3IpJYdGI+xcF2R7j3rv9LjF00OzxZTl4xUzKs5PJwm4HbT6qu0lh93Gxg4/JydiFAqS6UTWilmYYEnJQko5JqXMSykLwCcpWU1DwIBh135tm4ULQL4g+ZefHlOtFdIGsqhnQ2XVl6wqdTabKC3yQqhF3hjgrrSNilaViQ1VT1loU+SaoSxyhRzJnPKsU7qXnUur3lCLYUNpcNoFQkA6q4ryfO5qZbGmYxYeJ9FUtqgsuoO6DdV4zGJoJkk0lWPnuhDbe9R7p1tRvzypOgC4HbYqZXFoJMKWLn/NDCc9fTaaUgOaLGVRjSUlCyHEOsOfvwXomVLfBl4thHALITYB24C9S3ltqwkHh2f58E+O8uCRiXJlUc+GyqhKatNsKKdBujs8BjUwXV1tXWlVQXn6bXG/GsrCu/jKQq+xAEiRL13fYtlQGoQQarSqVpRnbE8eyUTwOrw4bWs3cBpwO8jmJefCCUIeR/Gz1uZ3EY5nqoLSZjg0oohhR2+IgTYfHqeNo2Pqc/3LE1Nc0hNkR2+wTFnkC5Knh2ar5lMYsbkrwJmpOBMx9bm1+kJVo5mps/cAvwQuEUKcF0L8AfBBIcTTQoingOcD7wCQUh4EvgI8A/wQeLOUMl/j1BbmwFRMLcCJdE5VWeuoU5hXN2ZRtsi7SkSQCFcrAdPUWZO2GpWps/osiybYUHqNBUAKrb1IKqJiOYuRDWWA22Evps4aq7ejmeiajlcAhLSWH8fHY2UtN9p9LjL5AnHNCv3WE0O84EMPmjYXPDQSQQjVU8pmE2zrDnJsPEomV2Df6Wmet6WDLd0BToyXbhCOjUdJZPJ1Z2Rv7vKTzUueOq++L1aAuxpNK1GUUr7GZPOn6+z/PuB9zbqetQQ9sySeyTesLPSKY9NsKCMhGFt8J8PQVTF+c74B7lzzbagyZaEH+/U+VItsCylloWwof0XMYi3HK6DUefb4RIzL+kpJEe2Glh8Bt4PvPDnMiYk4B4dni3NVdBwaibCx3Vf8v93WE+AXxyZ58vwMyWye527u4MREjK8/NkQ0lSXocfLkOfXZqqcstmjps/vOTJddk4USrAruVYipmFqAlbJojCxSdbOhjDaUYWxqcsbEhqqTOusw5LhXBst1ZdEEG6pMWeiCVe9wu4g2FIDbaSt2nQ1UxCyCzrUbr4BSB96ZRJZuLbgNpYU5HM+Qyxf41UmVkr1fW7iNODQSYee6Eulu7wkyHk3zwwOjCAHP3dzO1m69k6z63D1xbpaQx8FgR+33Wk+f3a9lU1nKohoWWaxChM2Uhc1R34bK1CKLRHUWUy6tUl3TkWolYHMAoiIbSku/tRk+bpXKoonZUHr1NkC6qCw0smiSDZVI5/BVxCwsZVH6/zDaUG0Gsjg4HCGqZTQ9dracLOLpHGfCiTKyuETLiPrKvnPsWhei1ecqkoXeN+qJczNcMdCKzVa7fUeb36WK/MbVjYWlLKphkcUqhG5DJTKashA2Nbu6TjZUIlurKK8yZqFlQxXnZVcoC70eo9KGqryDr4xtpGaUJXSB8yrqwagskvkU2JwQ15XFYpOFjWQ2T7zChrJiFiUbCqAnaFAWhv5QD59Q78v1WzvYf2a6LOh9eDSKlJSRxTYtIyqayvG8zar6f0O7D4dNcGIiRiKT4+hYtK4FpWNzpx8pweWwVWcFWrDIYjWiqCzSedVE0B1Sd9CZ2nUWqUweISh2Ti3CNBsqbWj1YRJjcLiqbahKsqiqBp9uigUFJWXhtDlJ5VPqNSSaRxZ6gZm/oijPIgtzZdEe0GIWiQy/PDnF9p4AL7q0l7FImqGZZHG/w6NKJesDkwDWt3qL/8/P26LIwmm3Mdjp5/h4jANDEfIF2RhZaIOQ2n1WE0EzWGSxClGmLFIR1RXVFZgzG8rrtJd/SQp5pRCqsqHStZUF1FAWFYtyZfFecqZ5ZKHFajq9naRzaaWOmmhD6TUDurLIFXLEs3GLLAxk0W0gi6DbgcMmGJ1N8eipMNdt6eRqLbBtjFscGokQ9DjoN8zQFkKwrSeITcC1m0qfxS1dfk5MxIrB7XqZUDr0HlFWjYU5LLJYhQjH1QIcz+SVDeVu0ZRFHRtqrlkWOnQiMOs4q6NyvnYmVqoCL+5TUbyXnG5a9XYsG8Npc9LibiGZTyqy0K9/sZWF01Ykaz2gq9tgaz1mYaxo7zEEuIUQtPldPHBknGQ2z/O2dLCjN4jPZecxjSyklOw7Pc3O3lDVXf/Ln7WO376mn5DB5traHeDMVIJHT4fpb/PSGXAzF/Qgt1VjYQ5ruscqRFmdhTAoi8TZmseYTsnTi+mMNpTdpYjArOOsjsr52pm4ugYjbDYVOzBWcHdub+TlzRuxTIygK4jb7iaVSymykFoOfxNsKH1mQ2XH2bWuLBx2G16nnWQ2X6ze1tHhd3F4NKoymjZ14LDbuHKglf1akPuHB0Y5PBrlg7c/q+q8f3jj5qptW7oC5AqSB49O8Ou7ehq6Pj19ttXKhDKFpSxWGVLZ0vxnpSy0mIUrULc3VMps8FFdZTGXDWUgi2zCPEXV4S6v4G6SDRXNRgk4A3gcHtL5dKnKHJpiQ+moUhZrnCxAWVEdfheuitiYnqp6aV+IFq1/0zUb2zg0EmU2meWffnSE7T0Bbr+mv6Hn0TOiMrkCVzVgQQFs6PBhtwmrL1QNWMpilcHY6jmRyYGMQHdILdaLYkNpqbPJsEqTNbszt7uqGwnOtV8zbahMjIArgNfuVQ39dAvM5iyl8C4SjAkCPnf54KO13BdKR9DjwOWozjTSU1Wv21IafnX1xjbyBclffuNpTk7G+dTv7S6bn10PerAaGotXgCL6f7z9WTyrf+0OqKoHiyxWGcKaBdUVdKtsqEJEKQunZ47eUA3aUDpZJMLKgjLLGqlUFrV6MOnnyiYVaTSpPXksGyPoDOJ2uFVDQV1ZLHJBHlAcrQoUi/IsG6qE/jYfIW91TEAfNqRnNAFcPaA+D999aoRrB9t4wc7uhp8n4HawrsXDeDRdVi0+F367QeWyFtEwWQghNgLbpJT3CSG8gENK2fgAXAtLgiktuL2h3cehkVnIazELvVtsPmday5DK5quzQGraUGnzjrPFfVyqmM94Hmf1HIFi/KOJ1dugyGIgMIDHXmFDLWJ7ch1GG0ovyiuSxRoPcAP82+uuxkwc9Lf58DrtXDtY+ky1+Jxs6w5wbDzGu1+yY97prJf2heiJZaprhyxcEBoiCyHEHwF3A+2oeRT9wMeBFzTv0ixcCPQai4E2L4fOjIAnr82c0N7qTMx0UU5m8/Q57XDsPhDA1ltr21D5tHmrDx12N+S0AHgx/dbEhipaWs3rCwUlG8rj8GgBbu1Oc5GD21BuQ+mps5ayKMGYEWXEXdcN8rLL11U9ftf1g5wNJ7hm4/wnKH7wt68gX5i7k62FxtCosngzavbErwCklMeEEI1rQgtLBj0TaqDdRxDt7t4dKtlFNcgikcmrO7AH3w8IRRam2VBu1a01PgHtW8wvwmFInTUjHOO58plSE8EmxiyCriAO4dCK8ty1r2mBMCoLvyEbymlz4rYvbnxkNcHjtDNgMvL0dc/ZeMHntFp2LC4aJYu0lDKjy0AhhAOwKHsZYiqewWkXdIc8BIW22HtCahY31CzMK2ZDJaZKaaXFhd5wB64vtNER6N9tfhG6Yig7h1nMoqLArwk2VEEWiGVjBJzqNaRyKaTbjYBFz4SCUszC7bDhsKvfI2lVvW1VBVtYyWiULPYIId4DeIUQvw78CfCd5l2WhQtFOJ6m3e8i4LYTKiqLFtC7rdbIiEpmdLIIqyaBUhoW+ooAN6g2IrVsKFOyMMuG0pRFE22oRDaBRBJ0BckWskgkGYcLd61rWiB0G6pspKrVRNDCKkCjdRbvBiaAp4E3At8H/qpZF2XhwhGOZ+jwu/G5HASF1lfHEyrd2Zv0h5JSksjmCTgKquI7l1SLfK1sKB21Fne7oY15cfCRSYC7Ulk0seOs3+nHY1eB7ZQev2kKWSjryThS1WoiaGE1oFFl4QX+S0r5SQAhhF3blqh7lIUlx1Q8Q0fAhd/lKI9Z6L2aTGyodK6AlNAqDEQSH1eE4fCCzZBNYixoq5kNZUidNRupqsPuhvy0ilkIm7rORYbeFypgIIak3UELNMeG0pRF5UjVds/8A7QWLCwnNKosfooiBx1e4L56Bwgh/ksIMS6EOGDY9k9CiMNCiKeEEN8QQrRq2weFEEkhxBPaz8fn+0IsKEzFMrT7Xfjc9vKYhZ4mqttCR34Ij6j/Zn3wUasxEzo+qdVH1OjpBHVsKFdjNpTenTY5o2VsLX5DAV1ZBJ1BPBrRpe322te0QOgxi6opeZaysLDC0ei30yOlLN6Sar+b+Apl+G/gxRXbfgJcJqV8FnAU+N+Gx05IKa/Uft7U4HVZqEA4rsiiSllU2lC/+g/42T8BFNuDtEjDVL3YuFIFzsrW4gZlUc+GKmShUDDYULWURbqp1dt6q42AK2CwoZpIFpoN5a+MWVhkYWGFo1GyiAshrtb/EEJcAyTr7I+U8mdAuGLbj6WU2qgyHkHVa1hYJKRzeWLpHB1+Fz6XnaBIUhB2tVDrC6NuQ00cVXOoc2mSmrIIFGZLJ9NtqKo5FI3YUIb243WzobTeUKmZplZvQ7mySOoKpok2lF69rWdjWQFuCysdjcYs3g58VQgxjCrZ6gXuWOBzvwH4suHvTUKIx4EI8FdSyp8v8PxrDnpBXkfAjd/tIEScjCOARwgtSC3U4p2KQHRYHRQdJZlRd/X+vJEsathQjkZsKMPIVJ0sKhUKlHpDNXHwUZmySGk2lNDIognKQm+Qp1dvx7NxCrJgKQsLKx4NKQsp5aPADuCPgTcBO6WU+y/0SYUQfwnkgC9qm0aADVLKq4B3Al8SQph+u4QQdwsh9gkh9k1MTFzoJTQVU7E0L/jQgxwbW4JuKJPHYOyg9ryKLNoNyiJt1xZEm01rJhiDqWOl46MjRWXhz2lk4QoYbKhKsmjAhjIji3rKItk8ZRHPqufXu84CpPRyh6YU5ZWnzlrV2xZWC+YTUbwWeBZwNfAaIcTvXcgTCiHuAl4OvE5qA3allGkp5ZT2+37gBGA63EBK+Qkp5W4p5e6urq4LuYSm4/h4jBMTcY4sMllMxdJ8/pEzZXOJ+cFfwHfeDhiUhd+F22EjJBKkbIYFUW9TPnG0tC0yTFKLWXiyM6omI7hOVWibTrjTlIXTp5oTmsFeYUMJu3l3V6OyaGLMwi7seB3eYswiqRfHNbE3VHGWRdoiCwurAw2RhRDi88A/AzegSONaoEb5bt3zvBh4F/AbUsqEYXuXlo6LEGIzsA04Od/zLxfMJLMApLOFRT3vv/z0GP/nmwc4PDbLu/a8i2emnlEKQBsRqjcRbPerGcKttiRJI1no0/Imj6DcRMqUhTs7o+IQgW4DWdRQFhUW1HhinDfd9yYmk5MGZZEpEU697rSp2YZsqDORM/zp/X+qOsc2iFg2ht/pRwhRyoYqKovGbKjPHvwsH3v8Yw3tW5kNZU3Js7Ba0GjMYjewS5bd0taHEOIe4GagUwhxHvhrVPaTG/iJ1vrgES3z6Sbgb4UQWaAAvElKGTY98QrAbEIji9zikUU8nePrjw0B8OTQeX5w+gdc2nkpuxJTxT5Mug3V4VeLdUgkiQvDoq7bUBNHoesSmD6tlIVHkYUrPQ2+DvB3wvihGjaURgS+ctvo0dFHeWjoIX50+ke8zqEtwrmUNsuiht3jcAFSVZc3YEM9NvYYD5x7gJMzJ7m089I594dSXyigZEO5/KquI7SuoXP8+MyPeWriKa7supIb+2+su69lQ1lYrWiULA6ggtojjZ5YSvkak82frrHvvcC9jZ57uWMmqRbtdC6/aOf81hPDRNMqkezI+CSg+hwpVaE4PBzP4LAJQl71tgZJcNaY4ewKqjQI9QAAIABJREFUqmyo+Dh071J39ZFhkm3qOp3pMLT0gb8bYntUy4/Ku2+dLCoW95G4+mjsObeH1228TW3Mp7UpeTWyrI2N9RqwofT4w1Rqas59dehT8oBS6mxLH7zzMAQbG7cZ1+Iuf/PLv+Gbv/nNsgK/Sqxr8fK7z93Ar21XFqlOFtbgIwsrHY3GLDqBZ4QQPxJCfFv/aeaFrWTMLLKykFLyhUfOsKM3SH+bl5NhtVimMzG1IOczkE0Vayz0hnUBEkSNZOEOqPhA+JSadx3qUzaUFrOwJ8OasuhS6ayZaMM21HBMZVc9OvYocb3HpB7grqksGmgdYkAip5zLqWTjZBHPxvFrmVhu7flSuVTDRAHKytrRvoOJ5AQf3v/huvvabYK/f+XlDHaq57RiFhZWCxoli/cCrwTeD3zI8GPBBIsds3js7AzPjES483kb2dod4OyM6qWU1lt7A6QjTGlkAYCU+GSCWWkovHf5YfKosn26LlGB7MhwMWYh9IFGAS1xQBbq2FDVZOGyucgVcvwyekJtLJJFjTvxsmrwxpXFZHJyzn11GG0op81ZalM+D8SzcXb37ObOnXfy1aNf5eSMeTjt+PRxHjj7QNm2SCaCXdiLhGXBwkpFo6mze8x+mn1xKxWlmMXi2FBffOQMAbeDV165nm3dAUYiiiTSxqaAqYhqIhjQFuBMHDsFZgtGsgiUus8WlcUoyXQOvy2DyCVLysJ4jBF2t/L7fZ1lm4fjw1y3/jqCriB7ws+ojfnMHDGLeSoLrc/UvGyoTLTMNioOQGoQBVkoqpOXbX4ZACdmT5ju+5HHPsJfPvSXZdsimQhBV9BqT25hxaPRbKjnCiEeFULEhBAZIUReCBGZ+8i1iVLMYuHKIhzP8N2nRrjt6vX43Q62dgfISbXYlZFFepapWJp2v6GFODCdN9pQBt+8c5sii7wakbrOqSWn+TpUzEJHVW8oB7z6S3DtHxQ3SSkZiY2wIbiBG/pu4GdTT1IALcBdx4YyKosGYhYXYkMZZ1mARhbzUBbJXBKJJOAMsM6vAuKj8dGq/XKFHPvH9hPNRIukBiWysGBhpaNRG+pfgdcAx1BNBP8Q+LdmXdRKx8wiKouv7jtHJl/gd5+rJoZt7Q4gbCpFNm2cTZFSNlSHbkNpXvlUznD3ri/aLRvU70G1+DkTo/Q4tHP5Oko2FJjPzr7kJRDsLf4ZToVJ5VP0Bfq4aeAmwpkIB11ak8CMSX8pHWXKogGymKeykFKW2VAAbrt7XspCt778Lj8t7hY8dk8xmG/EoalDxX2NZGL1hbKwWtBwUZ6U8jhgl1LmpZSfobpJoAUNOllkFqgsCgXJF391lmdvamd7j1rwtnYFQScLwx1sNjFLNJUrkUVKJwtD4Zxux3Rp9Y6hPgC8qXF6HVrPqLlsKBPoi2efv48b+m7Aho09Pu/cqbN6NpTdZU5KFagVs4hkIkynpqv2T+fT5GSuTFl4Hd55kYXeWyrgDCCEoNffa6os9o7uLf4+mig9Hk1bsywsrA40ShYJIYQLeEII8UEhxDvmceyaw0xicWyoZ0YinA0nuGP3QHFbi8+J35vXzl8qTktEVVlKe6BcWUxk3aWKb71xXucl6l9NWfhSY3TZDGThCqg5FlA77dWAoZiq/+gL9NHqaWVzaANHXc5SBXfdOguUBdWAp1/LhnrPz9/DOx58R9X+xoVeh8c+PxtKT5vVA9Tr/OtMyeLR0UeLCmYsPlbcbk3Js7Ba0OiCfydgB94CxIEB4PZmXdRKRiZXIK6loi40GyqiZVWtb/OWbW/1qfOm86ni3Xkyou6sS8pCxSxmpa9EWpXKItgLCPzpCTp0svB3qoVbVxcNZPGMxJSyWBdQ5NMfWM85p0PVdBSycyuLBvtC6coikomQzav/m2why97RvZyNnK3a39hEUIfbMT8byjhpD6DX31tlQ2XzWR4bf4wXbnwhUK4srJiFhdWCRrOhzkgpk1LKiJTyb6SU79RsKQsVmNUWeFh4zCKlHe912su2B3xqeyqfgValOjJxRRZtvnJlEZU+4loxXzHdtVurfrY7wd9FKDtBu4iqLCdPi/YkGlk00GxvOD5M0Bks2i39wQGGHA6k1oakppWlK4sGO84a23zocYuDkwdJ5pKEU2HyhfL/b31KnnGxnm82lLERIShlMZmcJKNVzQMcmDpAMpfkhvU30OHpKCoLKaUVs7CwatBoNtTLhRCPCyHCQoiIECJqZUOZYzZZWkQWakMlM+p4TwVZeFxaAL2QVZlLriD5hFISbRUxiyje4nAjNj8f7vwm9BvaeoXW0ZKbpE1E1R2+PhhIz4hqwIYajg0XVQVAf2gjSZuNKd2OmauCu8EmgvFsnG6fui7dito3tg+AvMwznS6PW0SzmrIwxizs3nnZUGbKAmAsUbKa9o7sRSDY3bObHn9PUVkkc0lyhZxFFhZWBRq1oT4KvB7okFKGpJRBKaX1DTCBHtwWYuFkoY87rVQWTqd6jlQhp9SCJ0RBs51avE61UzpCQdhJ4iae0ZSFzQ5bnl8eHwitpz0/QauMqHiFDr9WR9GADTUcH6bP31f8eyC0AYDz+oI6V51FgzZUIptgQ1CdW1cWOllAdeDbbP72fG2oSmWhk4UxbvHo6KNsb9tOq6eVXl9vUVlYTQQtrCY0ShbngAPzaSS4VqGTRWfAvWAbSq+s9rgq3iY9G4q8WuDdoaLtVCSLVIS8MwgI4uk61xFcR0dhimChgiwCjSkLKSXDsWH6AiWy6A+oAYjnM9qd/lwV3A3YUNlClkwhw0BQ2W5TySlyhRyPjz3OjvYdAEwkyuebVC70MP8At044xgA3lMgim8/yxMQTXNt7LQA9/p4iWVh9oSysJjTaSPBdwPeFEHuAtL5RSlm/Uc4ahN7qoyfkXnCAW1cWlTZUpqC8+yxSLfCeEPbZKB6nrbRvapa8dkeb0JWFGULraCFGLjcBvitL26+6E0LrwemtfSxqQYxn42VksT64HoDzmg20GMpCr7EokkVqisPhwyRyCV48+GIOhw9XKQuzALfX4SWdS9Mo4tk4brsbp12RcI9f9ZTSyeJc9BzpfJpdHbsApTyi2SjxbJzZtFJ7lg1lYTWgUWXxPiABeICg4cdCBfS02Z6gp2k2VEK7Y04JUVQWjkyUVq+hIjodQWqLVH1loRb59sxQeb+n9k1MXPobFGT911CssTCQhdvupjsvOVfQ7uBrkYU7qGyutsG6zwGl4HaHtwO/089kcpJ9o8qCetHgiwATG0qPNzhKz++2u0nmG5+HYWxECIps2txtxdd9OnIagE0tmwDo9ZVsqgOTBwDY3LK54eezYGG5olFl0SelvKypV7JKMJPIYrcJOgIuDgwv3Iay2wROezmn6/UGGSGKMQtnLkZrwFnaKRUBz9zKohBYhw0QukrRX0dqhhff+2L+7vq/46WbX1rz+GKNhSFmAdAvBeeF9vprxT1cfnjbk1VNCc2gW0o+h48OTwdTySmGokMMhgbpD/YTdAaZSJbbUNFMFL/Tj91WIluPw0OukCNXyOGwzf3xr2wXAuXps2ciZwDYoMVpdOUxFh/joeGH2Nq6tRjnsGBhJaNRZfF9IcQLm3olqwQzyQwtXicep33BFdypbKFKVUBp4cwLQdbTCp4WPIVYKV4BkI5g08hCr/swQ8ZnaNVtIIvRxCiZQoZjM8dMjiqhssZCR7+0c14bMVo3/TbQVcrAqgPdhvI5fXR4O5hITrB/fD/X9FwDQKev01RZVC70Xq3YMJ1vzIqqVBZAWRX3mcgZ2j3tRatJJ4ZTkVPsH9vP9X3XN/Q8FiwsdzRKFn8M/FAIkbRSZ+tjJpGl1evEZbctPHU2m8fjLH+LcoUc6Xwal1SWU8YTBHcIbyFOq89AFslpbNo0u0S6trJIes3JQk9N1edU1MJwfLhozRjRj5Nxh0ONMG2gVmMuxHMlZdHp7eTpiaeJZqLs7lVpwF3eLtMAd2VwuTiHu8HRrLFsrGrYkbGK+3TkNBtDG4uPdf//9t48PsrqXvx/n9knk30PCUJYAgECEeLKIooIKqJWq225rXRx11rt9Upv76+t/dXWWmu9LlfrrVbrVapisdarVkW8uICyhX2HAAkh+zqZmcxyvn88z0wmycwkgYQQct6v17zmmfNs52Qm85nPbs9EIHjnwDt4A15m5iphoTgz6FFYCCEMwEIppUFKaVehs7FpcnlJijNjNZ+8sHB7/d2c26EIH/RGPlYH2BKx4CPdGtZ0qPkYxtRRmAwi5HSPhBM7ThnsUdEhLOrdWvmQHoVF6zFyHDndSnCPFNo1K0ymfhEWQc3CYXaQakulPaD5hkqyNGGRbk/vZoZqcDd0ExadGiD1AqfX2cnnAZqwaPW20tLewpHmI52EhdloJs2exva67diMNqZnTe/DKhWK05cehYWUMoBWdbbPCCFeEEJUCyG2h42lCiE+FELs059T9HEhhHhCCLFfCLFVCDEk/8uCmoXVZMQfkPj8Jy4w3F5/BOe29qWZhKZFtJnitNBZIMuim1YaDgMSQ9pYshJtVDVF/2J0+wIcl7rPIJKwcPZCWMR372WdZ9TMPeUWu5YpfpKEfBa6GQq0EN2g2SfDnkGdqy5UB0tKyf7G/d2cy0GzVEt4efcYtLa34rB0N0MBHGg8QI2rppOwAMjSTXvnZJ+DNbx1rEIxhOmtGWqVEOI60fcOLi/SvTrtMmCVlHI8sEp/DXA5MF5/3AI808d7nRY0utpJjrNgNWl/2pPRLtzeQFTNIg1tvNYL7Wbt13O6RRcK9Xont5R8spNsVMYQFq72cGHR4WgOJr3VtNWE6jBFotJZSa4jt9t4nkETFkdtsUNve0vQbBR0cAMhExRomoXb7w5FQFW1VdHoaQzlYASJlIEdC6fXGdHBDR2VZkcnjo64X5mgFGcSvRUWtwJvAO198VlIKdcA9V2GrwZe0rdfQmvXGhz/i9RYByQLIbr/ZD3NaWzzkmQ394uwcLV31yyCtvtgn7oapxOn0BLn0oxBzeKQ9pw6huwkG8ebowuLtnYfx9H9DeGahUt72yQyYpVV0LScRk9jRM0izeTAHghQbumfX9ahvhJmB+l2bfVB5zZoDm4gZIraXb8bIKqwiNSTIhKt3tZuDu5gYt66ynVARyRU13so57biTKK3hQQTdJ+FuR98FllSyuB/6nEg6GHNRcsUD1Kuj3VCCHGLEGKDEGJDTU1N192Dis8foMXtIyXOglX/kj+ZLG6X14+1i4M7+KWZruc/1LQ6aZGasEgx6k7b+oOaaSoulZxEG8eb3ERLvnd5/RwIjMBnTQ6Zs6Bzg6FopqigP6Nr2CyAMFvJ9fkoN/U2Ojs2wXBhm8lGSXYJ1xdczyVnXRLan2HXCh/WtmkRUbvrdyMQFKQUdLpOqi0Vs8HcK2HR7m/HG/B20yzS7emYhInS6lKAUAmSIIvGLOLmopu7macUiqFMr3tSCCEWCyEe1R+L+uPmevmQPpUQkVI+J6UskVKWZGRk9HzCKaTZrUUdJceFaRYnkcUdy2eRqVc9rWtz0hjQInySRFBYHNIS3YQgO8mGy+un2RU5Isrt9fO8/woOff3DTjWj6t31IfNKNCd3UIiEJ+SFMNkY6fVR3k9dT5xeJ3GmOAzCQKIlkZ9f8PNOmdFBYRGuWYxKHEVcl6ZKBmHQQl9bI2tLXe8JdNMsjAYjmXGZeANechw52Ey2TvunpE/hh9N/qPpuK84oelt19mHgHmCn/rhHCPGbE7xnVdC8pD9X6+MVaH0yguTpY0OGBj17WxMWQc3i5IRFNJ9FulczLdW7nDT4Nb9AvNA759UfhFTNsZuTpO2rbI4cKtrW7seDBXNyZyWu3l1PYVohAhH1V3hIs4gkLIwW8nw+KgyBqFpNX2jztnX74g8naIYK5lrsrt/NhNQJEY/NdmR36jkRjVDzpAi1rYKmJqU9KIYLvf3ddwUwX0r5gpTyBTSn9ZUneM+30SrYoj//PWz8O3pU1PlAU5i5akgQLCLY2Wdx4maoSEl5IWHh0aJ5GtraqNdbp8bTBn4fNB6BVL38RJLmM4jm5A4WK7RbOu4jpaTeVU92XDYZcRmhLO2uHHMew2wwh3wInTBZyfP6cCF73TM7Fm3etm6/8MNJMCdgNVqpddXS3N5MRWtFN39FkBxHTq/MUNE0C1DCQjH86IuRILw0aFJvThBCLAfWAhOEEOVCiO8DDwPzhRD7gEv11wDvAgeB/cB/A3f0YW6nBcFeFslxlpCv4WSyuCMl5QVt92ltjQA0up3UerXQVLvfCc3lWnc6XbPI1jWL49GERXt3YeH0OmkPtJNmT2OEY0RMzSLHkYNBRPgYmazk+TTTV3lLea/WG4s2XxtxpuiahRAilGuxp34P0N25HSTbkU11WzW+QIwCi4SVODd31yyCTm4lLBTDhd56H38DbBZCrAYEMIeOkNeoSCm/GWXXvAjHSuDOXs7ntCSoWSTbzSEhcVLRUF4/Nkt3zcIkjCT4tS/5Fo+LBleAFmkn3tei+SsAUjTNIjPBihC9EBZhGkwwxyLVlsqI+BFsqdkS8dzK1sqIkVAAGDuExdGWoxRnFkc+rpc4vc6YZijQHM+1bbW9EhYBGaDWVRuzblOkEudBlLBQDDdiahZCiGDs39+A8/XnN4ELpJSvDfDchhwhYRF38maoQEDS7otshooz2bHqfoAWj4tGlxencCA8LR05FrpmYTYayIi3RhcWXj+mLsUKg2ajoLCoclZ1a1kKmhkqN757jgUAJhu5Ph+CU6NZgF7yw1XD7vrdpNnSIpvH6Pii78kUFfRZRBJSZ2edTX5SPlPSVX1NxfCgJzPUE/rzWillpZTybf3Rs3dwGNLo8iIEJNjMWE4yGirYfzuSg9thsGDThUVru5umNi9tBge4m7QcC6MVEjp+8eck2aiMkmvh8vo7maCgI8ci1ZZKjiMHn/R1K6Xh8XuoddWGvni7YbJglZBusPU6pyEWPTm4QdcsXLXsrt8dVauAMGHRGntesTSLgpQC3r7mbVJtPVfMVSjOBHoyQ3mFEM8BeUKIJ7rulFL+cGCmNTRpamsn0WbGaBAnnZTn1oVMpNBZh8GMRRcWbV43ja52PEZdWNQf0pzbho7fAVmJNsrqnBHvEynxr6tmAZp/ItxkU9ZUBsQww+hlLrLN8VGT+vpCTw5ugIy4DJrbm2nztjErd1bU40KtUXuIiOraf1uhGM70pFksAj4GXMDGCA9FGI0ub6jy68km5YVaqkZIyouTmpQ3YMDtc1PX2o7XFK+1Vq0/FPJXBMnpUvJj05EGVmwsD92nm2bRxWcBdIuI2tuwF6Bb0lsIvWBftiWpV2GqPdFbMxSAT/piahYOs4MES0KPmkVreysGYQiVNVcohjMxNQspZa0Q4g205kcvxTpWoRcRjNNKh5+8ZhHFDOVz4ggEwGjBZLCC8HG4rg1fSgK4q6G1GsbM7XROdpKdFrcPp8eHw2rit+/tZtORBhZOyY6oWdS760m0JGI2mqPa9/c27MVisETXLHRhkWVN4bPGHUgpTzhJTUqpCYsezFDBAoNA1ByLIOFlxqPR5tO0GZVcp1D0ruqsH/jGKZjLkKfR5SXRpsnfk83gDkYpdRUWbd42HH4fJI7AbLCAwYvL6ydgSYTGo+BtC+VYBMlJ0vIwjje7aXJ52XC4Aa9f8vn+2oiaRZ2rLmSLt5vspNpSu2Vx76nfw9jksdG7zQU1C3s6Lp+LFm/vqrxGwu13E5CBXmsWdpO9WwmOroR3uwvnlV2v8Py25wFNs4jkr1AohiO9zbP4XAjxlBBithBievAxoDMbgrS4vSTadDOU6eTMUNH6bzu9TuK8HkjMxWq0IYSeK2BLBKnfq4uwyA4KiyY3n+6rwR+QCAGf7KmOqlmEO25zHDndhMXehr3RTVAQ8llkxWVq947xK77N28aXlV9G3R8rOS6cjDhNWBSkFHRqpRqJHEdORPPYir0reHnny0gpI3bJUyiGK70VFsXAZOCXwO/1x6MDNamhSovbR4KuWZiNAiH6wcEdIc/C0d4GibnYjFYQWriuwR6WJ9nFZ5GdqAmLyiY3H++qJjnOzGWTsli9u4a2dj9xEXwW4SadEfGdE/NqXbXUuetiCwu9B0R2glbBJZaweGPvG/zggx/wxbEvIu53efXy5D2YoVKsKZgN5pj+iiDZjmyaPE2hWlsA/oCfI81HqHPXUdVWFbEtq0IxXOlt1dmLIzwu6fnM4UWL2xsSFkJoEVEnmsEdcnCbOpfhaPO24fC0QFIudrMNDJpmYQwKC2GE5C4ls3XN4liji0/21nDBeBPWtE843tzG/urWbqaurppFMIs7WONpX4PWlzumX2D0bLjuebJHXQTEFhZBZ/mjGx6NmM8RLMvetWNdV4wGI0/Ne4pbpt4S8zgIi4gKm9cx57FQB77ttds1wdwPXf4UijOB3hYSzBJCPC+EeE9/PUkv3aHQ8foDuL0BEmwdXeGsJuNJO7jtlo63qD3Qjk/6cPj9kJhLnNmK0DULi0OvxpI8sltnOpvZSEqcmfe3H6fe2Y5I/ozVNS9hsJXT7u+c+OcL+Gj0NIYaDAHkJeTh8XtCEVHBL/fxKeOjL8BogqLryYjLxCiMMYXF/sb9JJgT2Newj7f2v9Vtf/DXv93cc1TShSMuJFM3fcUi6LgPn1cwHBg0YaE0C4Wig96aoV4E/gkEy4vuBX40EBMaqrTo5cmDmgVoTu7e+izcXj/PrTnAxsNa2GpQs7CaOtdsAogLBCApD5vJhtmkHxevNzDqYoIKkp1kZ2dlM0aD4KhrAwC52ZppKdzU1ejRak6FaxbnZJ8DwNrKtYAmLDLsGb1KSDMajKTb06N2pgvIAIeaDrF43GKKM4p5cvOTtLS3sKN2B2/ufROP39Ox7h4c3H0hUpRXWXMZoGlS22u342zv3iVPoRiu9LY2VLqU8nUhxE8ApJQ+IcSJl1M9A2lxa7/wO2kWZkPP0VCBAHvXvc1tnydxsK6NK6fmMGNUKp4I1WBDjl4pIXEEVqMVky4s4hJ1YZHaued0kOxEK7sqoWi0lwMtWv2o+OSjwHmd7lHn0hPy7B2CYEzSGLId2XxW/hlfL/h6z87trvd2ZFPljCwsjrUew+VzMS55HFfkX8GSd5cw97W5IXOQyWAK9YvoT2dzRlxGt/Lrh5oOkWRNYmbuTN479B5+6e/RT6JQDBd6q1k4hRBp6I2KgiXEB2xWQ5BImoXFaIhphmr3BVj+6vMUfHATE707GJFko0mvL+WKkGcRNMc4AgFIzMNqtGIwaMc7EnWzUWp0zQIgJ1urHXVe9nk0+PcAnc1QweztcDOUEIJZubP48viXuHwuDjQeoCC1b8IiWmLegcYDAIxLHsfUjKncVXwXC/MX8pvZv8FusrOrfldo3f35xW02mMmIy+imWYxOHM2U9Cm0eltx+VxKs1AodHorLO5D6zcxRgjxOfAX4O4Bm9UQpDmkWYSboYwxzVDvbD3G0V3rAXhsnoPCnEQa9TLnrnZNyNhMHW9RyBwjTBCXitVoRRj8CAGO7PEw4UooWBjxXiN0J3ejKGVc8jgWj1uM09fCvKmSmeM6Cu4Fs5q72v1n5c7C6XXy1v638Aa8fdMs4rI57jwesQnS/sb9AIxJ1jSiW6fdykOzHmLRmEVMTJ3IrrpdobLsPTm4+0pufC5HWzo6+ZY1dQiLICp0VqHQ6K2w2AmsBNYDVWj9JvYO1KSGIkHNIrGrGSqGZlHe4GKcQXMa21oOkxRnpsGpCR23z4/FaMC091147mJoq+8wQ8WlgxAhzWJEkh2DNQ6++SqkR3Y633DOSP7/a8ewu2ErF+VdxPRMLU3m0ulOzhndYXI61HQIq9HarUDgednnYRImXtqhJfL31Qzl8XtC/pBwDjQeINOe2alFapCJqRPZXb+blnYtoa+/TUKT0iaxu343voCP1vZWalw1jE4azZikMaESH0qzUCg0eiss/gJMBH4NPAkUAC8P1KSGIlEd3DF8FrWtHgqNes2l+kMk2y00uXQzVLufa81fwOvfgWOboHxDRwhpfJZ2fd1nsfKOC3ucX1aijfTMMnzSx9yRc8mNzyUrLotNVZs6HVfWXMZZiWd1S2qLt8RzdtbZVLRWYDKYyE+MbO6KeG+HNt9ITu79jfsZmzw24nmFqYW0+drYU78Hk8GExWjp9T17w5T0Kbh8Lg42HeRw82EA8hPzMRlMFKYWAqjQWYVCp7fCYoqU8gdSytX642a0JD2FTkQHdw9mqLqWNsYE24w3HCIlzkyrx4fXH2By9T/4DU9C7gxtf83uDp+FXtzPZrLhDbSTqSfd9cQnRz8hxZpCUXoRQghmZM1gY9XGTuahoN0+EsFKrmOSxmDuEp4bi+y47jkN0BEJFU1YTEqbBMDGqo39GgkVpCi9CNDCZA82ab6c0UmjAZicrn28lWahUGj0Vlhs0p3aAAghzgM2DMyUhibRQ2ejaxaisRwr7WBJgPoyku3auY0tbSw69gRbDJPgprfBkQm1e3DqfbfjEvMAsBgtuH3uiL6ArvgCPj6r+IzZebNDWsOMrBnUuGpCzYm8fi/lLeXkJ0XWGmaO0HphTUiJXaSvK5ES4ECrZOv2uxmXPC7ieWOSx2A2mGnwNAxIVNJZCWeRYElgW+02yprLMAgDI/WM86npUwEimscUiuFIb4XFDOALIUSZEKIMra/2OUKIbUKIrX25oRBighCiNOzRLIT4kRDiF0KIirDxK/q4lkGlxe3FZjZ06jhnNRtDGdxH69uY9/tPONboCu1PcmrOXcbNA08T6SZNc2g/+Bn2gJO/264Bsx0yJkDNHpx6+KkjWav0ajPakMgee0mD1tq0ub2Zc7PPDY3NyNK0lg1VG0LH+KU/qmZRkFLA4rGLuXLMlb35k4RIs6dhMpi6CYtgJFQ0zcJsMIcS//rbuQ1alFdRehGpA42XAAAgAElEQVTba7dT1lRGXnxeyNQ1b9Q8fjv7t6oTnkKh01thsRDIBy7SH/n62CLgqr7cUEq5R0pZLKUsRhNCbWjOc4A/BPdJKd/ty3UHG60uVGfTTLhmsb2iiQM1TrZVdEQcp7v0ftl6BFO2X4tEMu17n3ZhYaddr9WoC4u2thqsgQCmJK2cR/CLze2P3AUvnGDUT3hJ8TFJY0K/rEFzbgNRNQshBA/NeoiZuTMj7o+GQRjIisvqFj7bNRIqEkHfwUDlO0xOm8y+hn3srt8dMkGBJqiuGHMFBtHbfxGF4symV0l5UsrDA3T/ecABKeXhod4zILyIYJDwDO76Ni0ktrrFA2jVaLMCZdTaM0nPmQZAWvsxIJWEIx+yzVKMCDpX0yeApxln05FQQh5omgVoLU4TSIg5v6CwyEvIC40JIZiUOolddbsAONSsCYtomsXJkBWXFVGzyIyLHAkVZKCFRVF6EX7p50jLEeaOnDsg91AozgQG+2fTN4DlYa/vEkJsFUK8IIRIiXSCEOIWIcQGIcSGmpqaSIcMCs1ubwTNwhiKhmrUk+1q9F7Yda3tvDWinIcyUiFlNABJrqOMFxXEOctZZz63IyEvQ/MR1DUdIcXvh6Rc/fpaGXCP39Pj/I62HMVusndKtgMoTCtkb8NevAEvZU1lZNgziLf0v1M3Jz6HsqayToUCDzQeiOqvCJ8f9G+pj3DCzUzhmoVCoejMoAkLIYQFWAy8oQ89A4xFK4deiVYGvRtSyueklCVSypKMjIxTMtfe0OL2hRofBQnPs6h3dtYsaltcVJv97DEDljiIz8buPMp8g9at9lMxoyOzWhcWx9sbyA4ANq1ooFXvGeHx9U5YnJVwVreub4WphbQH2jnYeJBDzYcG7Avz0rMupc5dxydHPwF6joQKUpBSgFEYByw5LiMuI+SAHwiNSqE4UxhMzeJyYJOUsgpASlklpfRLKQNoSX/nxjz7NCO8PHkQq8lAuz9AICBp0IVFla5ZNB/fT4PRQCVu7dd2aj7m5sNcatxEpaOQY/6UjppN8VlgS+K4wUC2wQ76F35IWPRCszjSfCQU6RNO8Jf7zrqdlDWV9Sl/oi9cPPJicuNzeXmXlp6ztWYrbr+bsUmxhYXNZGNJ4ZIBNREFQ2ij+WoUCsXgCotvEmaCEkKEpwxfC2w/5TM6CVrcPhKsnc1QFr1UR7s/0M1n0Vy5mYAQ+AhwzHkMUvIRx3dQbNjPjoSZuLx+bGb97RGC9owJ1JmMZIXZ93srLPwBPxWtFRGFxajEUcSZ4vji2Bc0tzcPmGZhNBj55sRvsrFqI2uPrWXZp8vIisvikrN6boty/zn3s2D0ggGZF8CV+VcyN29uNxOdQqHoYFCEhRDCAcwH/hY2/EhYKO7FwL2DMbcTJbKDW2+t6g2ENIugsHA27AwdV9ZUpvktPE0YkKy3nIfb6+9URLAqRYuAyrZ3mN56Kyyq26rxBryMTOwuLAzCwMTUiSHz0ECaYq4dfy12k507Vt1BdVs1f5j7B1JsEV1Tp5R5o+bx5Lwnu5noFApFB4MiLKSUTillmpSyKWzs21LKIinlVCnlYillZaxrnE74/AFcXn/E0FnQIp8adAd3XasHf0DicR4MHXe4+XCoWmyNIYPt/pG4vV4aAttDCXfHE7WSGdkJuR3X76WwCEZCRdIsQKvBFAy/HUgnb6IlkWvGXYMv4OM/zv8PijKKBuxeCoWif+ltPwtFDFo93bO3IVxYaJpFMO+irtWDz3ssdFxZcxnkamaWrY4LqXN6kXE7WNX4P5TWFHB25tkcj9NCY7PD7OrBaCi3L3aexZGWI0B0YRH0W1gMFkY4RkQ8pr+4b8Z9zB81P9RQSTH4eL1eysvLcbt7ztdRDC1sNht5eXmYzb0vzxMNJSz6gUilPkDL4AZNmLR4fEzJTWR7RTPVdQ0E/DVAEqMTR2uaxYwpULCQTb6rOV7mxuDQhMm+hn2cnXk2VTZdWIy/PHT98DyLWBxtOYrJYArVaOpKMJchUgHB/sZmsilBcZpRXl5OQkICo0ePVqa4MwgpJXV1dZSXl5Off/LBG4OdZ3FG0ByhiCB0aBbH9Qioidmac9p76DMajQKDNDA5fbKmWZjt8K3XcKcW0tjmxWjVrHDBkhjH2xtJsiZhz5gYun4wg7s3wiIvPi+qIBiTPAaLwaKigYYpbrebtLQ0JSjOMIQQpKWl9ZvGqDSLfqCjl0VkM1RVU1BYaNqB/egaqo0mrIYURieO5n8P/i8unwu7yU6yXRM4hqCwaNKFhfN4N82gt5pFeUt5p8ztrpgNZn5x4S+UsBjGKEFxZtKf76vSLPqBDjNU9wxugEpdWIzP0oRFWtUXHDQm4DClhBzKR5o1v0JynBkMLgyWRgSCQ41aCY7jzuPdGhL1RrOQUnKkJXKORThXjb1KFc1TKBRRUcKiH2iJ0FIVtAxugOO6sMhKtDIuro2Mtv0cN1hJsqSEQlWDzXeS4iwhE9SYhCKqXdU0tzdzvO14qIlQEJupZ82iwdOA0+vkrISzTnKVCsXAER8fu8RMWVkZU6b07cfM0qVLWbFixclMSxGGEhb9QFQHt26GqtR9FilxFi61aUX7mk2SNFt66Eu8rLlMP8aMwaYV3DsvYx6gZVc3eZpCZSmCGIQBs8Ecs9xHT2GzCoVC0RuUsOgHInXJgw4z1PEmrYdFcpyZC8U26mQ8PqOHTEc6ceY4MuMyQ5pFst2CwVpJwBdHcYYWNfTFsS8AugkL0HItYmkWSlgohhKtra3MmzeP6dOnU1RUxN///vfQPp/Px5IlSygsLOT666+nrU3r/7Jx40YuuugiZsyYwYIFC6isHDIpWkMKJSz6gRa3D6vJECrvESQUDdXkJt5qwmo0MNWzmQ+ZiBABcuK1bOz8xPyQZpEcZ8ZoO07Ak01ufC5Wo5UvKnRhESH0NSgsjrYcZeGbC3lu63OhRL6ADLDm6BoMwkBuWDKfQnG6YrPZWLlyJZs2bWL16tX8+Mc/Dn2e9+zZwx133MGuXbtITEzkv/7rv/B6vdx9992sWLGCjRs38r3vfY+f/vSng7yKMxMVDdUPNEdofAQdwqLZ7WNkqh08LST7athkuACoZmRSJqDVZ/rn4X8CkGAzYLAex9twLg6rFs66u343EFuzeGH7C1S0VvDk5ifZVbeLX878JY+sf4T3yt7j5qKbQ9neCsXpjJSSf//3f2fNmjUYDAYqKiqoqtI6RI4cOZKZM7XGW//yL//CE088wcKFC9m+fTvz588HwO/3k5OTE/X6ihNHCYs+UNPi4YfLN/PQtVMYk9HhkGtxe7uFzUKHGQogNc4CTq3/xnH9izsvsUNYNHmaaHQ3Ut9ejzB48XtysJmNIWEhEGTFZUW4h5WjLUfZXrudrxd8nVGJo3hs42OsXbEWp9fJ7dNu5/Zpt/fr30GhGCheeeUVampq2LhxI2azmdGjR4fyBLqGgQohkFIyefJk1q5dOxjTHVYoM1Qf2HK0kbUH6/jNe7s7jUcqIggd0VAAyWHCol7v051uTwc66jGVNZeFtIiAJwe72Rgq4Z1mT8Ns7K692Iw2NldvJiADfHfKd7lp8k38cf4fSbGmcN+M+7ij+A4VQ68YMjQ1NZGZmYnZbGb16tUcPtzRpPPIkSMhofDqq68ya9YsJkyYQE1NTWjc6/WyY8eOQZn7mY7SLPpATavmSP5wZxUbyuopGZ0KBHtZdP8itxg7hEWqwwJOzdncpCscaXatJHawL/Ybe99g9ZHVGPyJBDxZmrDQmwNFK9URzLW4PP/ykBP7/Jzzee+6905qrQrFYLBkyRKuuuoqioqKKCkpYeLEjooFEyZM4Omnn+Z73/sekyZN4vbbb8disbBixQp++MMf0tTUhM/n40c/+hGTJ08exFWcmShh0QeqmzVhkZFg5eH3dvPGbRcghKDF7SMr0dbteINBYDFqDZBSwjSLVpMfIU0kmLUkvdz4XEzCxNsH3qYgpQBv801slUasJgNjksYAkf0V0JHF/YOiH/T7ehWKU0VraysA6enpUU1Ku3fvjjheXFzMmjVruo2/+OKL/TY/hTJD9YnqFjepDgv3XlrAhsMNfLSrGohuhoIOJ3eqwwytmrDwGNuxGpJC5iGTwcS8UfNYNGYRL1/+Mhm2HKwmAwaDYGTiSGxGW9RyHbPzZvPdyd/tsT2pQqFQnAxKs+gD1S0eMhOs3FCSx3+u2stbpRXMn5QV1QwFmt+ixQMpDgvU1oAtmdRkH/FdurI9etGjoe2kOHOo8ZHZYOaFBS9EDX29afJN/bQ6hUKhiI4SFn2gusVDRoIVk9FAyahUthxtxB+QONv9MTQL7Us/ZIaKzyQn1UdWXPTwvq+dnce4zI5oK9UkSKFQDDbKDNUHanVhATBtZBLlDS7K6pxA9+ztIEEzlCYsasGRQa2rNhQJFYlZ49O5Y+64fp69QqFQnDiDJiyEEGV6z+1SIcQGfSxVCPGhEGKf/jz4DZp1pJTUtHjITNAcysUjtal9tq8W6F4XKogl5LOwgLOaQFwaDe4GUm2pp2DWCoVC0T8MtmZxsZSyWEpZor9eBqySUo4HVumvTwsa27y0+wNk6prFlNxEjAbBp7qwiJSUBx3d8lIcZnDW0BiXjF/6Q2GzCoVCMRQYbGHRlauBl/Ttl4BrBnEunahu0cJmMxM1YRFnMVGQlcDaA0HNIooZSs+1SLEKcDVQZ9V8EUpYKBQdGI1GiouLmTJlCl//+tdDRQL7k6VLl5Kfn09xcTHTp0/vFKL72GOPMXHiRIqKipg2bRr33XcfXq+33+cwlBlMYSGBD4QQG4UQt+hjWVLKYMnI40C3+hZCiFuEEBuEEBtqampO1VypbtFKDmTEd9RYKh6ZhLPdD0Q3Q1nNBhKsJsyeBgBqzVoSXbotus9CoRhu2O12SktL2b59OxaLhWeffbbX5/r9/l4f+7vf/Y7S0lIefvhhbr31VgCeffZZPvjgA9atW8e2bdtYv349mZmZuFyuPq/jTGYwo6FmSSkrhBCZwIdCiE4ZN1JKKYSQXU+SUj4HPAdQUlLSbf9AURPSLDqS74pHJrP8Ky0rO1yzeOfgOzR5mlhSuASryaCFzeoJeXV6dJTSLBSnIw/+Ywc7jzX36zUnjUjk51f1PqN69uzZbN26lU8++YRHH32Ud955B4C77rqLkpISli5dyujRo7nxxhv58MMP+bd/+zdSU1P5+c9/jsfjYezYsfz5z3+O2VBpzpw57N+/H4CHHnqINWvWkJycDIDFYmHZstPGAn7aMGiahZSyQn+uBlYC5wJVQogcAP25erDm15WQGSqhQ7OYNjI5tB2uWTxT+gx/3PJHpJRce3Ye3505Glq1pdTpZZqUsFAouuPz+XjvvfcoKuo5XDwtLY1NmzZx6aWX8qtf/YqPPvqITZs2UVJSwmOPPRbz3H/84x8UFRXR3NxMa2sr+fmq/3xPDIpmIYRwAAYpZYu+fRnwS+Bt4CbgYf3579GvcmqpbvbgsBhxWDv+ZOMzE4izGGkLy7OoaK3gSIvWT7uqrYorp+r5FFu+AqAOHxaDJVTqQ6E4neiLBtCfuFwuiouLAU2z+P73v88XX3wR85wbb7wRgHXr1rFz585Q+fL29nYuuOCCiOfcf//9/OpXvyIjI4Pnn3++2/5//vOfPPDAAzQ2NvLqq69y4YUXnsyyzigGywyVBazUy12YgFellO8LIdYDrwshvg8cBm4YjMkdaDxAbnxuqMc1aD6LzC71n4wGQVFuEpuPNIaS79Ye63Ca7azb2VHTKWiG8rtIs6epSrAKRRhBn0U4JpOJQCAQeh0sVR7E4XAAWlj7/PnzWb58eY/3+d3vfsf111/faSw+Pp5Dhw6Rn5/PggULWLBgAYsWLaK9vf1El3NGMihmKCnlQSnlNP0xWUr5kD5eJ6WcJ6UcL6W8VEpZf6rn5va5ufGdG3l558udxqvDEvLCWTRtBDPHdZiU1h5bS6otFYMwsKt+V8eBzhowWqhrbybNpkxQCkVPjBo1ip07d+LxeGhsbGTVqlURjzv//PP5/PPPQz4Ip9PJ3r17e32fn/zkJ9x+++00NjYCmvDpKpgUqtxHN2pcNXj8HrbXbu80XtvioXBEYrfjq00rkDnbCMgSpJR8efxLLsq7iJ11O9lV10VYODKoc9dHbGKkUCg6M3LkSG644QamTJlCfn4+Z599dsTjMjIyePHFF/nmN7+Jx6P5Fn/1q19RUFDQq/vcfvvtOJ1OzjvvPKxWK/Hx8cycOTPq/YYrSlh0odal5U3sbej8y6S6xcNFETSLjw5/RHlrOe8fep9RSVrHuwtGXICUknWV6zoOdNYgHelUt1UzKW3SgK5BoRhqBEuUd+WRRx7hkUce6TZeVlbW6fUll1zC+vXrY94jWslyIQT3338/999/f6/mOlw53ZLyBp3qNi1qqby1HKdXq/vU1u6j1eMLlfoIUuuqpby1HIHgyc1P8mn5p4DWfKgwrZAaVw01bXouiLOGQ3FJ1LvrmZymGrMoFIqhhRIWXQh9uQP7GvYBUNnUhsFa0SlsFmBLzRYAbp56M+Wt5fxp258oSCkg3Z5OYWohQIffwlnLp3oqxpy8OQO8CoVCoehflLDowt66io5t3RS1fPerxOU/hcHS2d++pWYLZoOZW6bewoysGXj8Hs7POR+AwjRdWNTtAimhtZo1spXxKeOjdr1TKBSK0xUlLLpwqKGSgDcJAjZ21O5BSskXh15GCEmrPNrp2C3VWyhMK8RqtPLjGT/GarRy6ahLAXCYHYxOHK1pFp4WWgLtbGqvY06u0ioUCsXQQzm4u1DTVoP0JuH3GlhTtpVdE3ZyVGrlD+pd+0PHef1edtTt4IbxWsx2UUYR6761DpOh409amFpIaU0pOGtYZ7fhQzI7b/apXZBCoVD0A0qz6EKTtw4TSYxJGk+N5xAvbPwfzFKS4vdTefiD0HG763fj8Xso/vwZ2PcRQCdBQVMFhYY4Kp2VrDn8MWvi7CQY7UzLmHaql6RQKBQnjRIWXXD5G0gwpXLD1HMRRg8fHHufi51tjPNIDjQdgEbNFBV0bk9ztsCOlR0XaDwCb90J/zmNqz55gvGJ+dy57QnedTiYmTGts0BRKBRA9BLlVVVVfOtb32LMmDHMmDGDCy64gJUrV/ZwtcgES3eUlZXx6quvhsZffPFF7rrrrh7Pnzt3LhMmTKC4uJjCwkKee+65E5pHNMLn8eyzz/KXv/ylX69/sihhEYbb58Yv2ki1ZTAtS3NQS4OPq1uduIxTOWQy4f/4IQBKa0rJMcWT5ffDgVWaExvgtW/D9jehYAHpfj/LC5ayJLWYdoNg/lmXDtbSFIrTmkglyqWUXHPNNcyZM4eDBw+yceNG/vrXv1JeXn5C9wjWmuoqLPrCK6+8QmlpKZ9//jkPPPDAgJUEue222/jOd74zINc+UdTP3DCq9fpNWY4MxiePByDVaOcCl5tjl13GQ1t3UlH2MSOlpLS6lLOFXTuxpRKqd2rblaVw+e9g+nfgN3lYj5WyLLGIWza/Q+qSawdjWQpF73lvGRzf1r/XzC6Cyx/u9eHBEuUff/wxFouF2267LbRv1KhR3H333d3OufPOO1mwYAGLFy/m2muvJSUlhRdeeIEXXniBAwcO8NBDDxEfH09rayvLli1j165dFBcXc9NNN5GSksKxY8dYuHAhBw4c4Nprr42YCBhOa2srDocDo1GrCXf77bezfv16XC4X119/PQ8++CAAy5Yt4+2338ZkMnHZZZfx6KOPUlNTw2233caRI1rB0ccffzxUBDHIL37xC+Lj4/nXf/1X5s6dy3nnncfq1atpbGzk+eefZ/bs2fj9fpYtW8Ynn3yCx+PhzjvvDPXoGAiUsAgjGDZ7VlI2ceY45uTNYXpTHWbDQQpHzICtsN/XjLdyPVVtVZzjT4DUsVB/APav0kp6GEww5Tow2yBnKpRvhKxJpFoSwWQZ5BUqFKc3wRLlCxcuZMeOHUyfPr1X582ePZtPP/2UxYsXU1FRQWWl1kPt008/5Rvf+EanYx9++OFOfTJefPFFSktL2bx5M1arlQkTJnD33XczcuTIbvdZsmQJVquVffv28fjjj4eExUMPPURqaip+v5958+axdetWcnNzWblyJbt370YIEao9dc8993Dvvfcya9Ysjhw5woIFC9i1a1e3e3X9u3z11Ve8++67PPjgg3z00Uc8//zzJCUlsX79ejweDzNnzuSyyy4bsHLrSliEERQW41JHAPD0vKfhzR9AYg5jU8YBcNBs5vA+rXL6nJoKmHwdGM2w7wOo3QfjLwOHXigwtwQ2vwxxKeDIOPULUij6Sh80gP4kUonyrt3y7rzzTj777DMsFku30h6zZ8/m8ccfZ+fOnUyaNImGhgYqKytZu3YtTzzxRI/3nzdvHklJSQBMmjSJw4cPRxQWr7zyCiUlJdTU1HDhhReycOFCRo0axeuvv85zzz2Hz+ejsrIyNA+bzcb3v/99Fi1axKJFiwD46KOP2LlzZ+iawZ4asfja174GwIwZM0KlTj744AO2bt3KihUrAGhqamLfvn1KWJwKyhq1XyMTM/I6BpsqIGkk8ZZ4su2Z7Lc4qTq+noKkMWQf+gQyJoDJBuue1o6fFvYrJu8c+OqPUPY5ZBaeuoUoFEOMSCXKJ0+ezJtvvhl6/fTTT1NbW0tJSUm383Nzc2lsbOT9999nzpw51NfX8/rrrxMfH09CQs+9Y6zWjuoMRqMRn88X8/iMjAymT5/Ol19+SSAQ4NFHH2X9+vWkpKSwdOlS3G43JpOJr776ilWrVrFixQqeeuopPv74YwKBAOvWrcNms8W8R6T5hc9NSsmTTz7JggULen2dk0E5uMOoaK5CSiOFmWEZ1s3lkJgLwNiU8Wyxx7HZXcmcRE3TIL0Axs3Ttm1JULCw49y8Gdqzqx4cque2QtEXLrnkEtxuN88880xoLBglFYnzzz+fxx9/nDlz5jB79mweffRRZs/unteUkJBAS0vLSc2tra2NzZs3M3bsWJqbm3E4HCQlJVFVVcV7770HaH6NpqYmrrjiCv7whz+wZYsWQXnZZZfx5JNPhq7VVUj2lgULFvDMM8/g9XoB2Lt3L06n86TWFQulWYRR665F+BOxW/Q/S8APzZWQpAuL5LF8fuxzAOYY9HLlGRPAngLWRCi6AUxh9aNS8iEuDdrqlBlKoegjQgjeeust7r33Xh555BEyMjJwOBz89re/jXj87Nmz+eCDDxg3bhyjRo2ivr4+orCYOnUqRqORadOmsXTpUlJSUno9pyVLlmC32/F4PCxdupQZM7QfhGeffTYTJ05k5MiRIWd1S0sLV199NW63GyllqNXrE088wZ133snUqVPx+XzMmTOnm8mtN/zgBz+grKyM6dOnI6UkIyODt956q8/X6S1CBkM+hyAlJSVyw4YN/Xa98/98A37pZv333tYGmivhsYlwxaNw7s2s3LeSn33xM5L8Af4v7RKMO1bCT46CEFp+RVw6WOI6X/TVG2Hv+zD3JzBXNYFXnH7s2rWLwkJlJj1TifT+CiE2Sim72/NioMxQYbgC9SSYwrrYNetFBZM0H8aY5DEAXOhyYdz7PmQUaIICIPms7oICNCc3KM1CoVAMaU65sBBCjBRCrBZC7BRC7BBC3KOP/0IIUSGEKNUfV5zKefn8AfyimVR7mG+hSU/+0YVFQUoBhYn5fK2lFZzVkD6h5wuPPEd7jlfd8RQKxdBlMHwWPuDHUspNQogEYKMQ4kN93x+klI8Owpw43NCIMLrIdmR2DAY1C93BbTfZeX3x3+DXWmgtGb1o2zh6Dnztv7WQWoVCoRiinHLNQkpZKaXcpG+3ALuA3FM9j7AJwdH1NH3wbwCMSsrp2NdUAeY4zYEdxGiCzInadm80C4MBpt6gEvIUCsWQZlB9FkKI0cDZwJf60F1CiK1CiBeEEBFDFIQQtwghNgghNtTU1EQ6pPdICS9dBc9fCoc1p/a4xNSO/U1HNa0i6JcIklWkPWf0QlgoFArFGcCgCQshRDzwJvAjKWUz8AwwFigGKoHfRzpPSvmclLJESlmSkXGSTuPyDVD2Kcy6j+V5VwJQQFgmZXNFKGy2E4WLYNQsSB51cvdXKBSKIcKgCAshhBlNULwipfwbgJSySkrpl1IGgP8Gzh2o+wcCkv9Zdxj/lr+Cyc4LaZm8H9jArDYXhc1VHQc2VYSc252YcDl89381k5RCoegX3nrrLYQQ7N69O+Zxjz/+eMzkvP7i2LFjXH+91tystLSUd999N7Tv7bff5uGHB6c0ymAxGNFQAnge2CWlfCxsPMxZwLXA9oGaw9qDdfzirVLaNr/BH8YU8Yet/wWtxfy8FgyVm7WDfO3QWgWJEYSFQqHod5YvX86sWbNYvnx5zONORFj4/f4+He/z+RgxYkSo7lJXYbF48WKWLRteeVOD8dN4JvBtYJsQIpjn/u/AN4UQxYAEyoABq7U7c1w6v59ey4dHfLzgPY7ZORPRcC0Jo/1QsVE7qKVSm0okM5RCcYby269+y+762L/s+8rE1Ik8cO4DMY9pbW3ls88+Y/Xq1Vx11VU8+OCD+P1+HnjgAd5//30MBgM333wzUkqOHTvGxRdfTHp6OqtXr2b58uX8+te/RkrJlVdeGcrwjo+P59Zbb+Wjjz7i6aefZtasWaH7rV+/nnvuuQen04nVamXVqlW8+eab/O1vf6O1tRW/389LL73EokWL2LRpEz/72c9wuVx89tln/OQnP8HlcrFhwwaeeuopqqqquO222zh48CAAzzzzTKjR0pnEKRcWUsrPABFh17sRxgaM8fIDvp2eir91LIGqxbx26/k4Du2CA/8LzrqOHItEJSwUioHm73//OwsXLqSgoIC0tDQ2btzIV199RVlZGaWlpZhMJurr60lNTeWxxx5j9erVpKenc+zYMS6kJJAAAAvKSURBVB544AE2btxISkoKl112GW+99RbXXHMNTqeT8847j9//vrP7s729nRtvvJHXXnuNc845h+bmZux2rTfNpk2b2Lp1K6mpqaHqrhaLhV/+8pch4QBaWfMgP/zhD7noootYuXIlfr+/xwqyQ5VhaXRvbq7gx63bSLbEcd3Yn3LZlWOZkpsE7Xrhv2ObtP4UCK1QoEIxTOhJAxgoli9fzj333APAN77xDZYvX86hQ4e47bbbMJm0r6nU1NRu561fv565c+cSDHZZsmQJa9as4ZprrsFoNHLdddd1O2fPnj3k5ORwzjlawmxiYmJo3/z58yPeJxYff/xxqAWq0WgMlTo/0xiWwqKicgNeYeT3Z/+Y4inndOwYUQwIKH0Fdr4NM5ZCcvea9gqFov+or6/n448/Ztu2bQgh8Pv9CCFCX+Ynis1mCzUnWrBgAVVVVZSUlISEUiQcDsdJ3fNMZljWhiqccDXvfns9xZM7d9DCmqDlTuxYCdZ4uOT/G5wJKhTDiBUrVvDtb3+bw4cPU1ZWxtGjR8nPz2fatGn88Y9/DPVvqK+vBzqXGD/33HP5v//7P2pra/H7/SxfvpyLLrqo2z3++c9/Ulpayp/+9CcmTJhAZWVlqIFSS0tLj/0rYpU1nzdvXqiMut/vp6mp6cT+EKc5w1JYAFjMtu7JdgC5uinq4v/o6HinUCgGjOXLl3PttZ3701933XVUVlZy1llnMXXqVKZNm8arr74KwC233MLChQu5+OKLycnJ4eGHH+biiy9m2rRpzJgxg6uvvjrm/SwWC6+99hp3330306ZNY/78+bjd7pjnXHzxxezcuZPi4mJee+21Tvv+8z//k9WrV1NUVMSMGTM6dcE7k1AlyrtyeC1sWQ5XPqbyKBTDAlWi/Mymv0qUq2/Droy6QHsoFAqFIsSwNUMpFAqFovcoYaFQKBjK5mhFdPrzfVXCQqEY5thsNurq6pTAOMOQUlJXV4fNZuuX6ymfhUIxzMnLy6O8vJyTLvmvOO2w2Wzk5fVPfTslLBSKYY7ZbCY/P3+wp6E4zVFmKIVCoVD0iBIWCoVCoegRJSwUCoVC0SNDOoNbCFEDHO7jaelA7QBMZ7BQ6zm9Ues5vRmu6xklpexTX+ohLSxOBCHEhr6muZ/OqPWc3qj1nN6o9fQeZYZSKBQKRY8oYaFQKBSKHhmOwuK5wZ5AP6PWc3qj1nN6o9bTS4adz0KhUCgUfWc4ahYKhUKh6CNKWCgUCoWiR4aVsBBCLBRC7BFC7BdCLBvs+cRCCFEmhNgmhCgVQmzQx1KFEB8KIfbpzyn6uBBCPKGva6sQYnrYdW7Sj98nhLjpFM7/BSFEtRBie9hYv81fCDFD//vs18+N0CN3wNfzCyFEhf4elQohrgjb9xN9bnuEEAvCxiN+BoUQ+UKIL/Xx14QQlgFcy0ghxGohxE4hxA4hxD36+JB8f2KsZ0i+P/r9bEKIr4QQW/Q1PRhrHkIIq/56v75/9ImuNSpSymHxAIzAAWAMYAG2AJMGe14x5lsGpHcZewRYpm8vA36rb18BvAcI4HzgS308FTioP6fo2ymnaP5zgOnA9oGYP/CVfqzQz718ENbzC+BfIxw7Sf98WYF8/XNnjPUZBF4HvqFvPwvcPoBryQGm69sJwF59zkPy/YmxniH5/uj3EEC8vm0GvtT/nhHnAdwBPKtvfwN47UTXGu0xnDSLc4H9UsqDUsp24K9A7M7upx9XAy/p2y8B14SN/0VqrAOShRA5wALgQyllvZSyAfgQWHgqJiqlXAPUdxnul/nr+xKllOuk9h/xl7Brncr1RONq4K9SSo+U8hCwH+3zF/EzqP/qvgRYoZ8f/rfpd6SUlVLKTfp2C7ALyGWIvj8x1hON0/r9AdD/1q36S7P+kDHmEf7erQDm6fPu01pjzWk4CYtc4GjY63Jif6AGGwl8IITYKIS4RR/LklJW6tvHgSx9O9raTrc199f8c/XtruODwV26aeaFoNmGvq8nDWiUUvq6jA84urnibLRfrkP+/emyHhjC748QwiiEKAWq0QTxgRjzCM1d39+kz7vfvhuGk7AYasySUk4HLgfuFELMCd+p/2IbsnHPQ33+Os8AY4FioBL4/eBOp28IIeKBN4EfSSmbw/cNxfcnwnqG9PsjpfRLKYuBPDRNYOJgzmc4CYsKYGTY6zx97LRESlmhP1cDK9E+LFW6io/+XK0fHm1tp9ua+2v+Ffp21/FTipSySv+HDgD/jfYeQd/XU4dm2jF1GR8whBBmtC/WV6SUf9OHh+z7E2k9Q/n9CUdK2QisBi6IMY/Q3PX9Sfq8+++7YSCdNKfTA60r4EE0J0/QoTN5sOcVZa4OICFs+ws0X8Pv6OyAfETfvpLODsiv9PFU4BCa8zFF3049hesYTWeHcL/Nn+4O1CsGYT05Ydv3otmGASbT2al4EM2hGPUzCLxBZ8flHQO4DoHmR3i8y/iQfH9irGdIvj/6PTKAZH3bDnwKLIo2D+BOOju4Xz/RtUad00D/g51OD7Sojr1otr+fDvZ8YsxzjP7mbQF2BOeKZoNcBewDPgr7xxTA0/q6tgElYdf6HppTaz/w3VO4huVoqr8XzR76/f6cP1ACbNfPeQq9GsEpXs/L+ny3Am93+XL6qT63PYRFAkX7DOrv+Vf6Ot8ArAO4llloJqatQKn+uGKovj8x1jMk3x/9flOBzfrctwM/izUPwKa/3q/vH3Oia432UOU+FAqFQtEjw8lnoVAoFIoTRAkLhUKhUPSIEhYKhUKh6BElLBQKhULRI0pYKBQKhaJHlLBQKBQKRY8oYaEY0gghkoUQd/RwzGghxLd6ca3RIqwEeYT9xV3KXC/uVWnnE0QIcY0QYtJAXV+h6AtKWCiGOslo5ZljMRroUVj0gmK0RCYApJRvSykf7ofrRuMatBLTCsWgo5LyFEMaIUSwtPIetMqcoBVflMCvpJSvCSHWAYVo5SheQqu19TJaKRWAu6SUX+gVS9+RUk6JcB8LWnasHa2Gzm/07RIp5V1CiBcBF1rF00y0zObvoNXz+VJKuVS/zmXAg2jlFw6gZT23CiEeBhYDPuAD4G/AO2jVQ5uA6/SpPI1WCqINuFlKuVu/txstazoRuE9K+Y4QYjLwZ7RyDgbgOinlvr79hRUKnYFMWVcP9RjoB2H1mtC+UD9Eq32TBRxBa4wzF00IBM+JA2z69nhgQ9drRbnXUuCpSK+BF9F6AgR7CDQDRWhf0hvRtJJ0YA3g0M95APgZWpmNPXT8eEsOu+b1YfdbBYzXt88DPg477n39XuPRypHYgCeBJfoxFsA+2O+XegzdR7B6oUJxJjALWC6l9KNVUP0/4By0L+5wzMBTQohiwA8U9NP9/yGllEKIbUCVlHIbgBBiB5ogykMzK32udxm1AGvRNAc38LwQ4h00jaITevntC4E3wjqUWsMOeV1q1VX3CSEOopWzXgv8VAiRB/xNKq1CcRIoYaEYjtwLVAHT0H6Nu/vpuh79ORC2HXxtQhNMH0opv9n1RCHEucA84HrgLrSOaOEY0BrfFEe5d1d7spRSviqE+BKtauy7QohbpZQf92VBCkUQ5eBWDHVa0Poug1bG+Ua9w1gGWt/sr7ocA1qt/0r9l/i30cxWfb3XibAOmCmEGAcghHAIIQp0rSFJSvkumiCb1vV+Umvmc0gI8XX9XCGEmBZ27a8LIQxCiLFolUn3CCHGAAellE8Af0erZKpQnBBKWCiGNFLKOjSzznY0Z/JWtNLuHwP/JqU8ro/5hRBbhBD3Av8F3CSE2IJmrnH28nargUlCiFIhxI0nMNcaND/HciHEVjQz0UQ0gfCOPvYZcJ9+yl+B+4UQm3UhsAT4vj7vHXTumXwETTC+B9wmpXQDNwDb9dacU9B6PigUJ4SKhlIohjh6NNQ7UsoVgz0XxZmL0iwUCoVC0SNKs1AouiCEWAD8tsvwISnltYMxH4XidEAJC4VCoVD0iDJDKRQKhaJHlLBQKBQKRY8oYaFQKBSKHlHCQqFQKBQ98v8AEHwP+il+ex8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pg_result_na[\"label\"] = \"Pure PG\"\n",
    "pgwb_result_wb[\"label\"] = \"PG with Baseline\"\n",
    "ac_result[\"label\"] = \"Actor-critic\"\n",
    "pg_result = pd.concat([pg_result_na, pgwb_result_wb, ac_result])\n",
    "ax = sns.lineplot(\n",
    "    x=\"total_timesteps\", \n",
    "    y=\"performance\", \n",
    "    data=pg_result, hue=\"label\",\n",
    ")\n",
    "ax.set_title(\"Policy Gradient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.2 -- Land on the Moon!\n",
    "\n",
    "(5 / 100 points)\n",
    "\n",
    "Let's try to land on the moon! In this section, we will try a harder environment: \"LunarLander-v2\". The agent needs to control the lander to land on a small interval of the moon marked by the flags. Due to the complex dynamic of rocket, the environment is hard to solve and it may take hours for you to train. \n",
    "\n",
    "The environment provides a 8-element vector as observation, which is different from the image observation provided by the following Pong environment in the next setion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "# e = gym.make(\"LunarLander-v2\"); e.reset()\n",
    "# frames = []\n",
    "# for _ in range(1000):\n",
    "#     frames.append(e.render(\"rgb_array\"))\n",
    "#     if e.step(e.action_space.sample())[2]: break\n",
    "# e.close();animate(frames);del e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(8,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ActorCriticTrainer LunarLander-v2 Training Start ===\n",
      "Config:\n",
      "  checked: true\n",
      "  clip_gradient: 10.0\n",
      "  env_name: LunarLander-v2\n",
      "  evaluate_interval: 1\n",
      "  evaluate_num_episodes: 5\n",
      "  gamma: 0.99\n",
      "  hidden_units: 64\n",
      "  learning_rate: 0.005\n",
      "  max_episode_length: 1000\n",
      "  max_iteration: 500\n",
      "  normalize_advantage: true\n",
      "  num_critic_update_steps: 10\n",
      "  num_critic_updates: 10\n",
      "  seed: 0\n",
      "  train_batch_size: 10000\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 0 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999999403953552\n",
      "  critic_loss: 80.96274181365966\n",
      "  evaluate_reward: -178.97608437884318\n",
      "  iter_episodes: 109\n",
      "  iter_time: 6.292596340179443\n",
      "  iter_timesteps: 10044\n",
      "  iteration: 0\n",
      "  mean_advantage: -9.115166399453756e-09\n",
      "  mean_baselines: -0.06175880879163742\n",
      "  mean_log_prob: -1.3817871809005737\n",
      "  performance: -184.34699345816713\n",
      "  policy_loss: -3.683338165283203\n",
      "  total_episodes: 109\n",
      "  total_time: 6.292596340179443\n",
      "  total_timesteps: 10044\n",
      "  training_episode_length:\n",
      "    max: 139.0\n",
      "    mean: 92.14678899082568\n",
      "    min: 56.0\n",
      "  training_episode_reward:\n",
      "    max: -8.311114052018638\n",
      "    mean: -184.34699345816713\n",
      "    min: -481.48515469502763\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 1 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999999403953552\n",
      "  critic_loss: 37.90556716918945\n",
      "  evaluate_reward: -101.05746007436761\n",
      "  iter_episodes: 105\n",
      "  iter_time: 6.5229315757751465\n",
      "  iter_timesteps: 10077\n",
      "  iteration: 1\n",
      "  mean_advantage: -6.6956888922220514e-09\n",
      "  mean_baselines: -22.22824478149414\n",
      "  mean_log_prob: -1.377764105796814\n",
      "  performance: -166.31927390204936\n",
      "  policy_loss: -105.96405029296875\n",
      "  total_episodes: 214\n",
      "  total_time: 12.81552791595459\n",
      "  total_timesteps: 20121\n",
      "  training_episode_length:\n",
      "    max: 160.0\n",
      "    mean: 95.97142857142858\n",
      "    min: 59.0\n",
      "  training_episode_reward:\n",
      "    max: 54.13870274574492\n",
      "    mean: -166.31927390204936\n",
      "    min: -561.3911969636483\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 2 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 44.97544834136963\n",
      "  evaluate_reward: -277.0567113912602\n",
      "  iter_episodes: 93\n",
      "  iter_time: 7.501986026763916\n",
      "  iter_timesteps: 10095\n",
      "  iteration: 2\n",
      "  mean_advantage: -7.746537633579464e-09\n",
      "  mean_baselines: -41.92174530029297\n",
      "  mean_log_prob: -1.3750243186950684\n",
      "  performance: -165.42713715611916\n",
      "  policy_loss: -172.17906188964844\n",
      "  total_episodes: 307\n",
      "  total_time: 20.317513942718506\n",
      "  total_timesteps: 30216\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 108.54838709677419\n",
      "    min: 60.0\n",
      "  training_episode_reward:\n",
      "    max: 48.68632833682737\n",
      "    mean: -165.42713715611916\n",
      "    min: -525.4677428936373\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 3 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999999403953552\n",
      "  critic_loss: 65.82150547027588\n",
      "  evaluate_reward: -269.74817473707225\n",
      "  iter_episodes: 101\n",
      "  iter_time: 6.499579906463623\n",
      "  iter_timesteps: 10007\n",
      "  iteration: 3\n",
      "  mean_advantage: 2.739895643699697e-09\n",
      "  mean_baselines: -54.15155792236328\n",
      "  mean_log_prob: -1.3634603023529053\n",
      "  performance: -170.90943767835373\n",
      "  policy_loss: -129.03965759277344\n",
      "  total_episodes: 408\n",
      "  total_time: 26.81709384918213\n",
      "  total_timesteps: 40223\n",
      "  training_episode_length:\n",
      "    max: 169.0\n",
      "    mean: 99.07920792079207\n",
      "    min: 62.0\n",
      "  training_episode_reward:\n",
      "    max: -3.376855099028546\n",
      "    mean: -170.90943767835373\n",
      "    min: -559.0043736973241\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 4 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 31.659201316833496\n",
      "  evaluate_reward: -138.90581547240484\n",
      "  iter_episodes: 96\n",
      "  iter_time: 6.80830454826355\n",
      "  iter_timesteps: 10060\n",
      "  iteration: 4\n",
      "  mean_advantage: 5.119126456065715e-09\n",
      "  mean_baselines: -61.86819076538086\n",
      "  mean_log_prob: -1.3540165424346924\n",
      "  performance: -154.89090813769266\n",
      "  policy_loss: -342.9485778808594\n",
      "  total_episodes: 504\n",
      "  total_time: 33.62539839744568\n",
      "  total_timesteps: 50283\n",
      "  training_episode_length:\n",
      "    max: 168.0\n",
      "    mean: 104.79166666666667\n",
      "    min: 60.0\n",
      "  training_episode_reward:\n",
      "    max: -31.91107682052585\n",
      "    mean: -154.89090813769266\n",
      "    min: -474.72177126060876\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 5 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 72.6798978805542\n",
      "  evaluate_reward: -242.4401497449362\n",
      "  iter_episodes: 91\n",
      "  iter_time: 6.96870756149292\n",
      "  iter_timesteps: 10041\n",
      "  iteration: 5\n",
      "  mean_advantage: -6.933395635400075e-09\n",
      "  mean_baselines: -65.03761291503906\n",
      "  mean_log_prob: -1.344773292541504\n",
      "  performance: -176.82815212568264\n",
      "  policy_loss: -330.513427734375\n",
      "  total_episodes: 595\n",
      "  total_time: 40.5941059589386\n",
      "  total_timesteps: 60324\n",
      "  training_episode_length:\n",
      "    max: 182.0\n",
      "    mean: 110.34065934065934\n",
      "    min: 69.0\n",
      "  training_episode_reward:\n",
      "    max: -31.245443231381955\n",
      "    mean: -176.82815212568264\n",
      "    min: -568.216117761159\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 6 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 87.18661529541015\n",
      "  evaluate_reward: -110.19104940420357\n",
      "  iter_episodes: 86\n",
      "  iter_time: 6.846235513687134\n",
      "  iter_timesteps: 10040\n",
      "  iteration: 6\n",
      "  mean_advantage: 7.242795252437872e-10\n",
      "  mean_baselines: -68.3348388671875\n",
      "  mean_log_prob: -1.3284038305282593\n",
      "  performance: -177.5674446177568\n",
      "  policy_loss: -206.8271484375\n",
      "  total_episodes: 681\n",
      "  total_time: 47.44034147262573\n",
      "  total_timesteps: 70364\n",
      "  training_episode_length:\n",
      "    max: 213.0\n",
      "    mean: 116.74418604651163\n",
      "    min: 63.0\n",
      "  training_episode_reward:\n",
      "    max: -5.367639785357369\n",
      "    mean: -177.5674446177568\n",
      "    min: -504.3366028396622\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 7 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 42.87539772033691\n",
      "  evaluate_reward: -168.4302896055588\n",
      "  iter_episodes: 87\n",
      "  iter_time: 6.876673460006714\n",
      "  iter_timesteps: 10110\n",
      "  iteration: 7\n",
      "  mean_advantage: 7.310559713147313e-09\n",
      "  mean_baselines: -67.70841217041016\n",
      "  mean_log_prob: -1.3205633163452148\n",
      "  performance: -155.67412758288776\n",
      "  policy_loss: -454.6003112792969\n",
      "  total_episodes: 768\n",
      "  total_time: 54.317014932632446\n",
      "  total_timesteps: 80474\n",
      "  training_episode_length:\n",
      "    max: 225.0\n",
      "    mean: 116.20689655172414\n",
      "    min: 67.0\n",
      "  training_episode_reward:\n",
      "    max: 13.609458437497835\n",
      "    mean: -155.67412758288776\n",
      "    min: -479.1671646713831\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 8 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 58.18757362365723\n",
      "  evaluate_reward: -148.22903904799563\n",
      "  iter_episodes: 87\n",
      "  iter_time: 7.066628456115723\n",
      "  iter_timesteps: 10071\n",
      "  iteration: 8\n",
      "  mean_advantage: 2.88820034555215e-09\n",
      "  mean_baselines: -78.04473114013672\n",
      "  mean_log_prob: -1.299760103225708\n",
      "  performance: -190.7487755532185\n",
      "  policy_loss: -180.07052612304688\n",
      "  total_episodes: 855\n",
      "  total_time: 61.38364338874817\n",
      "  total_timesteps: 90545\n",
      "  training_episode_length:\n",
      "    max: 205.0\n",
      "    mean: 115.75862068965517\n",
      "    min: 61.0\n",
      "  training_episode_reward:\n",
      "    max: 6.3877240537091495\n",
      "    mean: -190.7487755532185\n",
      "    min: -459.7618155246458\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 9 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 97.13274612426758\n",
      "  evaluate_reward: -120.58205841633654\n",
      "  iter_episodes: 78\n",
      "  iter_time: 7.191360712051392\n",
      "  iter_timesteps: 10048\n",
      "  iteration: 9\n",
      "  mean_advantage: 6.6438299306526005e-09\n",
      "  mean_baselines: -85.23552703857422\n",
      "  mean_log_prob: -1.2905460596084595\n",
      "  performance: -202.3283721721117\n",
      "  policy_loss: -120.30308532714844\n",
      "  total_episodes: 933\n",
      "  total_time: 68.57500410079956\n",
      "  total_timesteps: 100593\n",
      "  training_episode_length:\n",
      "    max: 223.0\n",
      "    mean: 128.82051282051282\n",
      "    min: 75.0\n",
      "  training_episode_reward:\n",
      "    max: 3.9260994646362235\n",
      "    mean: -202.3283721721117\n",
      "    min: -524.0315052591773\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 10 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 39.84043598175049\n",
      "  evaluate_reward: -156.96625024144703\n",
      "  iter_episodes: 78\n",
      "  iter_time: 7.447761297225952\n",
      "  iter_timesteps: 10033\n",
      "  iteration: 10\n",
      "  mean_advantage: 3.8021502613005964e-10\n",
      "  mean_baselines: -79.52536010742188\n",
      "  mean_log_prob: -1.2795885801315308\n",
      "  performance: -162.5593720302082\n",
      "  policy_loss: -285.11651611328125\n",
      "  total_episodes: 1011\n",
      "  total_time: 76.02276539802551\n",
      "  total_timesteps: 110626\n",
      "  training_episode_length:\n",
      "    max: 234.0\n",
      "    mean: 128.62820512820514\n",
      "    min: 70.0\n",
      "  training_episode_reward:\n",
      "    max: 8.74147700651855\n",
      "    mean: -162.5593720302082\n",
      "    min: -460.73478913544614\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 11 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 58.0923490524292\n",
      "  evaluate_reward: -173.7568254997201\n",
      "  iter_episodes: 77\n",
      "  iter_time: 7.293846607208252\n",
      "  iter_timesteps: 10047\n",
      "  iteration: 11\n",
      "  mean_advantage: 2.017077616045526e-09\n",
      "  mean_baselines: -90.37444305419922\n",
      "  mean_log_prob: -1.2541403770446777\n",
      "  performance: -194.87729673087594\n",
      "  policy_loss: -70.96062469482422\n",
      "  total_episodes: 1088\n",
      "  total_time: 83.31661200523376\n",
      "  total_timesteps: 120673\n",
      "  training_episode_length:\n",
      "    max: 227.0\n",
      "    mean: 130.4805194805195\n",
      "    min: 71.0\n",
      "  training_episode_reward:\n",
      "    max: -0.8929622514965416\n",
      "    mean: -194.87729673087594\n",
      "    min: -660.7726312334762\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 12 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 97.91886749267579\n",
      "  evaluate_reward: -267.2240746795593\n",
      "  iter_episodes: 78\n",
      "  iter_time: 7.342763662338257\n",
      "  iter_timesteps: 10129\n",
      "  iteration: 12\n",
      "  mean_advantage: 1.930133608496476e-09\n",
      "  mean_baselines: -91.38731384277344\n",
      "  mean_log_prob: -1.2421869039535522\n",
      "  performance: -192.39576722077913\n",
      "  policy_loss: -36.14866256713867\n",
      "  total_episodes: 1166\n",
      "  total_time: 90.65937566757202\n",
      "  total_timesteps: 130802\n",
      "  training_episode_length:\n",
      "    max: 251.0\n",
      "    mean: 129.85897435897436\n",
      "    min: 78.0\n",
      "  training_episode_reward:\n",
      "    max: 11.525259987609303\n",
      "    mean: -192.39576722077913\n",
      "    min: -587.8411782154885\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 13 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 49.281759109497074\n",
      "  evaluate_reward: -261.7143001838393\n",
      "  iter_episodes: 75\n",
      "  iter_time: 7.321732521057129\n",
      "  iter_timesteps: 10071\n",
      "  iteration: 13\n",
      "  mean_advantage: -3.4090235079276e-09\n",
      "  mean_baselines: -87.6858901977539\n",
      "  mean_log_prob: -1.2395730018615723\n",
      "  performance: -177.24542219371884\n",
      "  policy_loss: -221.11094665527344\n",
      "  total_episodes: 1241\n",
      "  total_time: 97.98110818862915\n",
      "  total_timesteps: 140873\n",
      "  training_episode_length:\n",
      "    max: 350.0\n",
      "    mean: 134.28\n",
      "    min: 82.0\n",
      "  training_episode_reward:\n",
      "    max: 24.189628046352894\n",
      "    mean: -177.24542219371884\n",
      "    min: -442.583356465387\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 14 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999999403953552\n",
      "  critic_loss: 59.50186637878418\n",
      "  evaluate_reward: -243.20743023804917\n",
      "  iter_episodes: 73\n",
      "  iter_time: 9.006419658660889\n",
      "  iter_timesteps: 10052\n",
      "  iteration: 14\n",
      "  mean_advantage: -6.261689833308992e-09\n",
      "  mean_baselines: -88.28624725341797\n",
      "  mean_log_prob: -1.2359997034072876\n",
      "  performance: -183.20503246737704\n",
      "  policy_loss: -128.62437438964844\n",
      "  total_episodes: 1314\n",
      "  total_time: 106.98752784729004\n",
      "  total_timesteps: 150925\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 137.6986301369863\n",
      "    min: 81.0\n",
      "  training_episode_reward:\n",
      "    max: 23.748131491013428\n",
      "    mean: -183.20503246737704\n",
      "    min: -492.60323876923974\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 15 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 76.1583194732666\n",
      "  evaluate_reward: -119.81189675912105\n",
      "  iter_episodes: 73\n",
      "  iter_time: 7.392946720123291\n",
      "  iter_timesteps: 10037\n",
      "  iteration: 15\n",
      "  mean_advantage: -2.13785711355996e-10\n",
      "  mean_baselines: -95.4312744140625\n",
      "  mean_log_prob: -1.2219599485397339\n",
      "  performance: -205.05287064819342\n",
      "  policy_loss: -159.6426544189453\n",
      "  total_episodes: 1387\n",
      "  total_time: 114.38047456741333\n",
      "  total_timesteps: 160962\n",
      "  training_episode_length:\n",
      "    max: 266.0\n",
      "    mean: 137.4931506849315\n",
      "    min: 75.0\n",
      "  training_episode_reward:\n",
      "    max: 0.49772556620857245\n",
      "    mean: -205.05287064819342\n",
      "    min: -633.2941212964122\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 16 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0000001192092896\n",
      "  critic_loss: 75.93504760742188\n",
      "  evaluate_reward: -194.25534917106432\n",
      "  iter_episodes: 76\n",
      "  iter_time: 7.301884651184082\n",
      "  iter_timesteps: 10005\n",
      "  iteration: 16\n",
      "  mean_advantage: -1.7634157467583123e-09\n",
      "  mean_baselines: -94.02886199951172\n",
      "  mean_log_prob: -1.2345654964447021\n",
      "  performance: -189.3690532385066\n",
      "  policy_loss: -250.7748260498047\n",
      "  total_episodes: 1463\n",
      "  total_time: 121.68235921859741\n",
      "  total_timesteps: 170967\n",
      "  training_episode_length:\n",
      "    max: 216.0\n",
      "    mean: 131.64473684210526\n",
      "    min: 73.0\n",
      "  training_episode_reward:\n",
      "    max: 27.386566958270592\n",
      "    mean: -189.3690532385066\n",
      "    min: -543.9323241271243\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 17 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 58.004399604797364\n",
      "  evaluate_reward: -187.85038313356236\n",
      "  iter_episodes: 80\n",
      "  iter_time: 7.299368381500244\n",
      "  iter_timesteps: 10050\n",
      "  iteration: 17\n",
      "  mean_advantage: -3.5110396812143563e-09\n",
      "  mean_baselines: -90.22127532958984\n",
      "  mean_log_prob: -1.2377140522003174\n",
      "  performance: -171.03970699190955\n",
      "  policy_loss: -428.583740234375\n",
      "  total_episodes: 1543\n",
      "  total_time: 128.98172760009766\n",
      "  total_timesteps: 181017\n",
      "  training_episode_length:\n",
      "    max: 229.0\n",
      "    mean: 125.625\n",
      "    min: 77.0\n",
      "  training_episode_reward:\n",
      "    max: 7.666002740424759\n",
      "    mean: -171.03970699190955\n",
      "    min: -444.9677904117437\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 18 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 44.54816951751709\n",
      "  evaluate_reward: -94.6881162091572\n",
      "  iter_episodes: 72\n",
      "  iter_time: 8.15761137008667\n",
      "  iter_timesteps: 10119\n",
      "  iteration: 18\n",
      "  mean_advantage: -6.0317377759133706e-09\n",
      "  mean_baselines: -83.84420013427734\n",
      "  mean_log_prob: -1.2409045696258545\n",
      "  performance: -141.24292346057854\n",
      "  policy_loss: -405.0762023925781\n",
      "  total_episodes: 1615\n",
      "  total_time: 137.13933897018433\n",
      "  total_timesteps: 191136\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 140.54166666666666\n",
      "    min: 74.0\n",
      "  training_episode_reward:\n",
      "    max: 29.593762415377313\n",
      "    mean: -141.24292346057854\n",
      "    min: -385.1856187084527\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 19 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0000001192092896\n",
      "  critic_loss: 66.3291833114624\n",
      "  evaluate_reward: -144.0164595692599\n",
      "  iter_episodes: 78\n",
      "  iter_time: 7.3679518699646\n",
      "  iter_timesteps: 10070\n",
      "  iteration: 19\n",
      "  mean_advantage: -6.564205623504904e-09\n",
      "  mean_baselines: -80.87996673583984\n",
      "  mean_log_prob: -1.2265092134475708\n",
      "  performance: -140.55980935877332\n",
      "  policy_loss: -392.60791015625\n",
      "  total_episodes: 1693\n",
      "  total_time: 144.50729084014893\n",
      "  total_timesteps: 201206\n",
      "  training_episode_length:\n",
      "    max: 318.0\n",
      "    mean: 129.10256410256412\n",
      "    min: 74.0\n",
      "  training_episode_reward:\n",
      "    max: 29.219951953434986\n",
      "    mean: -140.55980935877332\n",
      "    min: -422.62235867485873\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 20 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999999403953552\n",
      "  critic_loss: 60.087369689941404\n",
      "  evaluate_reward: -185.60863327077377\n",
      "  iter_episodes: 78\n",
      "  iter_time: 7.3225743770599365\n",
      "  iter_timesteps: 10028\n",
      "  iteration: 20\n",
      "  mean_advantage: 5.920046675100821e-09\n",
      "  mean_baselines: -75.86406707763672\n",
      "  mean_log_prob: -1.2204322814941406\n",
      "  performance: -155.0931844125608\n",
      "  policy_loss: -393.88507080078125\n",
      "  total_episodes: 1771\n",
      "  total_time: 151.82986521720886\n",
      "  total_timesteps: 211234\n",
      "  training_episode_length:\n",
      "    max: 229.0\n",
      "    mean: 128.56410256410257\n",
      "    min: 78.0\n",
      "  training_episode_reward:\n",
      "    max: 13.304383024740844\n",
      "    mean: -155.0931844125608\n",
      "    min: -564.3219210243362\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 21 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999998807907104\n",
      "  critic_loss: 50.13783332824707\n",
      "  evaluate_reward: -113.0264488654453\n",
      "  iter_episodes: 66\n",
      "  iter_time: 9.716933250427246\n",
      "  iter_timesteps: 10110\n",
      "  iteration: 21\n",
      "  mean_advantage: -3.4902027934435864e-09\n",
      "  mean_baselines: -84.11324310302734\n",
      "  mean_log_prob: -1.2288799285888672\n",
      "  performance: -150.90642053141315\n",
      "  policy_loss: -347.079345703125\n",
      "  total_episodes: 1837\n",
      "  total_time: 161.5467984676361\n",
      "  total_timesteps: 221344\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 153.1818181818182\n",
      "    min: 80.0\n",
      "  training_episode_reward:\n",
      "    max: 21.579849876967927\n",
      "    mean: -150.90642053141315\n",
      "    min: -417.6462684603409\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 22 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 39.80994514465332\n",
      "  evaluate_reward: -190.79392430173453\n",
      "  iter_episodes: 63\n",
      "  iter_time: 10.15342116355896\n",
      "  iter_timesteps: 10047\n",
      "  iteration: 22\n",
      "  mean_advantage: 4.674874265475637e-09\n",
      "  mean_baselines: -73.27388000488281\n",
      "  mean_log_prob: -1.2365529537200928\n",
      "  performance: -127.40894179992158\n",
      "  policy_loss: -486.1586608886719\n",
      "  total_episodes: 1900\n",
      "  total_time: 171.70021963119507\n",
      "  total_timesteps: 231391\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 159.47619047619048\n",
      "    min: 77.0\n",
      "  training_episode_reward:\n",
      "    max: 71.23155098227649\n",
      "    mean: -127.40894179992158\n",
      "    min: -407.1340452861777\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 23 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 53.81431579589844\n",
      "  evaluate_reward: -231.8129879583681\n",
      "  iter_episodes: 64\n",
      "  iter_time: 9.815410137176514\n",
      "  iter_timesteps: 10008\n",
      "  iteration: 23\n",
      "  mean_advantage: -2.954027245039015e-09\n",
      "  mean_baselines: -68.73603057861328\n",
      "  mean_log_prob: -1.2232489585876465\n",
      "  performance: -139.1777540661091\n",
      "  policy_loss: -442.68341064453125\n",
      "  total_episodes: 1964\n",
      "  total_time: 181.51562976837158\n",
      "  total_timesteps: 241399\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 156.375\n",
      "    min: 82.0\n",
      "  training_episode_reward:\n",
      "    max: 76.97180427217528\n",
      "    mean: -139.1777540661091\n",
      "    min: -403.31126432062445\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 24 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 39.739282302856445\n",
      "  evaluate_reward: -163.83687552172574\n",
      "  iter_episodes: 81\n",
      "  iter_time: 7.490695476531982\n",
      "  iter_timesteps: 10134\n",
      "  iteration: 24\n",
      "  mean_advantage: -8.469576151526326e-09\n",
      "  mean_baselines: -68.71316528320312\n",
      "  mean_log_prob: -1.1962897777557373\n",
      "  performance: -132.4379440878038\n",
      "  policy_loss: -829.68359375\n",
      "  total_episodes: 2045\n",
      "  total_time: 189.00632524490356\n",
      "  total_timesteps: 251533\n",
      "  training_episode_length:\n",
      "    max: 209.0\n",
      "    mean: 125.11111111111111\n",
      "    min: 63.0\n",
      "  training_episode_reward:\n",
      "    max: 14.513688138756109\n",
      "    mean: -132.4379440878038\n",
      "    min: -404.00983364193195\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 25 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 44.99323665618896\n",
      "  evaluate_reward: -173.87973060736\n",
      "  iter_episodes: 74\n",
      "  iter_time: 8.416280269622803\n",
      "  iter_timesteps: 10157\n",
      "  iteration: 25\n",
      "  mean_advantage: 6.713371636379861e-09\n",
      "  mean_baselines: -74.90570068359375\n",
      "  mean_log_prob: -1.1886855363845825\n",
      "  performance: -129.39469389929232\n",
      "  policy_loss: -594.6629638671875\n",
      "  total_episodes: 2119\n",
      "  total_time: 197.42260551452637\n",
      "  total_timesteps: 261690\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 137.25675675675674\n",
      "    min: 75.0\n",
      "  training_episode_reward:\n",
      "    max: 20.748139065596163\n",
      "    mean: -129.39469389929232\n",
      "    min: -384.55085552313557\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 26 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 37.35932525634766\n",
      "  evaluate_reward: -153.68270455962465\n",
      "  iter_episodes: 67\n",
      "  iter_time: 10.267022848129272\n",
      "  iter_timesteps: 10447\n",
      "  iteration: 26\n",
      "  mean_advantage: 3.1037548087198275e-09\n",
      "  mean_baselines: -74.15038299560547\n",
      "  mean_log_prob: -1.2009230852127075\n",
      "  performance: -140.41246593439428\n",
      "  policy_loss: -368.8288879394531\n",
      "  total_episodes: 2186\n",
      "  total_time: 207.68962836265564\n",
      "  total_timesteps: 272137\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 155.92537313432837\n",
      "    min: 79.0\n",
      "  training_episode_reward:\n",
      "    max: 37.656636673644215\n",
      "    mean: -140.41246593439428\n",
      "    min: -427.73773706483786\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 27 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999999403953552\n",
      "  critic_loss: 39.033363800048825\n",
      "  evaluate_reward: -55.25894264463153\n",
      "  iter_episodes: 78\n",
      "  iter_time: 7.291029453277588\n",
      "  iter_timesteps: 10019\n",
      "  iteration: 27\n",
      "  mean_advantage: 5.71119485037741e-10\n",
      "  mean_baselines: -66.26472473144531\n",
      "  mean_log_prob: -1.164488673210144\n",
      "  performance: -130.12715564320473\n",
      "  policy_loss: -682.2236938476562\n",
      "  total_episodes: 2264\n",
      "  total_time: 214.98065781593323\n",
      "  total_timesteps: 282156\n",
      "  training_episode_length:\n",
      "    max: 279.0\n",
      "    mean: 128.44871794871796\n",
      "    min: 70.0\n",
      "  training_episode_reward:\n",
      "    max: 74.52999089415124\n",
      "    mean: -130.12715564320473\n",
      "    min: -331.0206228536379\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 28 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 36.153372344970705\n",
      "  evaluate_reward: -113.176202112081\n",
      "  iter_episodes: 58\n",
      "  iter_time: 11.102317810058594\n",
      "  iter_timesteps: 10192\n",
      "  iteration: 28\n",
      "  mean_advantage: -1.8714174654377302e-10\n",
      "  mean_baselines: -74.45647430419922\n",
      "  mean_log_prob: -1.194725751876831\n",
      "  performance: -117.97085525450998\n",
      "  policy_loss: -576.9783325195312\n",
      "  total_episodes: 2322\n",
      "  total_time: 226.08297562599182\n",
      "  total_timesteps: 292348\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 175.72413793103448\n",
      "    min: 81.0\n",
      "  training_episode_reward:\n",
      "    max: 57.469792765589396\n",
      "    mean: -117.97085525450998\n",
      "    min: -366.8651153566528\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 29 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 35.642529029846195\n",
      "  evaluate_reward: -93.8382602779338\n",
      "  iter_episodes: 69\n",
      "  iter_time: 8.45549726486206\n",
      "  iter_timesteps: 10084\n",
      "  iteration: 29\n",
      "  mean_advantage: -6.324570644977712e-09\n",
      "  mean_baselines: -62.021331787109375\n",
      "  mean_log_prob: -1.161155104637146\n",
      "  performance: -101.18175604400722\n",
      "  policy_loss: -797.760009765625\n",
      "  total_episodes: 2391\n",
      "  total_time: 234.53847289085388\n",
      "  total_timesteps: 302432\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 146.14492753623188\n",
      "    min: 89.0\n",
      "  training_episode_reward:\n",
      "    max: 66.89902171665176\n",
      "    mean: -101.18175604400722\n",
      "    min: -331.67721505373373\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 30 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0000001192092896\n",
      "  critic_loss: 27.983212871551515\n",
      "  evaluate_reward: -203.18880440832518\n",
      "  iter_episodes: 63\n",
      "  iter_time: 10.284638166427612\n",
      "  iter_timesteps: 10221\n",
      "  iteration: 30\n",
      "  mean_advantage: 2.064381554589545e-09\n",
      "  mean_baselines: -62.837093353271484\n",
      "  mean_log_prob: -1.153707504272461\n",
      "  performance: -94.8014997694366\n",
      "  policy_loss: -886.7813720703125\n",
      "  total_episodes: 2454\n",
      "  total_time: 244.8231110572815\n",
      "  total_timesteps: 312653\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 162.23809523809524\n",
      "    min: 89.0\n",
      "  training_episode_reward:\n",
      "    max: 88.54938879591771\n",
      "    mean: -94.8014997694366\n",
      "    min: -314.69758516624887\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 31 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 28.190464115142824\n",
      "  evaluate_reward: -65.42695576414894\n",
      "  iter_episodes: 66\n",
      "  iter_time: 9.554978132247925\n",
      "  iter_timesteps: 10115\n",
      "  iteration: 31\n",
      "  mean_advantage: -4.714158508001276e-10\n",
      "  mean_baselines: -54.48966979980469\n",
      "  mean_log_prob: -1.1536260843276978\n",
      "  performance: -94.12904019496693\n",
      "  policy_loss: -1038.907470703125\n",
      "  total_episodes: 2520\n",
      "  total_time: 254.37808918952942\n",
      "  total_timesteps: 322768\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 153.25757575757575\n",
      "    min: 79.0\n",
      "  training_episode_reward:\n",
      "    max: 56.726754812457756\n",
      "    mean: -94.12904019496693\n",
      "    min: -290.6214118368681\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 32 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999999403953552\n",
      "  critic_loss: 25.632442817687988\n",
      "  evaluate_reward: -105.35635635667222\n",
      "  iter_episodes: 71\n",
      "  iter_time: 9.237018346786499\n",
      "  iter_timesteps: 10121\n",
      "  iteration: 32\n",
      "  mean_advantage: 6.077659708836336e-09\n",
      "  mean_baselines: -48.88717269897461\n",
      "  mean_log_prob: -1.1102921962738037\n",
      "  performance: -95.10796980818064\n",
      "  policy_loss: -1195.7825927734375\n",
      "  total_episodes: 2591\n",
      "  total_time: 263.6151075363159\n",
      "  total_timesteps: 332889\n",
      "  training_episode_length:\n",
      "    max: 243.0\n",
      "    mean: 142.54929577464787\n",
      "    min: 80.0\n",
      "  training_episode_reward:\n",
      "    max: 42.97314673344968\n",
      "    mean: -95.10796980818064\n",
      "    min: -294.9613404787797\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 33 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 33.00858028411865\n",
      "  evaluate_reward: -23.01168283265178\n",
      "  iter_episodes: 61\n",
      "  iter_time: 9.199581384658813\n",
      "  iter_timesteps: 10083\n",
      "  iteration: 33\n",
      "  mean_advantage: -6.904515181815896e-09\n",
      "  mean_baselines: -50.588199615478516\n",
      "  mean_log_prob: -1.1282275915145874\n",
      "  performance: -88.71656079080527\n",
      "  policy_loss: -877.9717407226562\n",
      "  total_episodes: 2652\n",
      "  total_time: 272.81468892097473\n",
      "  total_timesteps: 342972\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 165.29508196721312\n",
      "    min: 86.0\n",
      "  training_episode_reward:\n",
      "    max: 41.89154528695374\n",
      "    mean: -88.71656079080527\n",
      "    min: -307.0811254576909\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 34 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 30.952097644805907\n",
      "  evaluate_reward: -12.589196974970687\n",
      "  iter_episodes: 51\n",
      "  iter_time: 10.998583316802979\n",
      "  iter_timesteps: 10133\n",
      "  iteration: 34\n",
      "  mean_advantage: -3.141111148963205e-09\n",
      "  mean_baselines: -49.40108871459961\n",
      "  mean_log_prob: -1.1390544176101685\n",
      "  performance: -93.40017357193089\n",
      "  policy_loss: -817.5015258789062\n",
      "  total_episodes: 2703\n",
      "  total_time: 283.8132722377777\n",
      "  total_timesteps: 353105\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 198.68627450980392\n",
      "    min: 86.0\n",
      "  training_episode_reward:\n",
      "    max: 64.33984939448108\n",
      "    mean: -93.40017357193089\n",
      "    min: -325.3621792708469\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 35 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 32.492202281951904\n",
      "  evaluate_reward: -202.8182267153898\n",
      "  iter_episodes: 52\n",
      "  iter_time: 12.398818254470825\n",
      "  iter_timesteps: 10045\n",
      "  iteration: 35\n",
      "  mean_advantage: -6.17704687400078e-09\n",
      "  mean_baselines: -56.36127471923828\n",
      "  mean_log_prob: -1.1011732816696167\n",
      "  performance: -94.73420519584805\n",
      "  policy_loss: -865.777587890625\n",
      "  total_episodes: 2755\n",
      "  total_time: 296.21209049224854\n",
      "  total_timesteps: 363150\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 193.17307692307693\n",
      "    min: 94.0\n",
      "  training_episode_reward:\n",
      "    max: 53.71273866055327\n",
      "    mean: -94.73420519584805\n",
      "    min: -379.32324938397977\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 36 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 31.881580772399904\n",
      "  evaluate_reward: -58.20703354764422\n",
      "  iter_episodes: 47\n",
      "  iter_time: 10.58151888847351\n",
      "  iter_timesteps: 10060\n",
      "  iteration: 36\n",
      "  mean_advantage: -5.87751580738427e-09\n",
      "  mean_baselines: -56.682456970214844\n",
      "  mean_log_prob: -1.0805060863494873\n",
      "  performance: -116.6330427387502\n",
      "  policy_loss: -698.2796020507812\n",
      "  total_episodes: 2802\n",
      "  total_time: 306.79360938072205\n",
      "  total_timesteps: 373210\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 214.04255319148936\n",
      "    min: 89.0\n",
      "  training_episode_reward:\n",
      "    max: 40.046283302388304\n",
      "    mean: -116.6330427387502\n",
      "    min: -296.90890653832855\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 37 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 36.84718452453613\n",
      "  evaluate_reward: -225.70309343849013\n",
      "  iter_episodes: 50\n",
      "  iter_time: 10.16999077796936\n",
      "  iter_timesteps: 10038\n",
      "  iteration: 37\n",
      "  mean_advantage: -1.8526249423445051e-09\n",
      "  mean_baselines: -54.91330337524414\n",
      "  mean_log_prob: -1.0714020729064941\n",
      "  performance: -111.97588447074527\n",
      "  policy_loss: -708.89794921875\n",
      "  total_episodes: 2852\n",
      "  total_time: 316.9636001586914\n",
      "  total_timesteps: 383248\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 200.76\n",
      "    min: 95.0\n",
      "  training_episode_reward:\n",
      "    max: 84.11595140781984\n",
      "    mean: -111.97588447074527\n",
      "    min: -297.35109036766744\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 38 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 30.291159629821777\n",
      "  evaluate_reward: -53.6829127038484\n",
      "  iter_episodes: 44\n",
      "  iter_time: 10.726678371429443\n",
      "  iter_timesteps: 10007\n",
      "  iteration: 38\n",
      "  mean_advantage: 3.2521370041393993e-09\n",
      "  mean_baselines: -52.457523345947266\n",
      "  mean_log_prob: -1.0704728364944458\n",
      "  performance: -117.98529955347841\n",
      "  policy_loss: -627.1683349609375\n",
      "  total_episodes: 2896\n",
      "  total_time: 327.69027853012085\n",
      "  total_timesteps: 393255\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 227.4318181818182\n",
      "    min: 84.0\n",
      "  training_episode_reward:\n",
      "    max: 32.16833240107912\n",
      "    mean: -117.98529955347841\n",
      "    min: -385.72628844916613\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 39 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999999403953552\n",
      "  critic_loss: 32.900594291687014\n",
      "  evaluate_reward: -97.4770231532909\n",
      "  iter_episodes: 52\n",
      "  iter_time: 10.229344129562378\n",
      "  iter_timesteps: 10174\n",
      "  iteration: 39\n",
      "  mean_advantage: -9.373642140264593e-11\n",
      "  mean_baselines: -51.59746551513672\n",
      "  mean_log_prob: -1.0405142307281494\n",
      "  performance: -94.71277498334324\n",
      "  policy_loss: -750.9193115234375\n",
      "  total_episodes: 2948\n",
      "  total_time: 337.9196226596832\n",
      "  total_timesteps: 403429\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 195.65384615384616\n",
      "    min: 91.0\n",
      "  training_episode_reward:\n",
      "    max: 31.845850049670315\n",
      "    mean: -94.71277498334324\n",
      "    min: -269.0739380576731\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 40 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 35.356171226501466\n",
      "  evaluate_reward: -93.77470415763518\n",
      "  iter_episodes: 40\n",
      "  iter_time: 13.452853202819824\n",
      "  iter_timesteps: 10113\n",
      "  iteration: 40\n",
      "  mean_advantage: 2.734752868605028e-09\n",
      "  mean_baselines: -52.56947708129883\n",
      "  mean_log_prob: -1.0602253675460815\n",
      "  performance: -99.76143279539048\n",
      "  policy_loss: -652.320556640625\n",
      "  total_episodes: 2988\n",
      "  total_time: 351.37247586250305\n",
      "  total_timesteps: 413542\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 252.825\n",
      "    min: 93.0\n",
      "  training_episode_reward:\n",
      "    max: 57.43376380332016\n",
      "    mean: -99.76143279539048\n",
      "    min: -438.60533122116686\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 41 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999999403953552\n",
      "  critic_loss: 39.601368255615235\n",
      "  evaluate_reward: -78.54453958385159\n",
      "  iter_episodes: 38\n",
      "  iter_time: 13.096848011016846\n",
      "  iter_timesteps: 10093\n",
      "  iteration: 41\n",
      "  mean_advantage: -5.055144747245777e-09\n",
      "  mean_baselines: -55.55330276489258\n",
      "  mean_log_prob: -1.0675268173217773\n",
      "  performance: -120.53495034176302\n",
      "  policy_loss: -490.8829040527344\n",
      "  total_episodes: 3026\n",
      "  total_time: 364.4693238735199\n",
      "  total_timesteps: 423635\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 265.60526315789474\n",
      "    min: 106.0\n",
      "  training_episode_reward:\n",
      "    max: 81.46733803168269\n",
      "    mean: -120.53495034176302\n",
      "    min: -315.14360126810936\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 42 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999999403953552\n",
      "  critic_loss: 25.547900085449218\n",
      "  evaluate_reward: -107.1342076870254\n",
      "  iter_episodes: 46\n",
      "  iter_time: 13.04395055770874\n",
      "  iter_timesteps: 10178\n",
      "  iteration: 42\n",
      "  mean_advantage: 7.1680177349264795e-09\n",
      "  mean_baselines: -46.40340042114258\n",
      "  mean_log_prob: -1.0470162630081177\n",
      "  performance: -69.15867440699385\n",
      "  policy_loss: -966.9673461914062\n",
      "  total_episodes: 3072\n",
      "  total_time: 377.51327443122864\n",
      "  total_timesteps: 433813\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 221.2608695652174\n",
      "    min: 95.0\n",
      "  training_episode_reward:\n",
      "    max: 42.9504567848727\n",
      "    mean: -69.15867440699385\n",
      "    min: -275.69965937992083\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 43 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 23.996623725891112\n",
      "  evaluate_reward: -108.89216318765394\n",
      "  iter_episodes: 32\n",
      "  iter_time: 15.741141319274902\n",
      "  iter_timesteps: 10155\n",
      "  iteration: 43\n",
      "  mean_advantage: 3.991251329438228e-09\n",
      "  mean_baselines: -44.7440185546875\n",
      "  mean_log_prob: -1.100730299949646\n",
      "  performance: -69.86518458177818\n",
      "  policy_loss: -751.9183349609375\n",
      "  total_episodes: 3104\n",
      "  total_time: 393.25441575050354\n",
      "  total_timesteps: 443968\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 317.34375\n",
      "    min: 117.0\n",
      "  training_episode_reward:\n",
      "    max: 104.23076114248671\n",
      "    mean: -69.86518458177818\n",
      "    min: -320.16243173645967\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 44 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 29.416449546813965\n",
      "  evaluate_reward: -98.41781010670525\n",
      "  iter_episodes: 46\n",
      "  iter_time: 13.362321138381958\n",
      "  iter_timesteps: 10066\n",
      "  iteration: 44\n",
      "  mean_advantage: 1.4685030969019408e-09\n",
      "  mean_baselines: -43.907875061035156\n",
      "  mean_log_prob: -1.0020358562469482\n",
      "  performance: -97.5133386789583\n",
      "  policy_loss: -937.818115234375\n",
      "  total_episodes: 3150\n",
      "  total_time: 406.6167368888855\n",
      "  total_timesteps: 454034\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 218.82608695652175\n",
      "    min: 104.0\n",
      "  training_episode_reward:\n",
      "    max: 40.07332577065009\n",
      "    mean: -97.5133386789583\n",
      "    min: -253.88004567505996\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 45 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 26.60172645568848\n",
      "  evaluate_reward: -73.9399125234316\n",
      "  iter_episodes: 50\n",
      "  iter_time: 12.583258390426636\n",
      "  iter_timesteps: 10978\n",
      "  iteration: 45\n",
      "  mean_advantage: -5.6683595595075076e-09\n",
      "  mean_baselines: -40.00204086303711\n",
      "  mean_log_prob: -0.9830333590507507\n",
      "  performance: -90.33252761437616\n",
      "  policy_loss: -1109.572021484375\n",
      "  total_episodes: 3200\n",
      "  total_time: 419.19999527931213\n",
      "  total_timesteps: 465012\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 219.56\n",
      "    min: 111.0\n",
      "  training_episode_reward:\n",
      "    max: 55.21109609384726\n",
      "    mean: -90.33252761437616\n",
      "    min: -278.59201938490014\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 46 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0000001192092896\n",
      "  critic_loss: 29.025146942138672\n",
      "  evaluate_reward: -8.264139694001301\n",
      "  iter_episodes: 40\n",
      "  iter_time: 15.340892553329468\n",
      "  iter_timesteps: 10127\n",
      "  iteration: 46\n",
      "  mean_advantage: -5.650287349112659e-09\n",
      "  mean_baselines: -40.02862548828125\n",
      "  mean_log_prob: -1.0268473625183105\n",
      "  performance: -69.30460760696698\n",
      "  policy_loss: -1082.8404541015625\n",
      "  total_episodes: 3240\n",
      "  total_time: 434.5408878326416\n",
      "  total_timesteps: 475139\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 253.175\n",
      "    min: 115.0\n",
      "  training_episode_reward:\n",
      "    max: 72.91641179337299\n",
      "    mean: -69.30460760696698\n",
      "    min: -271.9139355456289\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 47 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0000001192092896\n",
      "  critic_loss: 26.92170747756958\n",
      "  evaluate_reward: -71.76092699562975\n",
      "  iter_episodes: 44\n",
      "  iter_time: 12.546274662017822\n",
      "  iter_timesteps: 10027\n",
      "  iteration: 47\n",
      "  mean_advantage: -1.1888829476447427e-08\n",
      "  mean_baselines: -36.76054382324219\n",
      "  mean_log_prob: -0.9972457885742188\n",
      "  performance: -60.051856258620724\n",
      "  policy_loss: -939.9169311523438\n",
      "  total_episodes: 3284\n",
      "  total_time: 447.0871624946594\n",
      "  total_timesteps: 485166\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 227.88636363636363\n",
      "    min: 110.0\n",
      "  training_episode_reward:\n",
      "    max: 89.5128049175616\n",
      "    mean: -60.051856258620724\n",
      "    min: -254.20430517814734\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 48 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 18.594137382507324\n",
      "  evaluate_reward: -73.91302522262279\n",
      "  iter_episodes: 31\n",
      "  iter_time: 15.533342599868774\n",
      "  iter_timesteps: 10414\n",
      "  iteration: 48\n",
      "  mean_advantage: -1.3232758000469858e-08\n",
      "  mean_baselines: -37.89102554321289\n",
      "  mean_log_prob: -1.0703932046890259\n",
      "  performance: -35.98795701121784\n",
      "  policy_loss: -1084.7822265625\n",
      "  total_episodes: 3315\n",
      "  total_time: 462.6205050945282\n",
      "  total_timesteps: 495580\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 335.93548387096774\n",
      "    min: 112.0\n",
      "  training_episode_reward:\n",
      "    max: 99.6912929907262\n",
      "    mean: -35.98795701121784\n",
      "    min: -221.48301858379426\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 49 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999998807907104\n",
      "  critic_loss: 28.231113967895507\n",
      "  evaluate_reward: -50.106092195875945\n",
      "  iter_episodes: 45\n",
      "  iter_time: 10.028991460800171\n",
      "  iter_timesteps: 10133\n",
      "  iteration: 49\n",
      "  mean_advantage: -8.188065336867112e-09\n",
      "  mean_baselines: -30.86126136779785\n",
      "  mean_log_prob: -0.952605128288269\n",
      "  performance: -83.06840073667249\n",
      "  policy_loss: -988.24169921875\n",
      "  total_episodes: 3360\n",
      "  total_time: 472.64949655532837\n",
      "  total_timesteps: 505713\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 225.17777777777778\n",
      "    min: 102.0\n",
      "  training_episode_reward:\n",
      "    max: 57.01916384936197\n",
      "    mean: -83.06840073667249\n",
      "    min: -276.0207854823544\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 50 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 19.307406902313232\n",
      "  evaluate_reward: -83.68753761016667\n",
      "  iter_episodes: 34\n",
      "  iter_time: 16.51712965965271\n",
      "  iter_timesteps: 10649\n",
      "  iteration: 50\n",
      "  mean_advantage: 5.7315387991252464e-09\n",
      "  mean_baselines: -32.046993255615234\n",
      "  mean_log_prob: -1.0384430885314941\n",
      "  performance: -45.29018591952147\n",
      "  policy_loss: -1180.494140625\n",
      "  total_episodes: 3394\n",
      "  total_time: 489.1666262149811\n",
      "  total_timesteps: 516362\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 313.20588235294116\n",
      "    min: 118.0\n",
      "  training_episode_reward:\n",
      "    max: 111.01517916905615\n",
      "    mean: -45.29018591952147\n",
      "    min: -215.09596861911285\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 51 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0000001192092896\n",
      "  critic_loss: 20.298900871276857\n",
      "  evaluate_reward: -36.482479577034404\n",
      "  iter_episodes: 35\n",
      "  iter_time: 13.77067518234253\n",
      "  iter_timesteps: 10721\n",
      "  iteration: 51\n",
      "  mean_advantage: -2.4907080842240248e-09\n",
      "  mean_baselines: -31.749591827392578\n",
      "  mean_log_prob: -0.9909846782684326\n",
      "  performance: -72.34644521585325\n",
      "  policy_loss: -982.6920776367188\n",
      "  total_episodes: 3429\n",
      "  total_time: 502.9373013973236\n",
      "  total_timesteps: 527083\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 306.3142857142857\n",
      "    min: 112.0\n",
      "  training_episode_reward:\n",
      "    max: 121.72126498787273\n",
      "    mean: -72.34644521585325\n",
      "    min: -303.4795027374664\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 52 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 15.790877113342285\n",
      "  evaluate_reward: -132.33680925200602\n",
      "  iter_episodes: 20\n",
      "  iter_time: 19.3906033039093\n",
      "  iter_timesteps: 10762\n",
      "  iteration: 52\n",
      "  mean_advantage: -4.164903533165898e-09\n",
      "  mean_baselines: -32.4702262878418\n",
      "  mean_log_prob: -1.0936490297317505\n",
      "  performance: -11.454731819786328\n",
      "  policy_loss: -1091.177001953125\n",
      "  total_episodes: 3449\n",
      "  total_time: 522.3279047012329\n",
      "  total_timesteps: 537845\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 538.1\n",
      "    min: 142.0\n",
      "  training_episode_reward:\n",
      "    max: 111.69799114641143\n",
      "    mean: -11.454731819786328\n",
      "    min: -254.53224317883053\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 53 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 18.107760391235352\n",
      "  evaluate_reward: -29.849421793546753\n",
      "  iter_episodes: 33\n",
      "  iter_time: 14.23895263671875\n",
      "  iter_timesteps: 10211\n",
      "  iteration: 53\n",
      "  mean_advantage: -3.875965326471942e-09\n",
      "  mean_baselines: -25.961151123046875\n",
      "  mean_log_prob: -0.9985029697418213\n",
      "  performance: -57.03644058200037\n",
      "  policy_loss: -855.9786376953125\n",
      "  total_episodes: 3482\n",
      "  total_time: 536.5668573379517\n",
      "  total_timesteps: 548056\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 309.42424242424244\n",
      "    min: 144.0\n",
      "  training_episode_reward:\n",
      "    max: 86.78532202630143\n",
      "    mean: -57.03644058200037\n",
      "    min: -219.2933269147671\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 54 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 15.62290934562683\n",
      "  evaluate_reward: 17.66786599837162\n",
      "  iter_episodes: 26\n",
      "  iter_time: 17.345991849899292\n",
      "  iter_timesteps: 10119\n",
      "  iteration: 54\n",
      "  mean_advantage: 4.523803553979633e-09\n",
      "  mean_baselines: -25.37682342529297\n",
      "  mean_log_prob: -0.999785840511322\n",
      "  performance: -59.48490649331715\n",
      "  policy_loss: -953.474609375\n",
      "  total_episodes: 3508\n",
      "  total_time: 553.912849187851\n",
      "  total_timesteps: 558175\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 389.1923076923077\n",
      "    min: 166.0\n",
      "  training_episode_reward:\n",
      "    max: 117.35308234458324\n",
      "    mean: -59.48490649331715\n",
      "    min: -274.3709164621747\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 55 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 15.470563688278197\n",
      "  evaluate_reward: -16.044639402315738\n",
      "  iter_episodes: 26\n",
      "  iter_time: 19.234900951385498\n",
      "  iter_timesteps: 10098\n",
      "  iteration: 55\n",
      "  mean_advantage: -1.3221865824419865e-09\n",
      "  mean_baselines: -23.812339782714844\n",
      "  mean_log_prob: -1.0032609701156616\n",
      "  performance: -19.21559833776778\n",
      "  policy_loss: -836.3995971679688\n",
      "  total_episodes: 3534\n",
      "  total_time: 573.1477501392365\n",
      "  total_timesteps: 568273\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 388.38461538461536\n",
      "    min: 118.0\n",
      "  training_episode_reward:\n",
      "    max: 134.57955372262603\n",
      "    mean: -19.21559833776778\n",
      "    min: -180.26617766023065\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 56 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 11.158804130554199\n",
      "  evaluate_reward: -22.208975889103108\n",
      "  iter_episodes: 22\n",
      "  iter_time: 16.68685555458069\n",
      "  iter_timesteps: 10675\n",
      "  iteration: 56\n",
      "  mean_advantage: -2.9034581405795734e-09\n",
      "  mean_baselines: -23.099777221679688\n",
      "  mean_log_prob: -1.0149388313293457\n",
      "  performance: -22.264824013744793\n",
      "  policy_loss: -821.7472534179688\n",
      "  total_episodes: 3556\n",
      "  total_time: 589.8346056938171\n",
      "  total_timesteps: 578948\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 485.22727272727275\n",
      "    min: 167.0\n",
      "  training_episode_reward:\n",
      "    max: 148.85989491869697\n",
      "    mean: -22.264824013744793\n",
      "    min: -225.10835798706262\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 57 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 11.50431634902954\n",
      "  evaluate_reward: -0.2419204880380562\n",
      "  iter_episodes: 20\n",
      "  iter_time: 19.56163001060486\n",
      "  iter_timesteps: 10462\n",
      "  iteration: 57\n",
      "  mean_advantage: 7.292481618570434e-10\n",
      "  mean_baselines: -20.787248611450195\n",
      "  mean_log_prob: -1.0243520736694336\n",
      "  performance: -5.696391695231666\n",
      "  policy_loss: -779.3646240234375\n",
      "  total_episodes: 3576\n",
      "  total_time: 609.396235704422\n",
      "  total_timesteps: 589410\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 523.1\n",
      "    min: 153.0\n",
      "  training_episode_reward:\n",
      "    max: 130.37331086991094\n",
      "    mean: -5.696391695231666\n",
      "    min: -194.79834438802644\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 58 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0000001192092896\n",
      "  critic_loss: 17.919783191680906\n",
      "  evaluate_reward: 1.1196004955142826\n",
      "  iter_episodes: 29\n",
      "  iter_time: 16.246713399887085\n",
      "  iter_timesteps: 10072\n",
      "  iteration: 58\n",
      "  mean_advantage: -6.627998705432958e-10\n",
      "  mean_baselines: -17.65428924560547\n",
      "  mean_log_prob: -0.96157306432724\n",
      "  performance: -24.323751667277673\n",
      "  policy_loss: -741.4024658203125\n",
      "  total_episodes: 3605\n",
      "  total_time: 625.6429491043091\n",
      "  total_timesteps: 599482\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 347.3103448275862\n",
      "    min: 108.0\n",
      "  training_episode_reward:\n",
      "    max: 118.66833681153517\n",
      "    mean: -24.323751667277673\n",
      "    min: -204.05375016679534\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 59 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 13.720068311691284\n",
      "  evaluate_reward: 24.598211143257778\n",
      "  iter_episodes: 30\n",
      "  iter_time: 16.253251552581787\n",
      "  iter_timesteps: 10710\n",
      "  iteration: 59\n",
      "  mean_advantage: 4.274170795071086e-09\n",
      "  mean_baselines: -16.550710678100586\n",
      "  mean_log_prob: -0.9487413167953491\n",
      "  performance: -18.577605864431348\n",
      "  policy_loss: -820.78125\n",
      "  total_episodes: 3635\n",
      "  total_time: 641.8962006568909\n",
      "  total_timesteps: 610192\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 357.0\n",
      "    min: 152.0\n",
      "  training_episode_reward:\n",
      "    max: 154.20271981341256\n",
      "    mean: -18.577605864431348\n",
      "    min: -226.6791537156634\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 60 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 14.125330839157105\n",
      "  evaluate_reward: -30.051051382992217\n",
      "  iter_episodes: 34\n",
      "  iter_time: 14.962070941925049\n",
      "  iter_timesteps: 10330\n",
      "  iteration: 60\n",
      "  mean_advantage: -7.385667633030835e-09\n",
      "  mean_baselines: -11.580940246582031\n",
      "  mean_log_prob: -0.9486056566238403\n",
      "  performance: -2.6268331380285863\n",
      "  policy_loss: -1059.4248046875\n",
      "  total_episodes: 3669\n",
      "  total_time: 656.8582715988159\n",
      "  total_timesteps: 620522\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 303.8235294117647\n",
      "    min: 137.0\n",
      "  training_episode_reward:\n",
      "    max: 163.11226137394817\n",
      "    mean: -2.6268331380285863\n",
      "    min: -144.7687456640779\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 61 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 10.909221305847169\n",
      "  evaluate_reward: -0.22712873292514074\n",
      "  iter_episodes: 27\n",
      "  iter_time: 16.59004855155945\n",
      "  iter_timesteps: 10865\n",
      "  iteration: 61\n",
      "  mean_advantage: -3.0721216681683927e-09\n",
      "  mean_baselines: -16.630380630493164\n",
      "  mean_log_prob: -0.9841192960739136\n",
      "  performance: 10.335225481458243\n",
      "  policy_loss: -1202.085693359375\n",
      "  total_episodes: 3696\n",
      "  total_time: 673.4483201503754\n",
      "  total_timesteps: 631387\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 402.4074074074074\n",
      "    min: 141.0\n",
      "  training_episode_reward:\n",
      "    max: 133.33725690949757\n",
      "    mean: 10.335225481458243\n",
      "    min: -72.429517604847\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 62 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 10.730503072738648\n",
      "  evaluate_reward: 5.4330140794120325\n",
      "  iter_episodes: 28\n",
      "  iter_time: 16.22197699546814\n",
      "  iter_timesteps: 10985\n",
      "  iteration: 62\n",
      "  mean_advantage: 1.562688867196016e-09\n",
      "  mean_baselines: -13.80728530883789\n",
      "  mean_log_prob: -0.978602945804596\n",
      "  performance: 21.549637740477717\n",
      "  policy_loss: -916.0818481445312\n",
      "  total_episodes: 3724\n",
      "  total_time: 689.6702971458435\n",
      "  total_timesteps: 642372\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 392.32142857142856\n",
      "    min: 120.0\n",
      "  training_episode_reward:\n",
      "    max: 165.8688715925102\n",
      "    mean: 21.549637740477717\n",
      "    min: -162.73657174164526\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 63 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 8.889166374206543\n",
      "  evaluate_reward: 3.531183974883541\n",
      "  iter_episodes: 32\n",
      "  iter_time: 13.383842706680298\n",
      "  iter_timesteps: 10318\n",
      "  iteration: 63\n",
      "  mean_advantage: 6.100262073260865e-09\n",
      "  mean_baselines: -8.465291976928711\n",
      "  mean_log_prob: -0.9640560746192932\n",
      "  performance: 20.884001286182652\n",
      "  policy_loss: -1243.8489990234375\n",
      "  total_episodes: 3756\n",
      "  total_time: 703.0541398525238\n",
      "  total_timesteps: 652690\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 322.4375\n",
      "    min: 99.0\n",
      "  training_episode_reward:\n",
      "    max: 162.87441321638866\n",
      "    mean: 20.884001286182652\n",
      "    min: -31.59107343259548\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 64 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 5.4863712072372435\n",
      "  evaluate_reward: -4.683229755228422\n",
      "  iter_episodes: 19\n",
      "  iter_time: 17.92182469367981\n",
      "  iter_timesteps: 10528\n",
      "  iteration: 64\n",
      "  mean_advantage: -3.261044323465967e-09\n",
      "  mean_baselines: -13.775264739990234\n",
      "  mean_log_prob: -1.0193647146224976\n",
      "  performance: 45.97093006111279\n",
      "  policy_loss: -965.1148681640625\n",
      "  total_episodes: 3775\n",
      "  total_time: 720.9759645462036\n",
      "  total_timesteps: 663218\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 554.1052631578947\n",
      "    min: 109.0\n",
      "  training_episode_reward:\n",
      "    max: 129.28098445678262\n",
      "    mean: 45.97093006111279\n",
      "    min: -26.428745899900093\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 65 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 9.19834885597229\n",
      "  evaluate_reward: 41.44076085578105\n",
      "  iter_episodes: 22\n",
      "  iter_time: 17.135212659835815\n",
      "  iter_timesteps: 10368\n",
      "  iteration: 65\n",
      "  mean_advantage: -2.2075794525733272e-09\n",
      "  mean_baselines: -7.882045269012451\n",
      "  mean_log_prob: -1.003705620765686\n",
      "  performance: 23.969322463693914\n",
      "  policy_loss: -856.9675903320312\n",
      "  total_episodes: 3797\n",
      "  total_time: 738.1111772060394\n",
      "  total_timesteps: 673586\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 471.27272727272725\n",
      "    min: 149.0\n",
      "  training_episode_reward:\n",
      "    max: 132.10757247432485\n",
      "    mean: 23.969322463693914\n",
      "    min: -73.47164246976371\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 66 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 7.674488935470581\n",
      "  evaluate_reward: -17.037610759195854\n",
      "  iter_episodes: 22\n",
      "  iter_time: 23.139885663986206\n",
      "  iter_timesteps: 10972\n",
      "  iteration: 66\n",
      "  mean_advantage: -1.651459302642877e-09\n",
      "  mean_baselines: -8.836444854736328\n",
      "  mean_log_prob: -0.974780261516571\n",
      "  performance: 40.73350720111396\n",
      "  policy_loss: -870.6171875\n",
      "  total_episodes: 3819\n",
      "  total_time: 761.2510628700256\n",
      "  total_timesteps: 684558\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 498.72727272727275\n",
      "    min: 151.0\n",
      "  training_episode_reward:\n",
      "    max: 132.800200255145\n",
      "    mean: 40.73350720111396\n",
      "    min: -39.235805383709845\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 67 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 5.92860179901123\n",
      "  evaluate_reward: -6.368248892170826\n",
      "  iter_episodes: 16\n",
      "  iter_time: 19.13381552696228\n",
      "  iter_timesteps: 10018\n",
      "  iteration: 67\n",
      "  mean_advantage: 3.1414706391785785e-09\n",
      "  mean_baselines: -7.7825026512146\n",
      "  mean_log_prob: -0.9960013628005981\n",
      "  performance: 48.299414576508354\n",
      "  policy_loss: -925.7841796875\n",
      "  total_episodes: 3835\n",
      "  total_time: 780.3848783969879\n",
      "  total_timesteps: 694576\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 626.125\n",
      "    min: 198.0\n",
      "  training_episode_reward:\n",
      "    max: 163.75579353949013\n",
      "    mean: 48.299414576508354\n",
      "    min: -41.64482272342361\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 68 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 6.0856701755523686\n",
      "  evaluate_reward: 40.04644857858378\n",
      "  iter_episodes: 14\n",
      "  iter_time: 24.18005609512329\n",
      "  iter_timesteps: 10354\n",
      "  iteration: 68\n",
      "  mean_advantage: -2.6710986755773547e-09\n",
      "  mean_baselines: -9.678937911987305\n",
      "  mean_log_prob: -1.0165960788726807\n",
      "  performance: 45.36747868945713\n",
      "  policy_loss: -962.531494140625\n",
      "  total_episodes: 3849\n",
      "  total_time: 804.5649344921112\n",
      "  total_timesteps: 704930\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 739.5714285714286\n",
      "    min: 152.0\n",
      "  training_episode_reward:\n",
      "    max: 137.36704861827866\n",
      "    mean: 45.36747868945713\n",
      "    min: -67.96974891020223\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 69 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 7.684360952377319\n",
      "  evaluate_reward: 3.7408845998817184\n",
      "  iter_episodes: 16\n",
      "  iter_time: 23.175336360931396\n",
      "  iter_timesteps: 10266\n",
      "  iteration: 69\n",
      "  mean_advantage: 5.75957592729992e-09\n",
      "  mean_baselines: -4.523584365844727\n",
      "  mean_log_prob: -0.9681015014648438\n",
      "  performance: 36.9312453949086\n",
      "  policy_loss: -762.7704467773438\n",
      "  total_episodes: 3865\n",
      "  total_time: 827.7402708530426\n",
      "  total_timesteps: 715196\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 641.625\n",
      "    min: 169.0\n",
      "  training_episode_reward:\n",
      "    max: 149.01065252654544\n",
      "    mean: 36.9312453949086\n",
      "    min: -31.782671215539594\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 70 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 6.418130602836609\n",
      "  evaluate_reward: 23.780028315454082\n",
      "  iter_episodes: 13\n",
      "  iter_time: 23.305140018463135\n",
      "  iter_timesteps: 10162\n",
      "  iteration: 70\n",
      "  mean_advantage: 4.692355393132175e-09\n",
      "  mean_baselines: -4.431874752044678\n",
      "  mean_log_prob: -1.0073750019073486\n",
      "  performance: 11.274865859506786\n",
      "  policy_loss: -961.091796875\n",
      "  total_episodes: 3878\n",
      "  total_time: 851.0454108715057\n",
      "  total_timesteps: 725358\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 781.6923076923077\n",
      "    min: 318.0\n",
      "  training_episode_reward:\n",
      "    max: 90.4256152672328\n",
      "    mean: 11.274865859506786\n",
      "    min: -127.88894742768963\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 71 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 5.9019840049743655\n",
      "  evaluate_reward: 13.83771571246346\n",
      "  iter_episodes: 12\n",
      "  iter_time: 24.24486470222473\n",
      "  iter_timesteps: 10682\n",
      "  iteration: 71\n",
      "  mean_advantage: 3.7050631451762683e-09\n",
      "  mean_baselines: -5.463421821594238\n",
      "  mean_log_prob: -1.0103527307510376\n",
      "  performance: 55.86606004619409\n",
      "  policy_loss: -503.97857666015625\n",
      "  total_episodes: 3890\n",
      "  total_time: 875.2902755737305\n",
      "  total_timesteps: 736040\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 890.1666666666666\n",
      "    min: 219.0\n",
      "  training_episode_reward:\n",
      "    max: 122.53774178268169\n",
      "    mean: 55.86606004619409\n",
      "    min: -43.25573341410871\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 72 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 7.013357515335083\n",
      "  evaluate_reward: -11.752010507756964\n",
      "  iter_episodes: 15\n",
      "  iter_time: 26.39730215072632\n",
      "  iter_timesteps: 10630\n",
      "  iteration: 72\n",
      "  mean_advantage: -3.868974918219692e-09\n",
      "  mean_baselines: -1.3180643320083618\n",
      "  mean_log_prob: -1.0084525346755981\n",
      "  performance: 2.975350834298186\n",
      "  policy_loss: -580.5886840820312\n",
      "  total_episodes: 3905\n",
      "  total_time: 901.6875777244568\n",
      "  total_timesteps: 746670\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 708.6666666666666\n",
      "    min: 211.0\n",
      "  training_episode_reward:\n",
      "    max: 123.22415978178908\n",
      "    mean: 2.975350834298186\n",
      "    min: -153.80592718480352\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 73 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 3.914736487865448\n",
      "  evaluate_reward: 23.661325907983702\n",
      "  iter_episodes: 12\n",
      "  iter_time: 23.775036096572876\n",
      "  iter_timesteps: 10763\n",
      "  iteration: 73\n",
      "  mean_advantage: -3.987303376362661e-09\n",
      "  mean_baselines: -7.276539325714111\n",
      "  mean_log_prob: -1.0134766101837158\n",
      "  performance: 94.29877074804278\n",
      "  policy_loss: -1150.675537109375\n",
      "  total_episodes: 3917\n",
      "  total_time: 925.4626138210297\n",
      "  total_timesteps: 757433\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 896.9166666666666\n",
      "    min: 232.0\n",
      "  training_episode_reward:\n",
      "    max: 164.34667571440738\n",
      "    mean: 94.29877074804278\n",
      "    min: -55.775784014879505\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 74 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999999403953552\n",
      "  critic_loss: 5.5829673719406125\n",
      "  evaluate_reward: 67.93150609493127\n",
      "  iter_episodes: 16\n",
      "  iter_time: 27.047075510025024\n",
      "  iter_timesteps: 10844\n",
      "  iteration: 74\n",
      "  mean_advantage: 3.5177953883192004e-09\n",
      "  mean_baselines: -0.9484718441963196\n",
      "  mean_log_prob: -1.0077540874481201\n",
      "  performance: 33.19882737187706\n",
      "  policy_loss: -852.9467163085938\n",
      "  total_episodes: 3933\n",
      "  total_time: 952.5096893310547\n",
      "  total_timesteps: 768277\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 677.75\n",
      "    min: 176.0\n",
      "  training_episode_reward:\n",
      "    max: 154.75094894483092\n",
      "    mean: 33.19882737187706\n",
      "    min: -54.64044351730819\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 75 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 5.317638716697693\n",
      "  evaluate_reward: 72.52348715728473\n",
      "  iter_episodes: 13\n",
      "  iter_time: 24.83364725112915\n",
      "  iter_timesteps: 10290\n",
      "  iteration: 75\n",
      "  mean_advantage: 1.0009409301403593e-08\n",
      "  mean_baselines: -1.1595441102981567\n",
      "  mean_log_prob: -1.0162019729614258\n",
      "  performance: 46.47014152524737\n",
      "  policy_loss: -1029.3050537109375\n",
      "  total_episodes: 3946\n",
      "  total_time: 977.3433365821838\n",
      "  total_timesteps: 778567\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 791.5384615384615\n",
      "    min: 288.0\n",
      "  training_episode_reward:\n",
      "    max: 159.93262797614307\n",
      "    mean: 46.47014152524737\n",
      "    min: -101.26223917215006\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 76 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 3.0817530512809754\n",
      "  evaluate_reward: 98.10226288763724\n",
      "  iter_episodes: 12\n",
      "  iter_time: 25.52626085281372\n",
      "  iter_timesteps: 10367\n",
      "  iteration: 76\n",
      "  mean_advantage: -7.727273043656169e-09\n",
      "  mean_baselines: -2.2869386672973633\n",
      "  mean_log_prob: -1.0149742364883423\n",
      "  performance: 90.90648002215711\n",
      "  policy_loss: -785.261962890625\n",
      "  total_episodes: 3958\n",
      "  total_time: 1002.8695974349976\n",
      "  total_timesteps: 788934\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 863.9166666666666\n",
      "    min: 157.0\n",
      "  training_episode_reward:\n",
      "    max: 146.35471801967418\n",
      "    mean: 90.90648002215711\n",
      "    min: 9.343538270265498\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 77 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 3.450599548816681\n",
      "  evaluate_reward: 54.62953557732557\n",
      "  iter_episodes: 12\n",
      "  iter_time: 27.76187562942505\n",
      "  iter_timesteps: 10787\n",
      "  iteration: 77\n",
      "  mean_advantage: -1.0609151335216893e-09\n",
      "  mean_baselines: -2.905428647994995\n",
      "  mean_log_prob: -1.0182414054870605\n",
      "  performance: 82.25706285298439\n",
      "  policy_loss: -1152.892822265625\n",
      "  total_episodes: 3970\n",
      "  total_time: 1030.6314730644226\n",
      "  total_timesteps: 799721\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 898.9166666666666\n",
      "    min: 158.0\n",
      "  training_episode_reward:\n",
      "    max: 189.2039033478829\n",
      "    mean: 82.25706285298439\n",
      "    min: -56.17769609107973\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 78 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 3.8852340269088743\n",
      "  evaluate_reward: 26.799753322841934\n",
      "  iter_episodes: 12\n",
      "  iter_time: 27.005286931991577\n",
      "  iter_timesteps: 10045\n",
      "  iteration: 78\n",
      "  mean_advantage: 5.506531675081305e-09\n",
      "  mean_baselines: -2.4780519008636475\n",
      "  mean_log_prob: -1.0063716173171997\n",
      "  performance: 79.3105733268133\n",
      "  policy_loss: -1203.0830078125\n",
      "  total_episodes: 3982\n",
      "  total_time: 1057.6367599964142\n",
      "  total_timesteps: 809766\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 837.0833333333334\n",
      "    min: 207.0\n",
      "  training_episode_reward:\n",
      "    max: 152.40382706903725\n",
      "    mean: 79.3105733268133\n",
      "    min: -85.89926770315117\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 79 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 2.61715580701828\n",
      "  evaluate_reward: 57.6297444456981\n",
      "  iter_episodes: 11\n",
      "  iter_time: 24.90653371810913\n",
      "  iter_timesteps: 10191\n",
      "  iteration: 79\n",
      "  mean_advantage: -6.83134393497653e-09\n",
      "  mean_baselines: -1.7030720710754395\n",
      "  mean_log_prob: -1.0084805488586426\n",
      "  performance: 93.46264198545191\n",
      "  policy_loss: -1236.84716796875\n",
      "  total_episodes: 3993\n",
      "  total_time: 1082.5432937145233\n",
      "  total_timesteps: 819957\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 926.4545454545455\n",
      "    min: 191.0\n",
      "  training_episode_reward:\n",
      "    max: 144.50005127054135\n",
      "    mean: 93.46264198545191\n",
      "    min: 36.67484343753887\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 80 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 5.695575261116028\n",
      "  evaluate_reward: 90.296664870934\n",
      "  iter_episodes: 16\n",
      "  iter_time: 22.60556435585022\n",
      "  iter_timesteps: 10404\n",
      "  iteration: 80\n",
      "  mean_advantage: -1.8332839690771152e-09\n",
      "  mean_baselines: 3.164820909500122\n",
      "  mean_log_prob: -1.0056861639022827\n",
      "  performance: 65.03390177948052\n",
      "  policy_loss: -611.6968383789062\n",
      "  total_episodes: 4009\n",
      "  total_time: 1105.1488580703735\n",
      "  total_timesteps: 830361\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 650.25\n",
      "    min: 178.0\n",
      "  training_episode_reward:\n",
      "    max: 143.0147167573568\n",
      "    mean: 65.03390177948052\n",
      "    min: -74.88655870318148\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 81 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 2.124892780780792\n",
      "  evaluate_reward: 90.72604150107897\n",
      "  iter_episodes: 11\n",
      "  iter_time: 27.169768571853638\n",
      "  iter_timesteps: 10374\n",
      "  iteration: 81\n",
      "  mean_advantage: -7.997846829255195e-09\n",
      "  mean_baselines: -0.826541006565094\n",
      "  mean_log_prob: -1.029364824295044\n",
      "  performance: 113.48216362870367\n",
      "  policy_loss: -993.9818115234375\n",
      "  total_episodes: 4020\n",
      "  total_time: 1132.3186266422272\n",
      "  total_timesteps: 840735\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 943.0909090909091\n",
      "    min: 374.0\n",
      "  training_episode_reward:\n",
      "    max: 179.54345315126858\n",
      "    mean: 113.48216362870367\n",
      "    min: 4.925651407704123\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 82 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0000001192092896\n",
      "  critic_loss: 4.821543135643005\n",
      "  evaluate_reward: 94.01493682180478\n",
      "  iter_episodes: 16\n",
      "  iter_time: 22.405577182769775\n",
      "  iter_timesteps: 10366\n",
      "  iteration: 82\n",
      "  mean_advantage: 1.6171913808094018e-09\n",
      "  mean_baselines: 4.05457067489624\n",
      "  mean_log_prob: -1.007309079170227\n",
      "  performance: 70.46387009609055\n",
      "  policy_loss: -888.8092651367188\n",
      "  total_episodes: 4036\n",
      "  total_time: 1154.724203824997\n",
      "  total_timesteps: 851101\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 647.875\n",
      "    min: 175.0\n",
      "  training_episode_reward:\n",
      "    max: 174.80758418407382\n",
      "    mean: 70.46387009609055\n",
      "    min: -15.694081535980928\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 83 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999998807907104\n",
      "  critic_loss: 4.545981612205505\n",
      "  evaluate_reward: 115.78876263001453\n",
      "  iter_episodes: 16\n",
      "  iter_time: 20.982184886932373\n",
      "  iter_timesteps: 10817\n",
      "  iteration: 83\n",
      "  mean_advantage: 7.053151951375014e-10\n",
      "  mean_baselines: 2.9052765369415283\n",
      "  mean_log_prob: -0.9831393361091614\n",
      "  performance: 83.16624330472584\n",
      "  policy_loss: -1153.5218505859375\n",
      "  total_episodes: 4052\n",
      "  total_time: 1175.7063887119293\n",
      "  total_timesteps: 861918\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 676.0625\n",
      "    min: 179.0\n",
      "  training_episode_reward:\n",
      "    max: 179.34981249570836\n",
      "    mean: 83.16624330472584\n",
      "    min: -13.64474265904893\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 84 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 3.975472795963287\n",
      "  evaluate_reward: 51.152809958926845\n",
      "  iter_episodes: 14\n",
      "  iter_time: 20.67273736000061\n",
      "  iter_timesteps: 10419\n",
      "  iteration: 84\n",
      "  mean_advantage: -7.299695514717541e-09\n",
      "  mean_baselines: 2.4148800373077393\n",
      "  mean_log_prob: -0.9986518025398254\n",
      "  performance: 86.75602549763643\n",
      "  policy_loss: -1399.6141357421875\n",
      "  total_episodes: 4066\n",
      "  total_time: 1196.37912607193\n",
      "  total_timesteps: 872337\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 744.2142857142857\n",
      "    min: 150.0\n",
      "  training_episode_reward:\n",
      "    max: 143.12166802493152\n",
      "    mean: 86.75602549763643\n",
      "    min: -46.092850669519805\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 85 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999999403953552\n",
      "  critic_loss: 2.564641485214233\n",
      "  evaluate_reward: 94.74942292126723\n",
      "  iter_episodes: 11\n",
      "  iter_time: 20.305402278900146\n",
      "  iter_timesteps: 10227\n",
      "  iteration: 85\n",
      "  mean_advantage: -3.6600877884040983e-09\n",
      "  mean_baselines: 0.6436996459960938\n",
      "  mean_log_prob: -0.9950073957443237\n",
      "  performance: 126.5533840692051\n",
      "  policy_loss: -1378.7850341796875\n",
      "  total_episodes: 4077\n",
      "  total_time: 1216.68452835083\n",
      "  total_timesteps: 882564\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 929.7272727272727\n",
      "    min: 227.0\n",
      "  training_episode_reward:\n",
      "    max: 175.12670439626808\n",
      "    mean: 126.5533840692051\n",
      "    min: 40.94743691973255\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 86 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 2.665258185863495\n",
      "  evaluate_reward: 77.40599500612731\n",
      "  iter_episodes: 11\n",
      "  iter_time: 24.15840435028076\n",
      "  iter_timesteps: 10286\n",
      "  iteration: 86\n",
      "  mean_advantage: -1.5761679739156875e-09\n",
      "  mean_baselines: 2.0263993740081787\n",
      "  mean_log_prob: -0.966830849647522\n",
      "  performance: 123.0158239528378\n",
      "  policy_loss: -1146.89404296875\n",
      "  total_episodes: 4088\n",
      "  total_time: 1240.8429327011108\n",
      "  total_timesteps: 892850\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 935.0909090909091\n",
      "    min: 286.0\n",
      "  training_episode_reward:\n",
      "    max: 175.01879272470973\n",
      "    mean: 123.0158239528378\n",
      "    min: 33.38363788236785\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 87 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0000001192092896\n",
      "  critic_loss: 2.055184403657913\n",
      "  evaluate_reward: 180.56145021438425\n",
      "  iter_episodes: 11\n",
      "  iter_time: 23.72810649871826\n",
      "  iter_timesteps: 10207\n",
      "  iteration: 87\n",
      "  mean_advantage: -2.2424007095622756e-09\n",
      "  mean_baselines: 2.538262367248535\n",
      "  mean_log_prob: -0.9183765053749084\n",
      "  performance: 118.6680636701729\n",
      "  policy_loss: -1445.4569091796875\n",
      "  total_episodes: 4099\n",
      "  total_time: 1264.571039199829\n",
      "  total_timesteps: 903057\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 927.9090909090909\n",
      "    min: 207.0\n",
      "  training_episode_reward:\n",
      "    max: 184.25684270841268\n",
      "    mean: 118.6680636701729\n",
      "    min: 61.404253008283376\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 88 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 1.8851450049877168\n",
      "  evaluate_reward: 123.39414451039322\n",
      "  iter_episodes: 12\n",
      "  iter_time: 23.104177236557007\n",
      "  iter_timesteps: 10734\n",
      "  iteration: 88\n",
      "  mean_advantage: -2.4432684764263968e-09\n",
      "  mean_baselines: 3.0890934467315674\n",
      "  mean_log_prob: -0.891691267490387\n",
      "  performance: 106.07879365018742\n",
      "  policy_loss: -1331.2391357421875\n",
      "  total_episodes: 4111\n",
      "  total_time: 1287.675216436386\n",
      "  total_timesteps: 913791\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 894.5\n",
      "    min: 324.0\n",
      "  training_episode_reward:\n",
      "    max: 165.04977550383526\n",
      "    mean: 106.07879365018742\n",
      "    min: -23.49998531437754\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 89 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999998807907104\n",
      "  critic_loss: 3.4610009813308715\n",
      "  evaluate_reward: 43.46396835565531\n",
      "  iter_episodes: 13\n",
      "  iter_time: 23.698725700378418\n",
      "  iter_timesteps: 10047\n",
      "  iteration: 89\n",
      "  mean_advantage: 2.2781112551939486e-09\n",
      "  mean_baselines: 5.724285125732422\n",
      "  mean_log_prob: -0.8680091500282288\n",
      "  performance: 87.34239057072976\n",
      "  policy_loss: -835.0567626953125\n",
      "  total_episodes: 4124\n",
      "  total_time: 1311.3739421367645\n",
      "  total_timesteps: 923838\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 772.8461538461538\n",
      "    min: 198.0\n",
      "  training_episode_reward:\n",
      "    max: 158.77070704513827\n",
      "    mean: 87.34239057072976\n",
      "    min: -12.632394753659014\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 90 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0000001192092896\n",
      "  critic_loss: 6.2660509300231935\n",
      "  evaluate_reward: 125.61573363792222\n",
      "  iter_episodes: 12\n",
      "  iter_time: 22.275092363357544\n",
      "  iter_timesteps: 10271\n",
      "  iteration: 90\n",
      "  mean_advantage: -4.456855773327106e-09\n",
      "  mean_baselines: 4.749544143676758\n",
      "  mean_log_prob: -0.832811176776886\n",
      "  performance: 146.35690594756076\n",
      "  policy_loss: -684.5650024414062\n",
      "  total_episodes: 4136\n",
      "  total_time: 1333.649034500122\n",
      "  total_timesteps: 934109\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 855.9166666666666\n",
      "    min: 376.0\n",
      "  training_episode_reward:\n",
      "    max: 228.78662229527464\n",
      "    mean: 146.35690594756076\n",
      "    min: -37.97803774018575\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 91 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 8.800204496383667\n",
      "  evaluate_reward: 136.340394419572\n",
      "  iter_episodes: 12\n",
      "  iter_time: 22.623932600021362\n",
      "  iter_timesteps: 10352\n",
      "  iteration: 91\n",
      "  mean_advantage: -2.2109913899726052e-09\n",
      "  mean_baselines: 5.895811080932617\n",
      "  mean_log_prob: -0.8405471444129944\n",
      "  performance: 177.32090304976055\n",
      "  policy_loss: -721.0737915039062\n",
      "  total_episodes: 4148\n",
      "  total_time: 1356.2729671001434\n",
      "  total_timesteps: 944461\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 862.6666666666666\n",
      "    min: 520.0\n",
      "  training_episode_reward:\n",
      "    max: 256.3424867415439\n",
      "    mean: 177.32090304976055\n",
      "    min: 82.16972290827016\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 92 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 12.218052225112915\n",
      "  evaluate_reward: 132.78157093395845\n",
      "  iter_episodes: 14\n",
      "  iter_time: 19.967649221420288\n",
      "  iter_timesteps: 10136\n",
      "  iteration: 92\n",
      "  mean_advantage: -4.139864895336132e-09\n",
      "  mean_baselines: 5.168684005737305\n",
      "  mean_log_prob: -0.8124862909317017\n",
      "  performance: 135.96078131409544\n",
      "  policy_loss: -665.8226928710938\n",
      "  total_episodes: 4162\n",
      "  total_time: 1376.2406163215637\n",
      "  total_timesteps: 954597\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 724.0\n",
      "    min: 276.0\n",
      "  training_episode_reward:\n",
      "    max: 239.9229766023635\n",
      "    mean: 135.96078131409544\n",
      "    min: -20.09931402437263\n",
      "\n",
      "=== ActorCriticTrainer LunarLander-v2 Iteration 93 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 10.817260971069336\n",
      "  evaluate_reward: 232.6513375313826\n",
      "  iter_episodes: 13\n",
      "  iter_time: 19.140456438064575\n",
      "  iter_timesteps: 10170\n",
      "  iteration: 93\n",
      "  mean_advantage: 3.750931565349447e-09\n",
      "  mean_baselines: 8.012215614318848\n",
      "  mean_log_prob: -0.7982161045074463\n",
      "  performance: 194.64529135709785\n",
      "  policy_loss: -618.5711669921875\n",
      "  total_episodes: 4175\n",
      "  total_time: 1395.3810727596283\n",
      "  total_timesteps: 964767\n",
      "  training_episode_length:\n",
      "    max: 1000.0\n",
      "    mean: 782.3076923076923\n",
      "    min: 581.0\n",
      "  training_episode_reward:\n",
      "    max: 261.98703542584923\n",
      "    mean: 194.64529135709785\n",
      "    min: 54.07925751061357\n",
      "\n",
      "In 93 iteration, current mean episode reward 232.651 is greater than reward threshold 200. Congratulation! Now we exit the training process.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "ac_trainer_lander, ac_result_lander = run(ActorCriticTrainer, dict(\n",
    "    env_name=\"LunarLander-v2\",\n",
    "    max_iteration=500,\n",
    "    learning_rate=0.005,\n",
    "    max_episode_length=1000,\n",
    "    train_batch_size=10000,\n",
    "    normalize_advantage=True,\n",
    "    evaluate_interval=1,\n",
    "    evaluate_num_episodes=5,\n",
    "), 200)\n",
    "\n",
    "# Hint: 1. This would take hours to train on personal laptop.\n",
    "#       2. Episode reward should greater than 0 after 80 iterations.\n",
    "#       3. We are using purely on-policy algorithm, so we expect the performance \n",
    "#          to be highly unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Episode Reward in LunarLander-v2')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Plot the result to diagnose potential problem.\n",
    "sns.lineplot(\n",
    "    x=\"total_timesteps\", y=\"performance\", data=ac_result_lander\n",
    ").set_title(\"Episode Reward in LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward for your Actor-critic agent in LunarLander-v2:  218.61478219695402\n",
      "One small step for agent, one giant leap for mankind.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# You should see a pop up window which display the movement of the cart and pole.\n",
    "print(\"Average episode reward for your Actor-critic agent in LunarLander-v2: \",\n",
    "      ac_trainer_lander.evaluate(1, render=False))\n",
    "print(\"One small step for agent, one giant leap for mankind.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.3: Even harder -- train Pong agents\n",
    "\n",
    "(5 / 100 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "# e = gym.make(\"PongDeterministic-v4\"); e.reset()\n",
    "# frames = []\n",
    "# for _ in range(1000):\n",
    "#     frames.append(e.render(\"rgb_array\"))\n",
    "#     if e.step(e.action_space.sample())[2]: break\n",
    "# e.close();animate(frames);del e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this section, we will train agents to play Pong game in Atari environment. The agent control the bat on the right with up or down action to make it hit the table tennis ball to left side. The agent need to beat a rule-based AI.\n",
    "\n",
    "The observation of the environment `PongDeterministic-v4` is an image of 210 X 160 X 3 pixels. We compress the image and flatten it to be a vector with 6400 elements. To do this, we provide you a wrapper class which wrap the actor-critic trainer to fit this environment. You need to implement the save and restore functions.\n",
    "\n",
    "In the assignment 4, we will trained state of the art Pong agents with various algorithms. Your agents will battle with other AI. Let's get familiar with Pong by finishing this section!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def process_img(img):\n",
    "    \"\"\"In this function, we process the raw image array into a flat vector.\n",
    "    We assume the img has four dimension, with shape in [Batch size, Width, Height, Channel].\n",
    "    Therefore the raw image is a 4D matrix containing the intensity of colors.\n",
    "    \"\"\"\n",
    "    assert img.shape[-3:] == (210, 160, 3), img.shape\n",
    "    \n",
    "    # Add extra axis if input is a single image and make it looks like a batch.\n",
    "    single_img = False\n",
    "    if img.ndim == 3:\n",
    "        img = img[None, ...]\n",
    "        single_img = True\n",
    "        \n",
    "    # Crop the image\n",
    "    img = img[:, 35:195]\n",
    "    \n",
    "    # Shrink the image size by taking only 1/4 of the pixel\n",
    "    img = img[:, ::2, ::2, 0]\n",
    "    \n",
    "    # Erase background\n",
    "    img[img == 144] = 0\n",
    "    img[img == 109] = 0\n",
    "    \n",
    "    # Paint everything (paddles, ball) to 1\n",
    "    img[img != 0 ] = 1\n",
    "    \n",
    "    # Flatten image to vector\n",
    "    img = img.astype(np.float).reshape(-1, 6400)\n",
    "    \n",
    "    # Reverse single image\n",
    "    if single_img:\n",
    "        img = img[0, ...]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "def pong_wrapper(trainer):\n",
    "    assert inspect.isclass(trainer)\n",
    "    \n",
    "    class WrappedTrainer(trainer):\n",
    "        def __init__(self, *a, **k):\n",
    "            super().__init__(*a, **k)\n",
    "            self.obs_dim = 6400\n",
    "            self.build_model()\n",
    "            assert \"Pong\" in self.config[\"env_name\"]\n",
    "            \n",
    "        def compute_action(self, obs):\n",
    "            obs = process_img(obs)\n",
    "            return super().compute_action(obs)\n",
    "            \n",
    "        def process_samples(self, samples):\n",
    "            samples[\"obs\"] = [process_img(traj_obs) for traj_obs in samples[\"obs\"]]\n",
    "            return super().process_samples(samples)\n",
    "        \n",
    "        def train(self):\n",
    "            ret = super().train()\n",
    "            if self.iteration % self.config[\"evaluate_interval\"] == 0:\n",
    "                self.save(\"iter{}\".format(self.iteration))\n",
    "                print(\"Finished {} iterations training. \"\n",
    "                      \"Checkpoint is saved in checkpoints directory.\".format(self.iteration))\n",
    "            return ret\n",
    "        \n",
    "        def save(self, surfix=\"checkpoint\"):\n",
    "            import pickle\n",
    "            import os\n",
    "            os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "            policy_file_name = \"checkpoints/pong-agent-{}-policy.pkl\".format(surfix)\n",
    "            with open(policy_file_name, \"wb\") as f:\n",
    "                # [TODO] Use function of pickle package and torch model to save \n",
    "                # the parameters of self.policy to file \"f\".\n",
    "                # Hint: a function called state_dict of torch model may help.\n",
    "                pickle.dump(self.policy.state_dict(), f)\n",
    "            \n",
    "            baseline_file_name = \"checkpoints/pong-agent-{}-baseline.pkl\".format(surfix)\n",
    "            with open(baseline_file_name, \"wb\") as f:\n",
    "                # [TODO] Use function of pickle package and torch model to save \n",
    "                # the parameters of self.baseline to file \"f\".\n",
    "                pickle.dump(self.baseline.state_dict(), f)\n",
    "                    \n",
    "        def restore(self, surfix=\"checkpoint\"):\n",
    "            import pickle\n",
    "            policy_file_name = \"checkpoints/pong-agent-{}-policy.pkl\".format(surfix)\n",
    "            with open(policy_file_name, \"rb\") as f:\n",
    "                # [TODO] Use function of pickle package and torch model to restore \n",
    "                # the parameters of self.policy from file \"f\".\n",
    "                # Hint: a function called load_state_dict of torch model may help.\n",
    "                self.policy.load_state_dict(pickle.load(f))\n",
    "                \n",
    "            baseline_file_name = \"checkpoints/pong-agent-{}-baseline.pkl\".format(surfix)\n",
    "            with open(baseline_file_name, \"rb\") as f:\n",
    "                # [TODO] Use function of pickle package and torch model to restore \n",
    "                # the parameters of self.baseline from file \"f\".\n",
    "                self.baseline.load_state_dict(pickle.load(f))\n",
    "        \n",
    "    return WrappedTrainer\n",
    "\n",
    "\n",
    "# Test save and restore\n",
    "pong_wrapper(ActorCriticTrainer)({\"env_name\": \"Pong-v0\"}).save(\"test\")\n",
    "pong_wrapper(ActorCriticTrainer)({\"env_name\": \"Pong-v0\"}).restore(\"test\")\n",
    "import os\n",
    "os.remove(\"checkpoints/pong-agent-test-baseline.pkl\")\n",
    "os.remove(\"checkpoints/pong-agent-test-policy.pkl\")\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WrappedTrainer PongDeterministic-v4 Training Start ===\n",
      "Config:\n",
      "  checked: true\n",
      "  clip_gradient: 10.0\n",
      "  env_name: PongDeterministic-v4\n",
      "  evaluate_interval: 10\n",
      "  evaluate_num_episodes: 10\n",
      "  gamma: 0.99\n",
      "  hidden_units: 100\n",
      "  learning_rate: 0.0005\n",
      "  max_episode_length: 5000\n",
      "  max_iteration: 200\n",
      "  normalize_advantage: true\n",
      "  num_critic_update_steps: 10\n",
      "  num_critic_updates: 10\n",
      "  seed: 0\n",
      "  train_batch_size: 5000\n",
      "\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 0 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999998807907104\n",
      "  critic_loss: 0.014600211950019003\n",
      "  evaluate_reward: -20.5\n",
      "  iter_episodes: 6\n",
      "  iter_time: 16.82163143157959\n",
      "  iter_timesteps: 5461\n",
      "  iteration: 0\n",
      "  mean_advantage: -3.929256919832369e-09\n",
      "  mean_baselines: -0.05588538572192192\n",
      "  mean_log_prob: -1.7910598516464233\n",
      "  performance: -20.166666666666668\n",
      "  policy_loss: 2.2734909057617188\n",
      "  total_episodes: 6\n",
      "  total_time: 16.82163143157959\n",
      "  total_timesteps: 5461\n",
      "  training_episode_length:\n",
      "    max: 1027.0\n",
      "    mean: 910.1666666666666\n",
      "    min: 764.0\n",
      "  training_episode_reward:\n",
      "    max: -19.0\n",
      "    mean: -20.166666666666668\n",
      "    min: -21.0\n",
      "\n",
      "Finished 10 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 10 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.015210393192246556\n",
      "  evaluate_reward: -20.7\n",
      "  iter_episodes: 6\n",
      "  iter_time: 18.51293420791626\n",
      "  iter_timesteps: 5781\n",
      "  iteration: 10\n",
      "  mean_advantage: -8.330834688763389e-09\n",
      "  mean_baselines: -1.3701727390289307\n",
      "  mean_log_prob: -1.7888479232788086\n",
      "  performance: -20.166666666666668\n",
      "  policy_loss: -5.578512191772461\n",
      "  total_episodes: 65\n",
      "  total_time: 203.2267804145813\n",
      "  total_timesteps: 61335\n",
      "  training_episode_length:\n",
      "    max: 1060.0\n",
      "    mean: 963.5\n",
      "    min: 886.0\n",
      "  training_episode_reward:\n",
      "    max: -19.0\n",
      "    mean: -20.166666666666668\n",
      "    min: -21.0\n",
      "\n",
      "Finished 20 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 20 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.010107235354371368\n",
      "  evaluate_reward: -20.2\n",
      "  iter_episodes: 6\n",
      "  iter_time: 17.459872722625732\n",
      "  iter_timesteps: 5602\n",
      "  iteration: 20\n",
      "  mean_advantage: -4.766669281508484e-09\n",
      "  mean_baselines: -1.6694886684417725\n",
      "  mean_log_prob: -1.787462592124939\n",
      "  performance: -20.0\n",
      "  policy_loss: -18.864479064941406\n",
      "  total_episodes: 123\n",
      "  total_time: 387.57252764701843\n",
      "  total_timesteps: 116676\n",
      "  training_episode_length:\n",
      "    max: 992.0\n",
      "    mean: 933.6666666666666\n",
      "    min: 792.0\n",
      "  training_episode_reward:\n",
      "    max: -19.0\n",
      "    mean: -20.0\n",
      "    min: -21.0\n",
      "\n",
      "Finished 30 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 30 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.010233142827637494\n",
      "  evaluate_reward: -20.9\n",
      "  iter_episodes: 6\n",
      "  iter_time: 16.685683250427246\n",
      "  iter_timesteps: 5351\n",
      "  iteration: 30\n",
      "  mean_advantage: -5.8813776071531265e-09\n",
      "  mean_baselines: -1.850590705871582\n",
      "  mean_log_prob: -1.7857462167739868\n",
      "  performance: -20.333333333333332\n",
      "  policy_loss: 4.374503135681152\n",
      "  total_episodes: 179\n",
      "  total_time: 573.5128221511841\n",
      "  total_timesteps: 171346\n",
      "  training_episode_length:\n",
      "    max: 937.0\n",
      "    mean: 891.8333333333334\n",
      "    min: 811.0\n",
      "  training_episode_reward:\n",
      "    max: -19.0\n",
      "    mean: -20.333333333333332\n",
      "    min: -21.0\n",
      "\n",
      "Finished 40 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 40 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.010275007095187902\n",
      "  evaluate_reward: -19.7\n",
      "  iter_episodes: 6\n",
      "  iter_time: 17.662696838378906\n",
      "  iter_timesteps: 5666\n",
      "  iteration: 40\n",
      "  mean_advantage: -3.3663054566090977e-09\n",
      "  mean_baselines: -1.8399406671524048\n",
      "  mean_log_prob: -1.7845230102539062\n",
      "  performance: -20.166666666666668\n",
      "  policy_loss: -52.027008056640625\n",
      "  total_episodes: 236\n",
      "  total_time: 760.1868674755096\n",
      "  total_timesteps: 226613\n",
      "  training_episode_length:\n",
      "    max: 1172.0\n",
      "    mean: 944.3333333333334\n",
      "    min: 792.0\n",
      "  training_episode_reward:\n",
      "    max: -19.0\n",
      "    mean: -20.166666666666668\n",
      "    min: -21.0\n",
      "\n",
      "Finished 50 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 50 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.009785346472635865\n",
      "  evaluate_reward: -20.1\n",
      "  iter_episodes: 6\n",
      "  iter_time: 17.854791164398193\n",
      "  iter_timesteps: 5908\n",
      "  iteration: 50\n",
      "  mean_advantage: 2.9055751138429287e-09\n",
      "  mean_baselines: -1.803771734237671\n",
      "  mean_log_prob: -1.780861258506775\n",
      "  performance: -20.333333333333332\n",
      "  policy_loss: -45.84129333496094\n",
      "  total_episodes: 293\n",
      "  total_time: 949.8735997676849\n",
      "  total_timesteps: 282957\n",
      "  training_episode_length:\n",
      "    max: 1167.0\n",
      "    mean: 984.6666666666666\n",
      "    min: 811.0\n",
      "  training_episode_reward:\n",
      "    max: -18.0\n",
      "    mean: -20.333333333333332\n",
      "    min: -21.0\n",
      "\n",
      "Finished 60 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 60 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.009481178903952241\n",
      "  evaluate_reward: -20.0\n",
      "  iter_episodes: 5\n",
      "  iter_time: 18.129400730133057\n",
      "  iter_timesteps: 5578\n",
      "  iteration: 60\n",
      "  mean_advantage: -8.548532548502408e-09\n",
      "  mean_baselines: -1.6811628341674805\n",
      "  mean_log_prob: -1.7682551145553589\n",
      "  performance: -19.0\n",
      "  policy_loss: -46.37432098388672\n",
      "  total_episodes: 345\n",
      "  total_time: 1139.570198059082\n",
      "  total_timesteps: 338938\n",
      "  training_episode_length:\n",
      "    max: 1202.0\n",
      "    mean: 1115.6\n",
      "    min: 968.0\n",
      "  training_episode_reward:\n",
      "    max: -18.0\n",
      "    mean: -19.0\n",
      "    min: -20.0\n",
      "\n",
      "Finished 70 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 70 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.010385256600566209\n",
      "  evaluate_reward: -19.2\n",
      "  iter_episodes: 5\n",
      "  iter_time: 17.553484201431274\n",
      "  iter_timesteps: 5459\n",
      "  iteration: 70\n",
      "  mean_advantage: 5.590323759463445e-09\n",
      "  mean_baselines: -1.6076751947402954\n",
      "  mean_log_prob: -1.7540180683135986\n",
      "  performance: -19.6\n",
      "  policy_loss: -78.81803131103516\n",
      "  total_episodes: 393\n",
      "  total_time: 1331.5722072124481\n",
      "  total_timesteps: 394020\n",
      "  training_episode_length:\n",
      "    max: 1171.0\n",
      "    mean: 1091.8\n",
      "    min: 896.0\n",
      "  training_episode_reward:\n",
      "    max: -19.0\n",
      "    mean: -19.6\n",
      "    min: -21.0\n",
      "\n",
      "Finished 80 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 80 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.010347010144032538\n",
      "  evaluate_reward: -19.5\n",
      "  iter_episodes: 4\n",
      "  iter_time: 16.211855173110962\n",
      "  iter_timesteps: 5224\n",
      "  iteration: 80\n",
      "  mean_advantage: -4.746464998817146e-09\n",
      "  mean_baselines: -1.4967728853225708\n",
      "  mean_log_prob: -1.71870756149292\n",
      "  performance: -18.75\n",
      "  policy_loss: -101.23876190185547\n",
      "  total_episodes: 438\n",
      "  total_time: 1525.7432980537415\n",
      "  total_timesteps: 449996\n",
      "  training_episode_length:\n",
      "    max: 1419.0\n",
      "    mean: 1306.0\n",
      "    min: 1169.0\n",
      "  training_episode_reward:\n",
      "    max: -17.0\n",
      "    mean: -18.75\n",
      "    min: -21.0\n",
      "\n",
      "Finished 90 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 90 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.011035853447392583\n",
      "  evaluate_reward: -18.0\n",
      "  iter_episodes: 4\n",
      "  iter_time: 16.85386061668396\n",
      "  iter_timesteps: 5513\n",
      "  iteration: 90\n",
      "  mean_advantage: -2.075837501891442e-09\n",
      "  mean_baselines: -1.4198142290115356\n",
      "  mean_log_prob: -1.6773031949996948\n",
      "  performance: -19.5\n",
      "  policy_loss: -83.07711791992188\n",
      "  total_episodes: 480\n",
      "  total_time: 1714.844978094101\n",
      "  total_timesteps: 505296\n",
      "  training_episode_length:\n",
      "    max: 1591.0\n",
      "    mean: 1378.25\n",
      "    min: 1245.0\n",
      "  training_episode_reward:\n",
      "    max: -17.0\n",
      "    mean: -19.5\n",
      "    min: -21.0\n",
      "\n",
      "Finished 100 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 100 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.009630741914734245\n",
      "  evaluate_reward: -18.6\n",
      "  iter_episodes: 4\n",
      "  iter_time: 18.452965021133423\n",
      "  iter_timesteps: 5833\n",
      "  iteration: 100\n",
      "  mean_advantage: -2.942934562710775e-09\n",
      "  mean_baselines: -1.350427269935608\n",
      "  mean_log_prob: -1.6448627710342407\n",
      "  performance: -19.0\n",
      "  policy_loss: -237.38970947265625\n",
      "  total_episodes: 520\n",
      "  total_time: 1911.2431373596191\n",
      "  total_timesteps: 560949\n",
      "  training_episode_length:\n",
      "    max: 1680.0\n",
      "    mean: 1458.25\n",
      "    min: 1265.0\n",
      "  training_episode_reward:\n",
      "    max: -16.0\n",
      "    mean: -19.0\n",
      "    min: -20.0\n",
      "\n",
      "Finished 110 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 110 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.00954799638595432\n",
      "  evaluate_reward: -17.9\n",
      "  iter_episodes: 4\n",
      "  iter_time: 18.7047278881073\n",
      "  iter_timesteps: 6488\n",
      "  iteration: 110\n",
      "  mean_advantage: -5.953114889933886e-09\n",
      "  mean_baselines: -1.259456753730774\n",
      "  mean_log_prob: -1.6136223077774048\n",
      "  performance: -18.0\n",
      "  policy_loss: -29.275951385498047\n",
      "  total_episodes: 560\n",
      "  total_time: 2119.5210444927216\n",
      "  total_timesteps: 620838\n",
      "  training_episode_length:\n",
      "    max: 1727.0\n",
      "    mean: 1622.0\n",
      "    min: 1502.0\n",
      "  training_episode_reward:\n",
      "    max: -18.0\n",
      "    mean: -18.0\n",
      "    min: -18.0\n",
      "\n",
      "Finished 120 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 120 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999999403953552\n",
      "  critic_loss: 0.00864957177080214\n",
      "  evaluate_reward: -18.2\n",
      "  iter_episodes: 3\n",
      "  iter_time: 15.638128519058228\n",
      "  iter_timesteps: 5010\n",
      "  iteration: 120\n",
      "  mean_advantage: -1.0374301417925835e-08\n",
      "  mean_baselines: -1.1864298582077026\n",
      "  mean_log_prob: -1.5222262144088745\n",
      "  performance: -16.333333333333332\n",
      "  policy_loss: -148.4439239501953\n",
      "  total_episodes: 599\n",
      "  total_time: 2325.261415243149\n",
      "  total_timesteps: 679938\n",
      "  training_episode_length:\n",
      "    max: 1677.0\n",
      "    mean: 1670.0\n",
      "    min: 1665.0\n",
      "  training_episode_reward:\n",
      "    max: -15.0\n",
      "    mean: -16.333333333333332\n",
      "    min: -17.0\n",
      "\n",
      "Finished 130 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 130 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.010633725966326892\n",
      "  evaluate_reward: -16.7\n",
      "  iter_episodes: 4\n",
      "  iter_time: 19.00595712661743\n",
      "  iter_timesteps: 6420\n",
      "  iteration: 130\n",
      "  mean_advantage: 2.3767583456901775e-09\n",
      "  mean_baselines: -1.0899944305419922\n",
      "  mean_log_prob: -1.477038860321045\n",
      "  performance: -15.25\n",
      "  policy_loss: -155.27682495117188\n",
      "  total_episodes: 636\n",
      "  total_time: 2530.557408809662\n",
      "  total_timesteps: 740253\n",
      "  training_episode_length:\n",
      "    max: 1763.0\n",
      "    mean: 1605.0\n",
      "    min: 1433.0\n",
      "  training_episode_reward:\n",
      "    max: -13.0\n",
      "    mean: -15.25\n",
      "    min: -17.0\n",
      "\n",
      "Finished 140 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 140 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.009658127618022263\n",
      "  evaluate_reward: -16.9\n",
      "  iter_episodes: 3\n",
      "  iter_time: 16.621140480041504\n",
      "  iter_timesteps: 5365\n",
      "  iteration: 140\n",
      "  mean_advantage: 6.5770642265761126e-09\n",
      "  mean_baselines: -1.0278671979904175\n",
      "  mean_log_prob: -1.4438056945800781\n",
      "  performance: -16.333333333333332\n",
      "  policy_loss: -162.8350830078125\n",
      "  total_episodes: 672\n",
      "  total_time: 2735.118374109268\n",
      "  total_timesteps: 799864\n",
      "  training_episode_length:\n",
      "    max: 2123.0\n",
      "    mean: 1788.3333333333333\n",
      "    min: 1546.0\n",
      "  training_episode_reward:\n",
      "    max: -14.0\n",
      "    mean: -16.333333333333332\n",
      "    min: -18.0\n",
      "\n",
      "Finished 150 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 150 ===\n",
      "Training Progress:\n",
      "  advantages_std: 0.9999998807907104\n",
      "  critic_loss: 0.0099403114663437\n",
      "  evaluate_reward: -16.9\n",
      "  iter_episodes: 4\n",
      "  iter_time: 18.900259971618652\n",
      "  iter_timesteps: 6484\n",
      "  iteration: 150\n",
      "  mean_advantage: -4.191813562925972e-09\n",
      "  mean_baselines: -1.0362547636032104\n",
      "  mean_log_prob: -1.3646883964538574\n",
      "  performance: -18.0\n",
      "  policy_loss: -122.67623901367188\n",
      "  total_episodes: 711\n",
      "  total_time: 2951.178488969803\n",
      "  total_timesteps: 863303\n",
      "  training_episode_length:\n",
      "    max: 1657.0\n",
      "    mean: 1621.0\n",
      "    min: 1583.0\n",
      "  training_episode_reward:\n",
      "    max: -18.0\n",
      "    mean: -18.0\n",
      "    min: -18.0\n",
      "\n",
      "Finished 160 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 160 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0000001192092896\n",
      "  critic_loss: 0.009479870102368296\n",
      "  evaluate_reward: -16.4\n",
      "  iter_episodes: 3\n",
      "  iter_time: 16.08572244644165\n",
      "  iter_timesteps: 5057\n",
      "  iteration: 160\n",
      "  mean_advantage: -5.280379689054371e-09\n",
      "  mean_baselines: -0.9645988941192627\n",
      "  mean_log_prob: -1.3169140815734863\n",
      "  performance: -16.666666666666668\n",
      "  policy_loss: -24.58266830444336\n",
      "  total_episodes: 745\n",
      "  total_time: 3149.9519159793854\n",
      "  total_timesteps: 920535\n",
      "  training_episode_length:\n",
      "    max: 1830.0\n",
      "    mean: 1685.6666666666667\n",
      "    min: 1598.0\n",
      "  training_episode_reward:\n",
      "    max: -15.0\n",
      "    mean: -16.666666666666668\n",
      "    min: -18.0\n",
      "\n",
      "Finished 170 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 170 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.00875202271156013\n",
      "  evaluate_reward: -17.2\n",
      "  iter_episodes: 3\n",
      "  iter_time: 16.97075057029724\n",
      "  iter_timesteps: 5390\n",
      "  iteration: 170\n",
      "  mean_advantage: -7.873563134808137e-09\n",
      "  mean_baselines: -0.9231858253479004\n",
      "  mean_log_prob: -1.2865941524505615\n",
      "  performance: -16.666666666666668\n",
      "  policy_loss: -172.06845092773438\n",
      "  total_episodes: 779\n",
      "  total_time: 3351.5101454257965\n",
      "  total_timesteps: 978879\n",
      "  training_episode_length:\n",
      "    max: 1916.0\n",
      "    mean: 1796.6666666666667\n",
      "    min: 1733.0\n",
      "  training_episode_reward:\n",
      "    max: -16.0\n",
      "    mean: -16.666666666666668\n",
      "    min: -17.0\n",
      "\n",
      "Finished 180 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 180 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.008329043993726373\n",
      "  evaluate_reward: -17.0\n",
      "  iter_episodes: 3\n",
      "  iter_time: 18.05071187019348\n",
      "  iter_timesteps: 5346\n",
      "  iteration: 180\n",
      "  mean_advantage: 8.919512906047089e-10\n",
      "  mean_baselines: -0.9195269346237183\n",
      "  mean_log_prob: -1.2483084201812744\n",
      "  performance: -16.666666666666668\n",
      "  policy_loss: -99.79637908935547\n",
      "  total_episodes: 812\n",
      "  total_time: 3552.2918877601624\n",
      "  total_timesteps: 1035266\n",
      "  training_episode_length:\n",
      "    max: 1809.0\n",
      "    mean: 1782.0\n",
      "    min: 1753.0\n",
      "  training_episode_reward:\n",
      "    max: -15.0\n",
      "    mean: -16.666666666666668\n",
      "    min: -19.0\n",
      "\n",
      "Finished 190 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 190 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.008675205665640532\n",
      "  evaluate_reward: -17.1\n",
      "  iter_episodes: 3\n",
      "  iter_time: 17.36888885498047\n",
      "  iter_timesteps: 5507\n",
      "  iteration: 190\n",
      "  mean_advantage: -5.628185029138422e-09\n",
      "  mean_baselines: -0.8993011116981506\n",
      "  mean_log_prob: -1.1879382133483887\n",
      "  performance: -15.333333333333334\n",
      "  policy_loss: -37.25742721557617\n",
      "  total_episodes: 844\n",
      "  total_time: 3749.893077135086\n",
      "  total_timesteps: 1091191\n",
      "  training_episode_length:\n",
      "    max: 2040.0\n",
      "    mean: 1835.6666666666667\n",
      "    min: 1580.0\n",
      "  training_episode_reward:\n",
      "    max: -13.0\n",
      "    mean: -15.333333333333334\n",
      "    min: -18.0\n",
      "\n",
      "Finished 200 iterations training. Checkpoint is saved in checkpoints directory.\n",
      "=== WrappedTrainer PongDeterministic-v4 Iteration 200 ===\n",
      "Training Progress:\n",
      "  advantages_std: 1.0\n",
      "  critic_loss: 0.008404341558925808\n",
      "  evaluate_reward: -17.4\n",
      "  iter_episodes: 3\n",
      "  iter_time: 17.75201940536499\n",
      "  iter_timesteps: 5409\n",
      "  iteration: 200\n",
      "  mean_advantage: -6.567640653543094e-09\n",
      "  mean_baselines: -0.9262654781341553\n",
      "  mean_log_prob: -1.1330912113189697\n",
      "  performance: -16.0\n",
      "  policy_loss: -235.92166137695312\n",
      "  total_episodes: 877\n",
      "  total_time: 3953.296173095703\n",
      "  total_timesteps: 1147047\n",
      "  training_episode_length:\n",
      "    max: 1963.0\n",
      "    mean: 1803.0\n",
      "    min: 1578.0\n",
      "  training_episode_reward:\n",
      "    max: -14.0\n",
      "    mean: -16.0\n",
      "    min: -18.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pong_ac_config = dict(\n",
    "    # environment\n",
    "    env_name=\"PongDeterministic-v4\",\n",
    "\n",
    "    # model\n",
    "    hidden_units=100,\n",
    "\n",
    "    # training inner loop\n",
    "    learning_rate=0.0005,\n",
    "\n",
    "    # training outer loop\n",
    "    max_iteration=200,\n",
    "    max_episode_length=5000,  # In fact the game end at 10000 steps\n",
    "    train_batch_size=5000,\n",
    "\n",
    "    # evaluation\n",
    "    evaluate_interval=10,\n",
    "    evaluate_num_episodes=10\n",
    ")\n",
    "\n",
    "ac_trainer_pong, ac_result_pong = run(pong_wrapper(ActorCriticTrainer), pong_ac_config, 18.0)\n",
    "\n",
    "# Hint: Performance should greater than -20.0 within one hour.\n",
    "# We do not require you to solve the task, but you need to make your agent achieve > -20 reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward for your Actor-critic agent in PongDeterministic-v4:  -19.0\n"
     ]
    }
   ],
   "source": [
    "surfix = \"iter130\"\n",
    "\n",
    "ac_trainer_pong_restored = pong_wrapper(ActorCriticTrainer)(pong_ac_config)\n",
    "ac_trainer_pong_restored.restore(surfix)\n",
    "\n",
    "print(\"Average episode reward for your Actor-critic agent in PongDeterministic-v4: \",\n",
    "      ac_trainer_pong_restored.evaluate(1, render=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Conclusion and Discussion\n",
    "\n",
    "In this assignment, we implement several policy gradient algorithms.\n",
    "\n",
    "It's OK to leave the following cells empty. In the next markdown cell, you can write whatever you like. Like the suggestion on the course, the confusing problems in the assignments, and so on.\n",
    "\n",
    "If you want to do more investigation, feel free to open new cells via `Esc + B` after the next cells and write codes in it, so that you can reuse some result in this notebook. Remember to write sufficient comments and documents to let others know what you are doing.\n",
    "\n",
    "Following the submission instruction in the assignment to submit your assignment to our staff. Thank you!\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
