{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "from gym import wrappers, logger\n",
    "import numpy as np\n",
    "from six.moves import cPickle as pickle\n",
    "import json, sys, os\n",
    "from os import path\n",
    "from _policies import BinaryActionLinearPolicy # Different file so it can be unpickled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cem(f, th_mean, batch_size, n_iter, elite_frac, initial_std=1.0):\n",
    "    \"\"\"\n",
    "    Generic implementation of the cross-entropy method for maximizing a black-box function\n",
    "    f: a function mapping from vector -> scalar\n",
    "    th_mean: initial mean over input distribution\n",
    "    batch_size: number of samples of theta to evaluate per batch\n",
    "    n_iter: number of batches\n",
    "    elite_frac: each batch, select this fraction of the top-performing samples\n",
    "    initial_std: initial standard deviation over parameter vectors\n",
    "    \"\"\"\n",
    "    n_elite = int(np.round(batch_size*elite_frac))\n",
    "    th_std = np.ones_like(th_mean) * initial_std\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        ths = np.array([th_mean + dth for dth in  th_std[None,:]*np.random.randn(batch_size, th_mean.size)])\n",
    "        ys = np.array([f(th) for th in ths])\n",
    "        elite_inds = ys.argsort()[::-1][:n_elite]\n",
    "        elite_ths = ths[elite_inds]\n",
    "        th_mean = elite_ths.mean(axis=0)\n",
    "        th_std = elite_ths.std(axis=0)\n",
    "        yield {'ys' : ys, 'theta_mean' : th_mean, 'y_mean' : ys.mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_rollout(agent, env, num_steps, render=False):\n",
    "    total_rew = 0\n",
    "    ob = env.reset()\n",
    "    for t in range(num_steps):\n",
    "        a = agent.act(ob)\n",
    "        (ob, reward, done, _info) = env.step(a)\n",
    "        total_rew += reward\n",
    "        if render and t%3==0: env.render()\n",
    "        if done: break\n",
    "    return total_rew, t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "params = dict(n_iter=100, batch_size=10, elite_frac = 0.2)\n",
    "num_steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_evaluation(theta):\n",
    "    agent = BinaryActionLinearPolicy(theta)\n",
    "    rew, T = do_rollout(agent, env, num_steps)\n",
    "    return rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0. Episode mean reward:  23.800\n",
      "Iteration  1. Episode mean reward:  92.000\n",
      "Iteration  2. Episode mean reward: 158.400\n",
      "Iteration  3. Episode mean reward: 179.100\n",
      "Iteration  4. Episode mean reward: 186.000\n",
      "Iteration  5. Episode mean reward: 188.300\n",
      "Iteration  6. Episode mean reward: 180.900\n",
      "Iteration  7. Episode mean reward: 188.700\n",
      "Iteration  8. Episode mean reward: 188.600\n",
      "Iteration  9. Episode mean reward: 185.300\n",
      "Iteration 10. Episode mean reward: 191.900\n",
      "Iteration 11. Episode mean reward: 193.000\n",
      "Iteration 12. Episode mean reward: 188.300\n",
      "Iteration 13. Episode mean reward: 183.400\n",
      "Iteration 14. Episode mean reward: 180.400\n",
      "Iteration 15. Episode mean reward: 197.100\n",
      "Iteration 16. Episode mean reward: 193.200\n",
      "Iteration 17. Episode mean reward: 188.500\n",
      "Iteration 18. Episode mean reward: 182.800\n",
      "Iteration 19. Episode mean reward: 193.900\n",
      "Iteration 20. Episode mean reward: 196.300\n",
      "Iteration 21. Episode mean reward: 184.700\n",
      "Iteration 22. Episode mean reward: 184.800\n",
      "Iteration 23. Episode mean reward: 188.100\n",
      "Iteration 24. Episode mean reward: 189.000\n",
      "Iteration 25. Episode mean reward: 186.700\n",
      "Iteration 26. Episode mean reward: 186.300\n",
      "Iteration 27. Episode mean reward: 192.100\n",
      "Iteration 28. Episode mean reward: 195.900\n",
      "Iteration 29. Episode mean reward: 188.100\n",
      "Iteration 30. Episode mean reward: 195.800\n",
      "Iteration 31. Episode mean reward: 188.700\n",
      "Iteration 32. Episode mean reward: 180.500\n",
      "Iteration 33. Episode mean reward: 188.000\n",
      "Iteration 34. Episode mean reward: 187.700\n",
      "Iteration 35. Episode mean reward: 199.700\n",
      "Iteration 36. Episode mean reward: 191.400\n",
      "Iteration 37. Episode mean reward: 190.700\n",
      "Iteration 38. Episode mean reward: 189.400\n",
      "Iteration 39. Episode mean reward: 184.600\n",
      "Iteration 40. Episode mean reward: 187.500\n",
      "Iteration 41. Episode mean reward: 196.300\n",
      "Iteration 42. Episode mean reward: 195.000\n",
      "Iteration 43. Episode mean reward: 176.300\n",
      "Iteration 44. Episode mean reward: 183.300\n",
      "Iteration 45. Episode mean reward: 195.900\n",
      "Iteration 46. Episode mean reward: 192.100\n",
      "Iteration 47. Episode mean reward: 196.000\n",
      "Iteration 48. Episode mean reward: 176.300\n",
      "Iteration 49. Episode mean reward: 179.500\n",
      "Iteration 50. Episode mean reward: 181.800\n",
      "Iteration 51. Episode mean reward: 176.400\n",
      "Iteration 52. Episode mean reward: 190.700\n",
      "Iteration 53. Episode mean reward: 181.800\n",
      "Iteration 54. Episode mean reward: 189.300\n",
      "Iteration 55. Episode mean reward: 192.200\n",
      "Iteration 56. Episode mean reward: 191.500\n",
      "Iteration 57. Episode mean reward: 185.700\n",
      "Iteration 58. Episode mean reward: 181.300\n",
      "Iteration 59. Episode mean reward: 183.700\n",
      "Iteration 60. Episode mean reward: 187.700\n",
      "Iteration 61. Episode mean reward: 188.400\n",
      "Iteration 62. Episode mean reward: 199.700\n",
      "Iteration 63. Episode mean reward: 192.200\n",
      "Iteration 64. Episode mean reward: 188.900\n",
      "Iteration 65. Episode mean reward: 187.900\n",
      "Iteration 66. Episode mean reward: 189.000\n",
      "Iteration 67. Episode mean reward: 189.000\n",
      "Iteration 68. Episode mean reward: 184.500\n",
      "Iteration 69. Episode mean reward: 191.400\n",
      "Iteration 70. Episode mean reward: 185.500\n",
      "Iteration 71. Episode mean reward: 198.700\n",
      "Iteration 72. Episode mean reward: 187.300\n",
      "Iteration 73. Episode mean reward: 190.300\n",
      "Iteration 74. Episode mean reward: 199.700\n",
      "Iteration 75. Episode mean reward: 188.600\n",
      "Iteration 76. Episode mean reward: 191.000\n",
      "Iteration 77. Episode mean reward: 184.300\n",
      "Iteration 78. Episode mean reward: 192.200\n",
      "Iteration 79. Episode mean reward: 192.700\n",
      "Iteration 80. Episode mean reward: 185.600\n",
      "Iteration 81. Episode mean reward: 175.400\n",
      "Iteration 82. Episode mean reward: 183.500\n",
      "Iteration 83. Episode mean reward: 195.800\n",
      "Iteration 84. Episode mean reward: 191.300\n",
      "Iteration 85. Episode mean reward: 192.000\n",
      "Iteration 86. Episode mean reward: 196.300\n",
      "Iteration 87. Episode mean reward: 197.300\n",
      "Iteration 88. Episode mean reward: 184.000\n",
      "Iteration 89. Episode mean reward: 192.800\n",
      "Iteration 90. Episode mean reward: 184.000\n",
      "Iteration 91. Episode mean reward: 184.700\n",
      "Iteration 92. Episode mean reward: 184.500\n",
      "Iteration 93. Episode mean reward: 192.400\n",
      "Iteration 94. Episode mean reward: 196.000\n",
      "Iteration 95. Episode mean reward: 193.800\n",
      "Iteration 96. Episode mean reward: 183.400\n",
      "Iteration 97. Episode mean reward: 196.400\n",
      "Iteration 98. Episode mean reward: 192.400\n",
      "Iteration 99. Episode mean reward: 178.500\n"
     ]
    }
   ],
   "source": [
    "# Train the agent, and snapshot each stage\n",
    "for (i, iterdata) in enumerate(cem(noisy_evaluation, np.zeros(env.observation_space.shape[0]+1), **params)):\n",
    "    print('Iteration %2i. Episode mean reward: %7.3f'%(i, iterdata['y_mean']))\n",
    "    agent = BinaryActionLinearPolicy(iterdata['theta_mean'])\n",
    "    do_rollout(agent, env, 200, render=False)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
