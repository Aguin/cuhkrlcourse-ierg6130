{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"gamma\": 0.99,\n",
    "    \"decay_rate\": 0.99,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": 20,\n",
    "    \"seed\": 87,\n",
    "    \"test\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9186f9a4b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "env.seed(args[\"seed\"])\n",
    "torch.manual_seed(args[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 80 * 80\n",
    "test = args[\"test\"]\n",
    "if test ==True:\n",
    "    render = True\n",
    "else:\n",
    "    render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 into 6400 \"\"\"\n",
    "    I = I[35:195]\n",
    "    I = I[::2, ::2, 0]\n",
    "    I[I == 144] = 0\n",
    "    I[I == 109] = 0\n",
    "    I[I != 0 ] = 1\n",
    "    return I.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGbaseline(nn.Module):\n",
    "    def __init__(self, num_actions=2):\n",
    "        super(PGbaseline, self).__init__()\n",
    "        self.affine1 = nn.Linear(6400, 200)\n",
    "        self.action_head = nn.Linear(200, num_actions) # action 1: static, action 2: move up, action 3: move down\n",
    "        self.value_head = nn.Linear(200, 1)\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.action_head(x)\n",
    "        state_values = self.value_head(x)\n",
    "        return F.softmax(action_scores, dim=-1), state_values\n",
    "\n",
    "\n",
    "    def select_action(self, x):\n",
    "        x = Variable(torch.from_numpy(x).float().unsqueeze(0))\n",
    "        if is_cuda: x = x.cuda()\n",
    "        probs, state_value = self.forward(x)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        self.saved_log_probs.append((m.log_prob(action), state_value))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built policy network\n",
    "policy = PGbaseline()\n",
    "if is_cuda:\n",
    "    policy.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check & load pretrain model\n",
    "if os.path.isfile('pgb_params.pkl'):\n",
    "    print('Load PGbaseline Network parametets ...')\n",
    "    if is_cuda:\n",
    "        policy.load_state_dict(torch.load('pgb_params.pkl'))\n",
    "    else:\n",
    "        policy.load_state_dict(torch.load('pgb_params.pkl', map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a optimal function\n",
    "optimizer = optim.RMSprop(policy.parameters(), lr=args[\"learning_rate\"], weight_decay=args[\"decay_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    value_loss = []\n",
    "    rewards = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + args[\"gamma\"] * R\n",
    "        rewards.insert(0, R)\n",
    "    # turn rewards to pytorch tensor and standardize\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-6)\n",
    "    if is_cuda: rewards = rewards.cuda()\n",
    "    for (log_prob, value), reward in zip(policy.saved_log_probs, rewards):\n",
    "        advantage = reward - value\n",
    "        policy_loss.append(- log_prob * advantage)         # policy gradient\n",
    "        value_loss.append(F.smooth_l1_loss(value, reward)) # value function approximation\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    value_loss = torch.stack(value_loss).sum()\n",
    "    loss = policy_loss + value_loss\n",
    "    if is_cuda:\n",
    "        loss.cuda()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # clean rewards and saved_actions\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Gradient with Baseline ep 001 done. reward: -21.000000. reward running mean: -21.000000\n",
      "Policy Gradient with Baseline ep 002 done. reward: -21.000000. reward running mean: -21.000000\n",
      "Policy Gradient with Baseline ep 003 done. reward: -21.000000. reward running mean: -21.000000\n",
      "Policy Gradient with Baseline ep 004 done. reward: -18.000000. reward running mean: -20.970000\n",
      "Policy Gradient with Baseline ep 005 done. reward: -21.000000. reward running mean: -20.970300\n",
      "Policy Gradient with Baseline ep 006 done. reward: -20.000000. reward running mean: -20.960597\n",
      "Policy Gradient with Baseline ep 007 done. reward: -19.000000. reward running mean: -20.940991\n",
      "Policy Gradient with Baseline ep 008 done. reward: -21.000000. reward running mean: -20.941581\n",
      "Policy Gradient with Baseline ep 009 done. reward: -20.000000. reward running mean: -20.932165\n",
      "Policy Gradient with Baseline ep 010 done. reward: -21.000000. reward running mean: -20.932844\n",
      "Policy Gradient with Baseline ep 011 done. reward: -21.000000. reward running mean: -20.933515\n",
      "Policy Gradient with Baseline ep 012 done. reward: -21.000000. reward running mean: -20.934180\n",
      "Policy Gradient with Baseline ep 013 done. reward: -21.000000. reward running mean: -20.934838\n",
      "Policy Gradient with Baseline ep 014 done. reward: -21.000000. reward running mean: -20.935490\n",
      "Policy Gradient with Baseline ep 015 done. reward: -21.000000. reward running mean: -20.936135\n",
      "Policy Gradient with Baseline ep 016 done. reward: -21.000000. reward running mean: -20.936774\n",
      "Policy Gradient with Baseline ep 017 done. reward: -21.000000. reward running mean: -20.937406\n",
      "Policy Gradient with Baseline ep 018 done. reward: -21.000000. reward running mean: -20.938032\n",
      "Policy Gradient with Baseline ep 019 done. reward: -21.000000. reward running mean: -20.938652\n",
      "Policy Gradient with Baseline ep 020 done. reward: -20.000000. reward running mean: -20.929265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Gradient with Baseline ep 021 done. reward: -20.000000. reward running mean: -20.919972\n",
      "Policy Gradient with Baseline ep 022 done. reward: -21.000000. reward running mean: -20.920773\n",
      "Policy Gradient with Baseline ep 023 done. reward: -21.000000. reward running mean: -20.921565\n",
      "Policy Gradient with Baseline ep 024 done. reward: -21.000000. reward running mean: -20.922349\n",
      "Policy Gradient with Baseline ep 025 done. reward: -21.000000. reward running mean: -20.923126\n",
      "Policy Gradient with Baseline ep 026 done. reward: -20.000000. reward running mean: -20.913895\n",
      "Policy Gradient with Baseline ep 027 done. reward: -21.000000. reward running mean: -20.914756\n",
      "Policy Gradient with Baseline ep 028 done. reward: -19.000000. reward running mean: -20.895608\n",
      "Policy Gradient with Baseline ep 029 done. reward: -20.000000. reward running mean: -20.886652\n",
      "Policy Gradient with Baseline ep 030 done. reward: -19.000000. reward running mean: -20.867785\n",
      "Policy Gradient with Baseline ep 031 done. reward: -20.000000. reward running mean: -20.859108\n",
      "Policy Gradient with Baseline ep 032 done. reward: -21.000000. reward running mean: -20.860516\n",
      "Policy Gradient with Baseline ep 033 done. reward: -21.000000. reward running mean: -20.861911\n",
      "Policy Gradient with Baseline ep 034 done. reward: -21.000000. reward running mean: -20.863292\n",
      "Policy Gradient with Baseline ep 035 done. reward: -21.000000. reward running mean: -20.864659\n",
      "Policy Gradient with Baseline ep 036 done. reward: -21.000000. reward running mean: -20.866013\n",
      "Policy Gradient with Baseline ep 037 done. reward: -18.000000. reward running mean: -20.837353\n",
      "Policy Gradient with Baseline ep 038 done. reward: -21.000000. reward running mean: -20.838979\n",
      "Policy Gradient with Baseline ep 039 done. reward: -21.000000. reward running mean: -20.840589\n",
      "Policy Gradient with Baseline ep 040 done. reward: -19.000000. reward running mean: -20.822183\n",
      "Policy Gradient with Baseline ep 041 done. reward: -20.000000. reward running mean: -20.813962\n",
      "Policy Gradient with Baseline ep 042 done. reward: -19.000000. reward running mean: -20.795822\n",
      "Policy Gradient with Baseline ep 043 done. reward: -20.000000. reward running mean: -20.787864\n",
      "Policy Gradient with Baseline ep 044 done. reward: -19.000000. reward running mean: -20.769985\n",
      "Policy Gradient with Baseline ep 045 done. reward: -20.000000. reward running mean: -20.762285\n",
      "Policy Gradient with Baseline ep 046 done. reward: -20.000000. reward running mean: -20.754662\n",
      "Policy Gradient with Baseline ep 047 done. reward: -20.000000. reward running mean: -20.747116\n",
      "Policy Gradient with Baseline ep 048 done. reward: -21.000000. reward running mean: -20.749645\n",
      "Policy Gradient with Baseline ep 049 done. reward: -21.000000. reward running mean: -20.752148\n",
      "Policy Gradient with Baseline ep 050 done. reward: -21.000000. reward running mean: -20.754627\n",
      "ep 50: model saving...\n",
      "Policy Gradient with Baseline ep 051 done. reward: -19.000000. reward running mean: -20.737080\n",
      "Policy Gradient with Baseline ep 052 done. reward: -20.000000. reward running mean: -20.729710\n",
      "Policy Gradient with Baseline ep 053 done. reward: -21.000000. reward running mean: -20.732412\n",
      "Policy Gradient with Baseline ep 054 done. reward: -21.000000. reward running mean: -20.735088\n",
      "Policy Gradient with Baseline ep 055 done. reward: -21.000000. reward running mean: -20.737737\n",
      "Policy Gradient with Baseline ep 056 done. reward: -21.000000. reward running mean: -20.740360\n",
      "Policy Gradient with Baseline ep 057 done. reward: -20.000000. reward running mean: -20.732956\n",
      "Policy Gradient with Baseline ep 058 done. reward: -21.000000. reward running mean: -20.735627\n",
      "Policy Gradient with Baseline ep 059 done. reward: -20.000000. reward running mean: -20.728271\n",
      "Policy Gradient with Baseline ep 060 done. reward: -21.000000. reward running mean: -20.730988\n",
      "Policy Gradient with Baseline ep 061 done. reward: -21.000000. reward running mean: -20.733678\n",
      "Policy Gradient with Baseline ep 062 done. reward: -21.000000. reward running mean: -20.736341\n",
      "Policy Gradient with Baseline ep 063 done. reward: -20.000000. reward running mean: -20.728978\n",
      "Policy Gradient with Baseline ep 064 done. reward: -21.000000. reward running mean: -20.731688\n",
      "Policy Gradient with Baseline ep 065 done. reward: -21.000000. reward running mean: -20.734371\n",
      "Policy Gradient with Baseline ep 066 done. reward: -21.000000. reward running mean: -20.737028\n",
      "Policy Gradient with Baseline ep 067 done. reward: -21.000000. reward running mean: -20.739657\n",
      "Policy Gradient with Baseline ep 068 done. reward: -21.000000. reward running mean: -20.742261\n",
      "Policy Gradient with Baseline ep 069 done. reward: -21.000000. reward running mean: -20.744838\n",
      "Policy Gradient with Baseline ep 070 done. reward: -21.000000. reward running mean: -20.747390\n",
      "Policy Gradient with Baseline ep 071 done. reward: -21.000000. reward running mean: -20.749916\n",
      "Policy Gradient with Baseline ep 072 done. reward: -20.000000. reward running mean: -20.742417\n",
      "Policy Gradient with Baseline ep 073 done. reward: -21.000000. reward running mean: -20.744992\n",
      "Policy Gradient with Baseline ep 074 done. reward: -21.000000. reward running mean: -20.747543\n",
      "Policy Gradient with Baseline ep 075 done. reward: -21.000000. reward running mean: -20.750067\n",
      "Policy Gradient with Baseline ep 076 done. reward: -21.000000. reward running mean: -20.752566\n",
      "Policy Gradient with Baseline ep 077 done. reward: -21.000000. reward running mean: -20.755041\n",
      "Policy Gradient with Baseline ep 078 done. reward: -21.000000. reward running mean: -20.757490\n",
      "Policy Gradient with Baseline ep 079 done. reward: -21.000000. reward running mean: -20.759915\n",
      "Policy Gradient with Baseline ep 080 done. reward: -21.000000. reward running mean: -20.762316\n",
      "Policy Gradient with Baseline ep 081 done. reward: -21.000000. reward running mean: -20.764693\n",
      "Policy Gradient with Baseline ep 082 done. reward: -21.000000. reward running mean: -20.767046\n",
      "Policy Gradient with Baseline ep 083 done. reward: -20.000000. reward running mean: -20.759376\n",
      "Policy Gradient with Baseline ep 084 done. reward: -21.000000. reward running mean: -20.761782\n",
      "Policy Gradient with Baseline ep 085 done. reward: -20.000000. reward running mean: -20.754164\n",
      "Policy Gradient with Baseline ep 086 done. reward: -21.000000. reward running mean: -20.756623\n",
      "Policy Gradient with Baseline ep 087 done. reward: -20.000000. reward running mean: -20.749056\n",
      "Policy Gradient with Baseline ep 088 done. reward: -20.000000. reward running mean: -20.741566\n",
      "Policy Gradient with Baseline ep 089 done. reward: -19.000000. reward running mean: -20.724150\n",
      "Policy Gradient with Baseline ep 090 done. reward: -21.000000. reward running mean: -20.726909\n",
      "Policy Gradient with Baseline ep 091 done. reward: -20.000000. reward running mean: -20.719639\n",
      "Policy Gradient with Baseline ep 092 done. reward: -21.000000. reward running mean: -20.722443\n",
      "Policy Gradient with Baseline ep 093 done. reward: -21.000000. reward running mean: -20.725219\n",
      "Policy Gradient with Baseline ep 094 done. reward: -21.000000. reward running mean: -20.727966\n",
      "Policy Gradient with Baseline ep 095 done. reward: -19.000000. reward running mean: -20.710687\n",
      "Policy Gradient with Baseline ep 096 done. reward: -21.000000. reward running mean: -20.713580\n",
      "Policy Gradient with Baseline ep 097 done. reward: -21.000000. reward running mean: -20.716444\n",
      "Policy Gradient with Baseline ep 098 done. reward: -21.000000. reward running mean: -20.719280\n",
      "Policy Gradient with Baseline ep 099 done. reward: -21.000000. reward running mean: -20.722087\n",
      "Policy Gradient with Baseline ep 100 done. reward: -21.000000. reward running mean: -20.724866\n",
      "ep 100: model saving...\n",
      "Policy Gradient with Baseline ep 101 done. reward: -21.000000. reward running mean: -20.727617\n",
      "Policy Gradient with Baseline ep 102 done. reward: -20.000000. reward running mean: -20.720341\n",
      "Policy Gradient with Baseline ep 103 done. reward: -21.000000. reward running mean: -20.723138\n",
      "Policy Gradient with Baseline ep 104 done. reward: -21.000000. reward running mean: -20.725906\n",
      "Policy Gradient with Baseline ep 105 done. reward: -20.000000. reward running mean: -20.718647\n",
      "Policy Gradient with Baseline ep 106 done. reward: -20.000000. reward running mean: -20.711461\n",
      "Policy Gradient with Baseline ep 107 done. reward: -21.000000. reward running mean: -20.714346\n",
      "Policy Gradient with Baseline ep 108 done. reward: -21.000000. reward running mean: -20.717203\n",
      "Policy Gradient with Baseline ep 109 done. reward: -21.000000. reward running mean: -20.720031\n",
      "Policy Gradient with Baseline ep 110 done. reward: -21.000000. reward running mean: -20.722830\n",
      "Policy Gradient with Baseline ep 111 done. reward: -21.000000. reward running mean: -20.725602\n",
      "Policy Gradient with Baseline ep 112 done. reward: -20.000000. reward running mean: -20.718346\n",
      "Policy Gradient with Baseline ep 113 done. reward: -20.000000. reward running mean: -20.711163\n",
      "Policy Gradient with Baseline ep 114 done. reward: -21.000000. reward running mean: -20.714051\n",
      "Policy Gradient with Baseline ep 115 done. reward: -21.000000. reward running mean: -20.716911\n",
      "Policy Gradient with Baseline ep 116 done. reward: -21.000000. reward running mean: -20.719741\n",
      "Policy Gradient with Baseline ep 117 done. reward: -21.000000. reward running mean: -20.722544\n",
      "Policy Gradient with Baseline ep 118 done. reward: -21.000000. reward running mean: -20.725319\n",
      "Policy Gradient with Baseline ep 119 done. reward: -21.000000. reward running mean: -20.728065\n",
      "Policy Gradient with Baseline ep 120 done. reward: -20.000000. reward running mean: -20.720785\n",
      "Policy Gradient with Baseline ep 121 done. reward: -20.000000. reward running mean: -20.713577\n",
      "Policy Gradient with Baseline ep 122 done. reward: -20.000000. reward running mean: -20.706441\n",
      "Policy Gradient with Baseline ep 123 done. reward: -21.000000. reward running mean: -20.709377\n",
      "Policy Gradient with Baseline ep 124 done. reward: -20.000000. reward running mean: -20.702283\n",
      "Policy Gradient with Baseline ep 125 done. reward: -21.000000. reward running mean: -20.705260\n",
      "Policy Gradient with Baseline ep 126 done. reward: -21.000000. reward running mean: -20.708208\n",
      "Policy Gradient with Baseline ep 127 done. reward: -21.000000. reward running mean: -20.711125\n",
      "Policy Gradient with Baseline ep 128 done. reward: -16.000000. reward running mean: -20.664014\n",
      "Policy Gradient with Baseline ep 129 done. reward: -21.000000. reward running mean: -20.667374\n",
      "Policy Gradient with Baseline ep 130 done. reward: -21.000000. reward running mean: -20.670700\n",
      "Policy Gradient with Baseline ep 131 done. reward: -21.000000. reward running mean: -20.673993\n",
      "Policy Gradient with Baseline ep 132 done. reward: -20.000000. reward running mean: -20.667253\n",
      "Policy Gradient with Baseline ep 133 done. reward: -19.000000. reward running mean: -20.650581\n",
      "Policy Gradient with Baseline ep 134 done. reward: -21.000000. reward running mean: -20.654075\n",
      "Policy Gradient with Baseline ep 135 done. reward: -20.000000. reward running mean: -20.647534\n",
      "Policy Gradient with Baseline ep 136 done. reward: -21.000000. reward running mean: -20.651059\n",
      "Policy Gradient with Baseline ep 137 done. reward: -19.000000. reward running mean: -20.634548\n",
      "Policy Gradient with Baseline ep 138 done. reward: -20.000000. reward running mean: -20.628203\n",
      "Policy Gradient with Baseline ep 139 done. reward: -21.000000. reward running mean: -20.631921\n",
      "Policy Gradient with Baseline ep 140 done. reward: -21.000000. reward running mean: -20.635602\n",
      "Policy Gradient with Baseline ep 141 done. reward: -20.000000. reward running mean: -20.629246\n",
      "Policy Gradient with Baseline ep 142 done. reward: -20.000000. reward running mean: -20.622953\n",
      "Policy Gradient with Baseline ep 143 done. reward: -21.000000. reward running mean: -20.626724\n",
      "Policy Gradient with Baseline ep 144 done. reward: -21.000000. reward running mean: -20.630456\n",
      "Policy Gradient with Baseline ep 145 done. reward: -21.000000. reward running mean: -20.634152\n",
      "Policy Gradient with Baseline ep 146 done. reward: -21.000000. reward running mean: -20.637810\n",
      "Policy Gradient with Baseline ep 147 done. reward: -20.000000. reward running mean: -20.631432\n",
      "Policy Gradient with Baseline ep 148 done. reward: -19.000000. reward running mean: -20.615118\n",
      "Policy Gradient with Baseline ep 149 done. reward: -21.000000. reward running mean: -20.618967\n",
      "Policy Gradient with Baseline ep 150 done. reward: -20.000000. reward running mean: -20.612777\n",
      "ep 150: model saving...\n",
      "Policy Gradient with Baseline ep 151 done. reward: -21.000000. reward running mean: -20.616649\n",
      "Policy Gradient with Baseline ep 152 done. reward: -21.000000. reward running mean: -20.620483\n",
      "Policy Gradient with Baseline ep 153 done. reward: -20.000000. reward running mean: -20.614278\n",
      "Policy Gradient with Baseline ep 154 done. reward: -20.000000. reward running mean: -20.608135\n",
      "Policy Gradient with Baseline ep 155 done. reward: -19.000000. reward running mean: -20.592054\n",
      "Policy Gradient with Baseline ep 156 done. reward: -21.000000. reward running mean: -20.596133\n",
      "Policy Gradient with Baseline ep 157 done. reward: -21.000000. reward running mean: -20.600172\n",
      "Policy Gradient with Baseline ep 158 done. reward: -21.000000. reward running mean: -20.604170\n",
      "Policy Gradient with Baseline ep 159 done. reward: -20.000000. reward running mean: -20.598129\n",
      "Policy Gradient with Baseline ep 160 done. reward: -20.000000. reward running mean: -20.592147\n",
      "Policy Gradient with Baseline ep 161 done. reward: -21.000000. reward running mean: -20.596226\n",
      "Policy Gradient with Baseline ep 162 done. reward: -16.000000. reward running mean: -20.550264\n",
      "Policy Gradient with Baseline ep 163 done. reward: -21.000000. reward running mean: -20.554761\n",
      "Policy Gradient with Baseline ep 164 done. reward: -21.000000. reward running mean: -20.559213\n",
      "Policy Gradient with Baseline ep 165 done. reward: -21.000000. reward running mean: -20.563621\n",
      "Policy Gradient with Baseline ep 166 done. reward: -21.000000. reward running mean: -20.567985\n",
      "Policy Gradient with Baseline ep 167 done. reward: -20.000000. reward running mean: -20.562305\n",
      "Policy Gradient with Baseline ep 168 done. reward: -20.000000. reward running mean: -20.556682\n",
      "Policy Gradient with Baseline ep 169 done. reward: -21.000000. reward running mean: -20.561115\n",
      "Policy Gradient with Baseline ep 170 done. reward: -19.000000. reward running mean: -20.545504\n",
      "Policy Gradient with Baseline ep 171 done. reward: -21.000000. reward running mean: -20.550049\n",
      "Policy Gradient with Baseline ep 172 done. reward: -21.000000. reward running mean: -20.554549\n",
      "Policy Gradient with Baseline ep 173 done. reward: -20.000000. reward running mean: -20.549003\n",
      "Policy Gradient with Baseline ep 174 done. reward: -21.000000. reward running mean: -20.553513\n",
      "Policy Gradient with Baseline ep 175 done. reward: -21.000000. reward running mean: -20.557978\n",
      "Policy Gradient with Baseline ep 176 done. reward: -21.000000. reward running mean: -20.562398\n",
      "Policy Gradient with Baseline ep 177 done. reward: -19.000000. reward running mean: -20.546774\n",
      "Policy Gradient with Baseline ep 178 done. reward: -20.000000. reward running mean: -20.541306\n",
      "Policy Gradient with Baseline ep 179 done. reward: -20.000000. reward running mean: -20.535893\n",
      "Policy Gradient with Baseline ep 180 done. reward: -20.000000. reward running mean: -20.530534\n",
      "Policy Gradient with Baseline ep 181 done. reward: -21.000000. reward running mean: -20.535229\n",
      "Policy Gradient with Baseline ep 182 done. reward: -20.000000. reward running mean: -20.529877\n",
      "Policy Gradient with Baseline ep 183 done. reward: -20.000000. reward running mean: -20.524578\n",
      "Policy Gradient with Baseline ep 184 done. reward: -21.000000. reward running mean: -20.529332\n",
      "Policy Gradient with Baseline ep 185 done. reward: -19.000000. reward running mean: -20.514039\n",
      "Policy Gradient with Baseline ep 186 done. reward: -18.000000. reward running mean: -20.488898\n",
      "Policy Gradient with Baseline ep 187 done. reward: -21.000000. reward running mean: -20.494009\n",
      "Policy Gradient with Baseline ep 188 done. reward: -21.000000. reward running mean: -20.499069\n",
      "Policy Gradient with Baseline ep 189 done. reward: -21.000000. reward running mean: -20.504079\n",
      "Policy Gradient with Baseline ep 190 done. reward: -18.000000. reward running mean: -20.479038\n",
      "Policy Gradient with Baseline ep 191 done. reward: -19.000000. reward running mean: -20.464248\n",
      "Policy Gradient with Baseline ep 192 done. reward: -21.000000. reward running mean: -20.469605\n",
      "Policy Gradient with Baseline ep 193 done. reward: -18.000000. reward running mean: -20.444909\n",
      "Policy Gradient with Baseline ep 194 done. reward: -21.000000. reward running mean: -20.450460\n",
      "Policy Gradient with Baseline ep 195 done. reward: -18.000000. reward running mean: -20.425955\n",
      "Policy Gradient with Baseline ep 196 done. reward: -21.000000. reward running mean: -20.431696\n",
      "Policy Gradient with Baseline ep 197 done. reward: -21.000000. reward running mean: -20.437379\n",
      "Policy Gradient with Baseline ep 198 done. reward: -21.000000. reward running mean: -20.443005\n",
      "Policy Gradient with Baseline ep 199 done. reward: -20.000000. reward running mean: -20.438575\n",
      "Policy Gradient with Baseline ep 200 done. reward: -20.000000. reward running mean: -20.434189\n",
      "ep 200: model saving...\n",
      "Policy Gradient with Baseline ep 201 done. reward: -21.000000. reward running mean: -20.439847\n",
      "Policy Gradient with Baseline ep 202 done. reward: -20.000000. reward running mean: -20.435449\n",
      "Policy Gradient with Baseline ep 203 done. reward: -17.000000. reward running mean: -20.401094\n",
      "Policy Gradient with Baseline ep 204 done. reward: -21.000000. reward running mean: -20.407083\n",
      "Policy Gradient with Baseline ep 205 done. reward: -19.000000. reward running mean: -20.393013\n",
      "Policy Gradient with Baseline ep 206 done. reward: -21.000000. reward running mean: -20.399082\n",
      "Policy Gradient with Baseline ep 207 done. reward: -21.000000. reward running mean: -20.405092\n",
      "Policy Gradient with Baseline ep 208 done. reward: -21.000000. reward running mean: -20.411041\n",
      "Policy Gradient with Baseline ep 209 done. reward: -19.000000. reward running mean: -20.396930\n",
      "Policy Gradient with Baseline ep 210 done. reward: -20.000000. reward running mean: -20.392961\n",
      "Policy Gradient with Baseline ep 211 done. reward: -18.000000. reward running mean: -20.369031\n",
      "Policy Gradient with Baseline ep 212 done. reward: -17.000000. reward running mean: -20.335341\n",
      "Policy Gradient with Baseline ep 213 done. reward: -20.000000. reward running mean: -20.331988\n",
      "Policy Gradient with Baseline ep 214 done. reward: -18.000000. reward running mean: -20.308668\n",
      "Policy Gradient with Baseline ep 215 done. reward: -19.000000. reward running mean: -20.295581\n",
      "Policy Gradient with Baseline ep 216 done. reward: -21.000000. reward running mean: -20.302625\n",
      "Policy Gradient with Baseline ep 217 done. reward: -21.000000. reward running mean: -20.309599\n",
      "Policy Gradient with Baseline ep 218 done. reward: -21.000000. reward running mean: -20.316503\n",
      "Policy Gradient with Baseline ep 219 done. reward: -21.000000. reward running mean: -20.323338\n",
      "Policy Gradient with Baseline ep 220 done. reward: -19.000000. reward running mean: -20.310105\n",
      "Policy Gradient with Baseline ep 221 done. reward: -21.000000. reward running mean: -20.317004\n",
      "Policy Gradient with Baseline ep 222 done. reward: -21.000000. reward running mean: -20.323834\n",
      "Policy Gradient with Baseline ep 223 done. reward: -21.000000. reward running mean: -20.330595\n",
      "Policy Gradient with Baseline ep 224 done. reward: -19.000000. reward running mean: -20.317289\n",
      "Policy Gradient with Baseline ep 225 done. reward: -20.000000. reward running mean: -20.314116\n",
      "Policy Gradient with Baseline ep 226 done. reward: -21.000000. reward running mean: -20.320975\n",
      "Policy Gradient with Baseline ep 227 done. reward: -21.000000. reward running mean: -20.327765\n",
      "Policy Gradient with Baseline ep 228 done. reward: -21.000000. reward running mean: -20.334488\n",
      "Policy Gradient with Baseline ep 229 done. reward: -21.000000. reward running mean: -20.341143\n",
      "Policy Gradient with Baseline ep 230 done. reward: -20.000000. reward running mean: -20.337732\n",
      "Policy Gradient with Baseline ep 231 done. reward: -18.000000. reward running mean: -20.314354\n",
      "Policy Gradient with Baseline ep 232 done. reward: -21.000000. reward running mean: -20.321211\n",
      "Policy Gradient with Baseline ep 233 done. reward: -21.000000. reward running mean: -20.327999\n",
      "Policy Gradient with Baseline ep 234 done. reward: -20.000000. reward running mean: -20.324719\n",
      "Policy Gradient with Baseline ep 235 done. reward: -21.000000. reward running mean: -20.331471\n",
      "Policy Gradient with Baseline ep 236 done. reward: -21.000000. reward running mean: -20.338157\n",
      "Policy Gradient with Baseline ep 237 done. reward: -21.000000. reward running mean: -20.344775\n",
      "Policy Gradient with Baseline ep 238 done. reward: -20.000000. reward running mean: -20.341327\n",
      "Policy Gradient with Baseline ep 239 done. reward: -21.000000. reward running mean: -20.347914\n",
      "Policy Gradient with Baseline ep 240 done. reward: -21.000000. reward running mean: -20.354435\n",
      "Policy Gradient with Baseline ep 241 done. reward: -20.000000. reward running mean: -20.350891\n",
      "Policy Gradient with Baseline ep 242 done. reward: -20.000000. reward running mean: -20.347382\n",
      "Policy Gradient with Baseline ep 243 done. reward: -21.000000. reward running mean: -20.353908\n",
      "Policy Gradient with Baseline ep 244 done. reward: -20.000000. reward running mean: -20.350369\n",
      "Policy Gradient with Baseline ep 245 done. reward: -17.000000. reward running mean: -20.316865\n",
      "Policy Gradient with Baseline ep 246 done. reward: -20.000000. reward running mean: -20.313696\n",
      "Policy Gradient with Baseline ep 247 done. reward: -19.000000. reward running mean: -20.300559\n",
      "Policy Gradient with Baseline ep 248 done. reward: -21.000000. reward running mean: -20.307554\n",
      "Policy Gradient with Baseline ep 249 done. reward: -20.000000. reward running mean: -20.304478\n",
      "Policy Gradient with Baseline ep 250 done. reward: -21.000000. reward running mean: -20.311434\n",
      "ep 250: model saving...\n",
      "Policy Gradient with Baseline ep 251 done. reward: -21.000000. reward running mean: -20.318319\n",
      "Policy Gradient with Baseline ep 252 done. reward: -21.000000. reward running mean: -20.325136\n",
      "Policy Gradient with Baseline ep 253 done. reward: -18.000000. reward running mean: -20.301885\n",
      "Policy Gradient with Baseline ep 254 done. reward: -21.000000. reward running mean: -20.308866\n",
      "Policy Gradient with Baseline ep 255 done. reward: -19.000000. reward running mean: -20.295777\n",
      "Policy Gradient with Baseline ep 256 done. reward: -20.000000. reward running mean: -20.292819\n",
      "Policy Gradient with Baseline ep 257 done. reward: -20.000000. reward running mean: -20.289891\n",
      "Policy Gradient with Baseline ep 258 done. reward: -20.000000. reward running mean: -20.286992\n",
      "Policy Gradient with Baseline ep 259 done. reward: -19.000000. reward running mean: -20.274122\n",
      "Policy Gradient with Baseline ep 260 done. reward: -21.000000. reward running mean: -20.281381\n",
      "Policy Gradient with Baseline ep 261 done. reward: -21.000000. reward running mean: -20.288567\n",
      "Policy Gradient with Baseline ep 262 done. reward: -20.000000. reward running mean: -20.285682\n",
      "Policy Gradient with Baseline ep 263 done. reward: -21.000000. reward running mean: -20.292825\n",
      "Policy Gradient with Baseline ep 264 done. reward: -20.000000. reward running mean: -20.289897\n",
      "Policy Gradient with Baseline ep 265 done. reward: -21.000000. reward running mean: -20.296998\n",
      "Policy Gradient with Baseline ep 266 done. reward: -21.000000. reward running mean: -20.304028\n",
      "Policy Gradient with Baseline ep 267 done. reward: -21.000000. reward running mean: -20.310987\n",
      "Policy Gradient with Baseline ep 268 done. reward: -21.000000. reward running mean: -20.317878\n",
      "Policy Gradient with Baseline ep 269 done. reward: -20.000000. reward running mean: -20.314699\n",
      "Policy Gradient with Baseline ep 270 done. reward: -20.000000. reward running mean: -20.311552\n",
      "Policy Gradient with Baseline ep 271 done. reward: -21.000000. reward running mean: -20.318436\n",
      "Policy Gradient with Baseline ep 272 done. reward: -21.000000. reward running mean: -20.325252\n",
      "Policy Gradient with Baseline ep 273 done. reward: -20.000000. reward running mean: -20.321999\n",
      "Policy Gradient with Baseline ep 274 done. reward: -21.000000. reward running mean: -20.328779\n",
      "Policy Gradient with Baseline ep 275 done. reward: -21.000000. reward running mean: -20.335492\n",
      "Policy Gradient with Baseline ep 276 done. reward: -21.000000. reward running mean: -20.342137\n",
      "Policy Gradient with Baseline ep 277 done. reward: -20.000000. reward running mean: -20.338715\n",
      "Policy Gradient with Baseline ep 278 done. reward: -19.000000. reward running mean: -20.325328\n",
      "Policy Gradient with Baseline ep 279 done. reward: -21.000000. reward running mean: -20.332075\n",
      "Policy Gradient with Baseline ep 280 done. reward: -20.000000. reward running mean: -20.328754\n",
      "Policy Gradient with Baseline ep 281 done. reward: -18.000000. reward running mean: -20.305467\n",
      "Policy Gradient with Baseline ep 282 done. reward: -20.000000. reward running mean: -20.302412\n",
      "Policy Gradient with Baseline ep 283 done. reward: -21.000000. reward running mean: -20.309388\n",
      "Policy Gradient with Baseline ep 284 done. reward: -21.000000. reward running mean: -20.316294\n",
      "Policy Gradient with Baseline ep 285 done. reward: -20.000000. reward running mean: -20.313131\n",
      "Policy Gradient with Baseline ep 286 done. reward: -20.000000. reward running mean: -20.310000\n",
      "Policy Gradient with Baseline ep 287 done. reward: -20.000000. reward running mean: -20.306900\n",
      "Policy Gradient with Baseline ep 288 done. reward: -21.000000. reward running mean: -20.313831\n",
      "Policy Gradient with Baseline ep 289 done. reward: -20.000000. reward running mean: -20.310692\n",
      "Policy Gradient with Baseline ep 290 done. reward: -20.000000. reward running mean: -20.307585\n",
      "Policy Gradient with Baseline ep 291 done. reward: -20.000000. reward running mean: -20.304510\n",
      "Policy Gradient with Baseline ep 292 done. reward: -21.000000. reward running mean: -20.311464\n",
      "Policy Gradient with Baseline ep 293 done. reward: -21.000000. reward running mean: -20.318350\n",
      "Policy Gradient with Baseline ep 294 done. reward: -21.000000. reward running mean: -20.325166\n",
      "Policy Gradient with Baseline ep 295 done. reward: -19.000000. reward running mean: -20.311915\n",
      "Policy Gradient with Baseline ep 296 done. reward: -20.000000. reward running mean: -20.308796\n",
      "Policy Gradient with Baseline ep 297 done. reward: -19.000000. reward running mean: -20.295708\n",
      "Policy Gradient with Baseline ep 298 done. reward: -19.000000. reward running mean: -20.282750\n",
      "Policy Gradient with Baseline ep 299 done. reward: -20.000000. reward running mean: -20.279923\n",
      "Policy Gradient with Baseline ep 300 done. reward: -21.000000. reward running mean: -20.287124\n",
      "ep 300: model saving...\n",
      "Policy Gradient with Baseline ep 301 done. reward: -21.000000. reward running mean: -20.294253\n",
      "Policy Gradient with Baseline ep 302 done. reward: -21.000000. reward running mean: -20.301310\n",
      "Policy Gradient with Baseline ep 303 done. reward: -19.000000. reward running mean: -20.288297\n",
      "Policy Gradient with Baseline ep 304 done. reward: -21.000000. reward running mean: -20.295414\n",
      "Policy Gradient with Baseline ep 305 done. reward: -21.000000. reward running mean: -20.302460\n",
      "Policy Gradient with Baseline ep 306 done. reward: -21.000000. reward running mean: -20.309435\n",
      "Policy Gradient with Baseline ep 307 done. reward: -20.000000. reward running mean: -20.306341\n",
      "Policy Gradient with Baseline ep 308 done. reward: -21.000000. reward running mean: -20.313277\n",
      "Policy Gradient with Baseline ep 309 done. reward: -21.000000. reward running mean: -20.320145\n",
      "Policy Gradient with Baseline ep 310 done. reward: -21.000000. reward running mean: -20.326943\n",
      "Policy Gradient with Baseline ep 311 done. reward: -20.000000. reward running mean: -20.323674\n",
      "Policy Gradient with Baseline ep 312 done. reward: -19.000000. reward running mean: -20.310437\n",
      "Policy Gradient with Baseline ep 313 done. reward: -21.000000. reward running mean: -20.317333\n",
      "Policy Gradient with Baseline ep 314 done. reward: -21.000000. reward running mean: -20.324159\n",
      "Policy Gradient with Baseline ep 315 done. reward: -21.000000. reward running mean: -20.330918\n",
      "Policy Gradient with Baseline ep 316 done. reward: -21.000000. reward running mean: -20.337609\n",
      "Policy Gradient with Baseline ep 317 done. reward: -21.000000. reward running mean: -20.344232\n",
      "Policy Gradient with Baseline ep 318 done. reward: -20.000000. reward running mean: -20.340790\n",
      "Policy Gradient with Baseline ep 319 done. reward: -20.000000. reward running mean: -20.337382\n",
      "Policy Gradient with Baseline ep 320 done. reward: -21.000000. reward running mean: -20.344008\n",
      "Policy Gradient with Baseline ep 321 done. reward: -21.000000. reward running mean: -20.350568\n",
      "Policy Gradient with Baseline ep 322 done. reward: -21.000000. reward running mean: -20.357063\n",
      "Policy Gradient with Baseline ep 323 done. reward: -21.000000. reward running mean: -20.363492\n",
      "Policy Gradient with Baseline ep 324 done. reward: -21.000000. reward running mean: -20.369857\n",
      "Policy Gradient with Baseline ep 325 done. reward: -20.000000. reward running mean: -20.366159\n",
      "Policy Gradient with Baseline ep 326 done. reward: -21.000000. reward running mean: -20.372497\n",
      "Policy Gradient with Baseline ep 327 done. reward: -20.000000. reward running mean: -20.368772\n",
      "Policy Gradient with Baseline ep 328 done. reward: -21.000000. reward running mean: -20.375084\n",
      "Policy Gradient with Baseline ep 329 done. reward: -20.000000. reward running mean: -20.371333\n",
      "Policy Gradient with Baseline ep 330 done. reward: -20.000000. reward running mean: -20.367620\n",
      "Policy Gradient with Baseline ep 331 done. reward: -20.000000. reward running mean: -20.363944\n",
      "Policy Gradient with Baseline ep 332 done. reward: -20.000000. reward running mean: -20.360304\n",
      "Policy Gradient with Baseline ep 333 done. reward: -21.000000. reward running mean: -20.366701\n",
      "Policy Gradient with Baseline ep 334 done. reward: -21.000000. reward running mean: -20.373034\n",
      "Policy Gradient with Baseline ep 335 done. reward: -21.000000. reward running mean: -20.379304\n",
      "Policy Gradient with Baseline ep 336 done. reward: -21.000000. reward running mean: -20.385511\n",
      "Policy Gradient with Baseline ep 337 done. reward: -21.000000. reward running mean: -20.391656\n",
      "Policy Gradient with Baseline ep 338 done. reward: -21.000000. reward running mean: -20.397739\n",
      "Policy Gradient with Baseline ep 339 done. reward: -21.000000. reward running mean: -20.403762\n",
      "Policy Gradient with Baseline ep 340 done. reward: -21.000000. reward running mean: -20.409724\n",
      "Policy Gradient with Baseline ep 341 done. reward: -21.000000. reward running mean: -20.415627\n",
      "Policy Gradient with Baseline ep 342 done. reward: -21.000000. reward running mean: -20.421471\n",
      "Policy Gradient with Baseline ep 343 done. reward: -21.000000. reward running mean: -20.427256\n",
      "Policy Gradient with Baseline ep 344 done. reward: -20.000000. reward running mean: -20.422984\n",
      "Policy Gradient with Baseline ep 345 done. reward: -21.000000. reward running mean: -20.428754\n",
      "Policy Gradient with Baseline ep 346 done. reward: -20.000000. reward running mean: -20.424466\n",
      "Policy Gradient with Baseline ep 347 done. reward: -19.000000. reward running mean: -20.410222\n",
      "Policy Gradient with Baseline ep 348 done. reward: -21.000000. reward running mean: -20.416119\n",
      "Policy Gradient with Baseline ep 349 done. reward: -21.000000. reward running mean: -20.421958\n",
      "Policy Gradient with Baseline ep 350 done. reward: -21.000000. reward running mean: -20.427739\n",
      "ep 350: model saving...\n",
      "Policy Gradient with Baseline ep 351 done. reward: -20.000000. reward running mean: -20.423461\n",
      "Policy Gradient with Baseline ep 352 done. reward: -20.000000. reward running mean: -20.419227\n",
      "Policy Gradient with Baseline ep 353 done. reward: -19.000000. reward running mean: -20.405034\n",
      "Policy Gradient with Baseline ep 354 done. reward: -20.000000. reward running mean: -20.400984\n",
      "Policy Gradient with Baseline ep 355 done. reward: -21.000000. reward running mean: -20.406974\n",
      "Policy Gradient with Baseline ep 356 done. reward: -21.000000. reward running mean: -20.412904\n",
      "Policy Gradient with Baseline ep 357 done. reward: -19.000000. reward running mean: -20.398775\n",
      "Policy Gradient with Baseline ep 358 done. reward: -18.000000. reward running mean: -20.374788\n",
      "Policy Gradient with Baseline ep 359 done. reward: -21.000000. reward running mean: -20.381040\n",
      "Policy Gradient with Baseline ep 360 done. reward: -20.000000. reward running mean: -20.377229\n",
      "Policy Gradient with Baseline ep 361 done. reward: -20.000000. reward running mean: -20.373457\n",
      "Policy Gradient with Baseline ep 362 done. reward: -21.000000. reward running mean: -20.379722\n",
      "Policy Gradient with Baseline ep 363 done. reward: -20.000000. reward running mean: -20.375925\n",
      "Policy Gradient with Baseline ep 364 done. reward: -21.000000. reward running mean: -20.382166\n",
      "Policy Gradient with Baseline ep 365 done. reward: -21.000000. reward running mean: -20.388344\n",
      "Policy Gradient with Baseline ep 366 done. reward: -20.000000. reward running mean: -20.384461\n",
      "Policy Gradient with Baseline ep 367 done. reward: -21.000000. reward running mean: -20.390616\n",
      "Policy Gradient with Baseline ep 368 done. reward: -21.000000. reward running mean: -20.396710\n",
      "Policy Gradient with Baseline ep 369 done. reward: -17.000000. reward running mean: -20.362743\n",
      "Policy Gradient with Baseline ep 370 done. reward: -21.000000. reward running mean: -20.369116\n",
      "Policy Gradient with Baseline ep 371 done. reward: -21.000000. reward running mean: -20.375424\n",
      "Policy Gradient with Baseline ep 372 done. reward: -21.000000. reward running mean: -20.381670\n",
      "Policy Gradient with Baseline ep 373 done. reward: -20.000000. reward running mean: -20.377853\n",
      "Policy Gradient with Baseline ep 374 done. reward: -21.000000. reward running mean: -20.384075\n",
      "Policy Gradient with Baseline ep 375 done. reward: -20.000000. reward running mean: -20.380234\n",
      "Policy Gradient with Baseline ep 376 done. reward: -20.000000. reward running mean: -20.376432\n",
      "Policy Gradient with Baseline ep 377 done. reward: -21.000000. reward running mean: -20.382667\n",
      "Policy Gradient with Baseline ep 378 done. reward: -21.000000. reward running mean: -20.388841\n",
      "Policy Gradient with Baseline ep 379 done. reward: -20.000000. reward running mean: -20.384952\n",
      "Policy Gradient with Baseline ep 380 done. reward: -21.000000. reward running mean: -20.391103\n",
      "Policy Gradient with Baseline ep 381 done. reward: -21.000000. reward running mean: -20.397192\n",
      "Policy Gradient with Baseline ep 382 done. reward: -21.000000. reward running mean: -20.403220\n",
      "Policy Gradient with Baseline ep 383 done. reward: -21.000000. reward running mean: -20.409188\n",
      "Policy Gradient with Baseline ep 384 done. reward: -20.000000. reward running mean: -20.405096\n",
      "Policy Gradient with Baseline ep 385 done. reward: -21.000000. reward running mean: -20.411045\n",
      "Policy Gradient with Baseline ep 386 done. reward: -20.000000. reward running mean: -20.406934\n",
      "Policy Gradient with Baseline ep 387 done. reward: -21.000000. reward running mean: -20.412865\n",
      "Policy Gradient with Baseline ep 388 done. reward: -20.000000. reward running mean: -20.408736\n",
      "Policy Gradient with Baseline ep 389 done. reward: -21.000000. reward running mean: -20.414649\n",
      "Policy Gradient with Baseline ep 390 done. reward: -20.000000. reward running mean: -20.410503\n",
      "Policy Gradient with Baseline ep 391 done. reward: -21.000000. reward running mean: -20.416398\n",
      "Policy Gradient with Baseline ep 392 done. reward: -20.000000. reward running mean: -20.412234\n",
      "Policy Gradient with Baseline ep 393 done. reward: -21.000000. reward running mean: -20.418111\n",
      "Policy Gradient with Baseline ep 394 done. reward: -21.000000. reward running mean: -20.423930\n",
      "Policy Gradient with Baseline ep 395 done. reward: -21.000000. reward running mean: -20.429691\n",
      "Policy Gradient with Baseline ep 396 done. reward: -21.000000. reward running mean: -20.435394\n",
      "Policy Gradient with Baseline ep 397 done. reward: -20.000000. reward running mean: -20.431040\n",
      "Policy Gradient with Baseline ep 398 done. reward: -20.000000. reward running mean: -20.426730\n",
      "Policy Gradient with Baseline ep 399 done. reward: -21.000000. reward running mean: -20.432462\n",
      "Policy Gradient with Baseline ep 400 done. reward: -20.000000. reward running mean: -20.428138\n",
      "ep 400: model saving...\n",
      "Policy Gradient with Baseline ep 401 done. reward: -21.000000. reward running mean: -20.433856\n",
      "Policy Gradient with Baseline ep 402 done. reward: -21.000000. reward running mean: -20.439518\n",
      "Policy Gradient with Baseline ep 403 done. reward: -21.000000. reward running mean: -20.445123\n",
      "Policy Gradient with Baseline ep 404 done. reward: -20.000000. reward running mean: -20.440671\n",
      "Policy Gradient with Baseline ep 405 done. reward: -21.000000. reward running mean: -20.446265\n",
      "Policy Gradient with Baseline ep 406 done. reward: -20.000000. reward running mean: -20.441802\n",
      "Policy Gradient with Baseline ep 407 done. reward: -21.000000. reward running mean: -20.447384\n",
      "Policy Gradient with Baseline ep 408 done. reward: -20.000000. reward running mean: -20.442910\n",
      "Policy Gradient with Baseline ep 409 done. reward: -20.000000. reward running mean: -20.438481\n",
      "Policy Gradient with Baseline ep 410 done. reward: -21.000000. reward running mean: -20.444096\n",
      "Policy Gradient with Baseline ep 411 done. reward: -21.000000. reward running mean: -20.449655\n",
      "Policy Gradient with Baseline ep 412 done. reward: -21.000000. reward running mean: -20.455159\n",
      "Policy Gradient with Baseline ep 413 done. reward: -21.000000. reward running mean: -20.460607\n",
      "Policy Gradient with Baseline ep 414 done. reward: -21.000000. reward running mean: -20.466001\n",
      "Policy Gradient with Baseline ep 415 done. reward: -20.000000. reward running mean: -20.461341\n",
      "Policy Gradient with Baseline ep 416 done. reward: -19.000000. reward running mean: -20.446728\n",
      "Policy Gradient with Baseline ep 417 done. reward: -20.000000. reward running mean: -20.442260\n",
      "Policy Gradient with Baseline ep 418 done. reward: -21.000000. reward running mean: -20.447838\n",
      "Policy Gradient with Baseline ep 419 done. reward: -21.000000. reward running mean: -20.453359\n",
      "Policy Gradient with Baseline ep 420 done. reward: -20.000000. reward running mean: -20.448826\n",
      "Policy Gradient with Baseline ep 421 done. reward: -21.000000. reward running mean: -20.454338\n",
      "Policy Gradient with Baseline ep 422 done. reward: -20.000000. reward running mean: -20.449794\n",
      "Policy Gradient with Baseline ep 423 done. reward: -20.000000. reward running mean: -20.445296\n",
      "Policy Gradient with Baseline ep 424 done. reward: -21.000000. reward running mean: -20.450843\n",
      "Policy Gradient with Baseline ep 425 done. reward: -21.000000. reward running mean: -20.456335\n",
      "Policy Gradient with Baseline ep 426 done. reward: -21.000000. reward running mean: -20.461771\n",
      "Policy Gradient with Baseline ep 427 done. reward: -20.000000. reward running mean: -20.457154\n",
      "Policy Gradient with Baseline ep 428 done. reward: -18.000000. reward running mean: -20.432582\n",
      "Policy Gradient with Baseline ep 429 done. reward: -20.000000. reward running mean: -20.428256\n",
      "Policy Gradient with Baseline ep 430 done. reward: -21.000000. reward running mean: -20.433974\n",
      "Policy Gradient with Baseline ep 431 done. reward: -21.000000. reward running mean: -20.439634\n",
      "Policy Gradient with Baseline ep 432 done. reward: -21.000000. reward running mean: -20.445238\n",
      "Policy Gradient with Baseline ep 433 done. reward: -19.000000. reward running mean: -20.430785\n",
      "Policy Gradient with Baseline ep 434 done. reward: -19.000000. reward running mean: -20.416477\n",
      "Policy Gradient with Baseline ep 435 done. reward: -19.000000. reward running mean: -20.402313\n",
      "Policy Gradient with Baseline ep 436 done. reward: -21.000000. reward running mean: -20.408290\n",
      "Policy Gradient with Baseline ep 437 done. reward: -21.000000. reward running mean: -20.414207\n",
      "Policy Gradient with Baseline ep 438 done. reward: -21.000000. reward running mean: -20.420065\n",
      "Policy Gradient with Baseline ep 439 done. reward: -21.000000. reward running mean: -20.425864\n",
      "Policy Gradient with Baseline ep 440 done. reward: -20.000000. reward running mean: -20.421605\n",
      "Policy Gradient with Baseline ep 441 done. reward: -21.000000. reward running mean: -20.427389\n",
      "Policy Gradient with Baseline ep 442 done. reward: -21.000000. reward running mean: -20.433115\n",
      "Policy Gradient with Baseline ep 443 done. reward: -21.000000. reward running mean: -20.438784\n",
      "Policy Gradient with Baseline ep 444 done. reward: -20.000000. reward running mean: -20.434396\n",
      "Policy Gradient with Baseline ep 445 done. reward: -19.000000. reward running mean: -20.420052\n",
      "Policy Gradient with Baseline ep 446 done. reward: -19.000000. reward running mean: -20.405852\n",
      "Policy Gradient with Baseline ep 447 done. reward: -21.000000. reward running mean: -20.411793\n",
      "Policy Gradient with Baseline ep 448 done. reward: -20.000000. reward running mean: -20.407675\n",
      "Policy Gradient with Baseline ep 449 done. reward: -21.000000. reward running mean: -20.413599\n",
      "Policy Gradient with Baseline ep 450 done. reward: -20.000000. reward running mean: -20.409463\n",
      "ep 450: model saving...\n",
      "Policy Gradient with Baseline ep 451 done. reward: -19.000000. reward running mean: -20.395368\n",
      "Policy Gradient with Baseline ep 452 done. reward: -21.000000. reward running mean: -20.401414\n",
      "Policy Gradient with Baseline ep 453 done. reward: -21.000000. reward running mean: -20.407400\n",
      "Policy Gradient with Baseline ep 454 done. reward: -20.000000. reward running mean: -20.403326\n",
      "Policy Gradient with Baseline ep 455 done. reward: -21.000000. reward running mean: -20.409293\n",
      "Policy Gradient with Baseline ep 456 done. reward: -21.000000. reward running mean: -20.415200\n",
      "Policy Gradient with Baseline ep 457 done. reward: -20.000000. reward running mean: -20.411048\n",
      "Policy Gradient with Baseline ep 458 done. reward: -21.000000. reward running mean: -20.416938\n",
      "Policy Gradient with Baseline ep 459 done. reward: -21.000000. reward running mean: -20.422768\n",
      "Policy Gradient with Baseline ep 460 done. reward: -21.000000. reward running mean: -20.428541\n",
      "Policy Gradient with Baseline ep 461 done. reward: -20.000000. reward running mean: -20.424255\n",
      "Policy Gradient with Baseline ep 462 done. reward: -21.000000. reward running mean: -20.430013\n",
      "Policy Gradient with Baseline ep 463 done. reward: -20.000000. reward running mean: -20.425712\n",
      "Policy Gradient with Baseline ep 464 done. reward: -21.000000. reward running mean: -20.431455\n",
      "Policy Gradient with Baseline ep 465 done. reward: -20.000000. reward running mean: -20.427141\n",
      "Policy Gradient with Baseline ep 466 done. reward: -19.000000. reward running mean: -20.412869\n",
      "Policy Gradient with Baseline ep 467 done. reward: -21.000000. reward running mean: -20.418741\n",
      "Policy Gradient with Baseline ep 468 done. reward: -20.000000. reward running mean: -20.414553\n",
      "Policy Gradient with Baseline ep 469 done. reward: -21.000000. reward running mean: -20.420408\n",
      "Policy Gradient with Baseline ep 470 done. reward: -20.000000. reward running mean: -20.416204\n",
      "Policy Gradient with Baseline ep 471 done. reward: -20.000000. reward running mean: -20.412042\n",
      "Policy Gradient with Baseline ep 472 done. reward: -21.000000. reward running mean: -20.417921\n",
      "Policy Gradient with Baseline ep 473 done. reward: -21.000000. reward running mean: -20.423742\n",
      "Policy Gradient with Baseline ep 474 done. reward: -21.000000. reward running mean: -20.429505\n",
      "Policy Gradient with Baseline ep 475 done. reward: -21.000000. reward running mean: -20.435210\n",
      "Policy Gradient with Baseline ep 476 done. reward: -19.000000. reward running mean: -20.420857\n",
      "Policy Gradient with Baseline ep 477 done. reward: -21.000000. reward running mean: -20.426649\n",
      "Policy Gradient with Baseline ep 478 done. reward: -21.000000. reward running mean: -20.432382\n",
      "Policy Gradient with Baseline ep 479 done. reward: -21.000000. reward running mean: -20.438059\n",
      "Policy Gradient with Baseline ep 480 done. reward: -21.000000. reward running mean: -20.443678\n",
      "Policy Gradient with Baseline ep 481 done. reward: -20.000000. reward running mean: -20.439241\n",
      "Policy Gradient with Baseline ep 482 done. reward: -20.000000. reward running mean: -20.434849\n",
      "Policy Gradient with Baseline ep 483 done. reward: -21.000000. reward running mean: -20.440500\n",
      "Policy Gradient with Baseline ep 484 done. reward: -18.000000. reward running mean: -20.416095\n",
      "Policy Gradient with Baseline ep 485 done. reward: -21.000000. reward running mean: -20.421934\n",
      "Policy Gradient with Baseline ep 486 done. reward: -20.000000. reward running mean: -20.417715\n",
      "Policy Gradient with Baseline ep 487 done. reward: -20.000000. reward running mean: -20.413538\n",
      "Policy Gradient with Baseline ep 488 done. reward: -20.000000. reward running mean: -20.409402\n",
      "Policy Gradient with Baseline ep 489 done. reward: -20.000000. reward running mean: -20.405308\n",
      "Policy Gradient with Baseline ep 490 done. reward: -21.000000. reward running mean: -20.411255\n",
      "Policy Gradient with Baseline ep 491 done. reward: -21.000000. reward running mean: -20.417143\n",
      "Policy Gradient with Baseline ep 492 done. reward: -21.000000. reward running mean: -20.422971\n",
      "Policy Gradient with Baseline ep 493 done. reward: -21.000000. reward running mean: -20.428742\n",
      "Policy Gradient with Baseline ep 494 done. reward: -21.000000. reward running mean: -20.434454\n",
      "Policy Gradient with Baseline ep 495 done. reward: -21.000000. reward running mean: -20.440110\n",
      "Policy Gradient with Baseline ep 496 done. reward: -20.000000. reward running mean: -20.435709\n",
      "Policy Gradient with Baseline ep 497 done. reward: -21.000000. reward running mean: -20.441352\n",
      "Policy Gradient with Baseline ep 498 done. reward: -21.000000. reward running mean: -20.446938\n",
      "Policy Gradient with Baseline ep 499 done. reward: -20.000000. reward running mean: -20.442469\n",
      "Policy Gradient with Baseline ep 500 done. reward: -20.000000. reward running mean: -20.438044\n",
      "ep 500: model saving...\n",
      "Policy Gradient with Baseline ep 501 done. reward: -21.000000. reward running mean: -20.443663\n",
      "Policy Gradient with Baseline ep 502 done. reward: -21.000000. reward running mean: -20.449227\n",
      "Policy Gradient with Baseline ep 503 done. reward: -21.000000. reward running mean: -20.454735\n",
      "Policy Gradient with Baseline ep 504 done. reward: -21.000000. reward running mean: -20.460187\n",
      "Policy Gradient with Baseline ep 505 done. reward: -21.000000. reward running mean: -20.465585\n",
      "Policy Gradient with Baseline ep 506 done. reward: -20.000000. reward running mean: -20.460930\n",
      "Policy Gradient with Baseline ep 507 done. reward: -21.000000. reward running mean: -20.466320\n",
      "Policy Gradient with Baseline ep 508 done. reward: -21.000000. reward running mean: -20.471657\n",
      "Policy Gradient with Baseline ep 509 done. reward: -20.000000. reward running mean: -20.466940\n",
      "Policy Gradient with Baseline ep 510 done. reward: -21.000000. reward running mean: -20.472271\n",
      "Policy Gradient with Baseline ep 511 done. reward: -21.000000. reward running mean: -20.477548\n",
      "Policy Gradient with Baseline ep 512 done. reward: -20.000000. reward running mean: -20.472773\n",
      "Policy Gradient with Baseline ep 513 done. reward: -21.000000. reward running mean: -20.478045\n",
      "Policy Gradient with Baseline ep 514 done. reward: -21.000000. reward running mean: -20.483265\n",
      "Policy Gradient with Baseline ep 515 done. reward: -21.000000. reward running mean: -20.488432\n",
      "Policy Gradient with Baseline ep 516 done. reward: -21.000000. reward running mean: -20.493548\n",
      "Policy Gradient with Baseline ep 517 done. reward: -20.000000. reward running mean: -20.488612\n",
      "Policy Gradient with Baseline ep 518 done. reward: -21.000000. reward running mean: -20.493726\n",
      "Policy Gradient with Baseline ep 519 done. reward: -20.000000. reward running mean: -20.488789\n",
      "Policy Gradient with Baseline ep 520 done. reward: -21.000000. reward running mean: -20.493901\n",
      "Policy Gradient with Baseline ep 521 done. reward: -21.000000. reward running mean: -20.498962\n",
      "Policy Gradient with Baseline ep 522 done. reward: -21.000000. reward running mean: -20.503972\n",
      "Policy Gradient with Baseline ep 523 done. reward: -19.000000. reward running mean: -20.488933\n",
      "Policy Gradient with Baseline ep 524 done. reward: -21.000000. reward running mean: -20.494043\n",
      "Policy Gradient with Baseline ep 525 done. reward: -21.000000. reward running mean: -20.499103\n",
      "Policy Gradient with Baseline ep 526 done. reward: -21.000000. reward running mean: -20.504112\n",
      "Policy Gradient with Baseline ep 527 done. reward: -20.000000. reward running mean: -20.499071\n",
      "Policy Gradient with Baseline ep 528 done. reward: -20.000000. reward running mean: -20.494080\n",
      "Policy Gradient with Baseline ep 529 done. reward: -21.000000. reward running mean: -20.499139\n",
      "Policy Gradient with Baseline ep 530 done. reward: -21.000000. reward running mean: -20.504148\n",
      "Policy Gradient with Baseline ep 531 done. reward: -21.000000. reward running mean: -20.509106\n",
      "Policy Gradient with Baseline ep 532 done. reward: -21.000000. reward running mean: -20.514015\n",
      "Policy Gradient with Baseline ep 533 done. reward: -21.000000. reward running mean: -20.518875\n",
      "Policy Gradient with Baseline ep 534 done. reward: -21.000000. reward running mean: -20.523686\n",
      "Policy Gradient with Baseline ep 535 done. reward: -21.000000. reward running mean: -20.528449\n",
      "Policy Gradient with Baseline ep 536 done. reward: -21.000000. reward running mean: -20.533165\n",
      "Policy Gradient with Baseline ep 537 done. reward: -20.000000. reward running mean: -20.527833\n",
      "Policy Gradient with Baseline ep 538 done. reward: -20.000000. reward running mean: -20.522555\n",
      "Policy Gradient with Baseline ep 539 done. reward: -19.000000. reward running mean: -20.507329\n",
      "Policy Gradient with Baseline ep 540 done. reward: -20.000000. reward running mean: -20.502256\n",
      "Policy Gradient with Baseline ep 541 done. reward: -21.000000. reward running mean: -20.507234\n",
      "Policy Gradient with Baseline ep 542 done. reward: -21.000000. reward running mean: -20.512161\n",
      "Policy Gradient with Baseline ep 543 done. reward: -21.000000. reward running mean: -20.517040\n",
      "Policy Gradient with Baseline ep 544 done. reward: -21.000000. reward running mean: -20.521869\n",
      "Policy Gradient with Baseline ep 545 done. reward: -20.000000. reward running mean: -20.516651\n",
      "Policy Gradient with Baseline ep 546 done. reward: -20.000000. reward running mean: -20.511484\n",
      "Policy Gradient with Baseline ep 547 done. reward: -21.000000. reward running mean: -20.516369\n",
      "Policy Gradient with Baseline ep 548 done. reward: -19.000000. reward running mean: -20.501206\n",
      "Policy Gradient with Baseline ep 549 done. reward: -21.000000. reward running mean: -20.506193\n",
      "Policy Gradient with Baseline ep 550 done. reward: -20.000000. reward running mean: -20.501132\n",
      "ep 550: model saving...\n",
      "Policy Gradient with Baseline ep 551 done. reward: -21.000000. reward running mean: -20.506120\n",
      "Policy Gradient with Baseline ep 552 done. reward: -21.000000. reward running mean: -20.511059\n",
      "Policy Gradient with Baseline ep 553 done. reward: -18.000000. reward running mean: -20.485948\n",
      "Policy Gradient with Baseline ep 554 done. reward: -21.000000. reward running mean: -20.491089\n",
      "Policy Gradient with Baseline ep 555 done. reward: -21.000000. reward running mean: -20.496178\n",
      "Policy Gradient with Baseline ep 556 done. reward: -21.000000. reward running mean: -20.501216\n",
      "Policy Gradient with Baseline ep 557 done. reward: -19.000000. reward running mean: -20.486204\n",
      "Policy Gradient with Baseline ep 558 done. reward: -20.000000. reward running mean: -20.481342\n",
      "Policy Gradient with Baseline ep 559 done. reward: -20.000000. reward running mean: -20.476529\n",
      "Policy Gradient with Baseline ep 560 done. reward: -21.000000. reward running mean: -20.481763\n",
      "Policy Gradient with Baseline ep 561 done. reward: -21.000000. reward running mean: -20.486946\n",
      "Policy Gradient with Baseline ep 562 done. reward: -21.000000. reward running mean: -20.492076\n",
      "Policy Gradient with Baseline ep 563 done. reward: -21.000000. reward running mean: -20.497156\n",
      "Policy Gradient with Baseline ep 564 done. reward: -20.000000. reward running mean: -20.492184\n",
      "Policy Gradient with Baseline ep 565 done. reward: -21.000000. reward running mean: -20.497262\n",
      "Policy Gradient with Baseline ep 566 done. reward: -21.000000. reward running mean: -20.502289\n",
      "Policy Gradient with Baseline ep 567 done. reward: -21.000000. reward running mean: -20.507267\n",
      "Policy Gradient with Baseline ep 568 done. reward: -21.000000. reward running mean: -20.512194\n",
      "Policy Gradient with Baseline ep 569 done. reward: -21.000000. reward running mean: -20.517072\n",
      "Policy Gradient with Baseline ep 570 done. reward: -20.000000. reward running mean: -20.511901\n",
      "Policy Gradient with Baseline ep 571 done. reward: -21.000000. reward running mean: -20.516782\n",
      "Policy Gradient with Baseline ep 572 done. reward: -18.000000. reward running mean: -20.491614\n",
      "Policy Gradient with Baseline ep 573 done. reward: -20.000000. reward running mean: -20.486698\n",
      "Policy Gradient with Baseline ep 574 done. reward: -20.000000. reward running mean: -20.481831\n",
      "Policy Gradient with Baseline ep 575 done. reward: -20.000000. reward running mean: -20.477013\n",
      "Policy Gradient with Baseline ep 576 done. reward: -19.000000. reward running mean: -20.462243\n",
      "Policy Gradient with Baseline ep 577 done. reward: -20.000000. reward running mean: -20.457620\n",
      "Policy Gradient with Baseline ep 578 done. reward: -20.000000. reward running mean: -20.453044\n",
      "Policy Gradient with Baseline ep 579 done. reward: -21.000000. reward running mean: -20.458514\n",
      "Policy Gradient with Baseline ep 580 done. reward: -21.000000. reward running mean: -20.463929\n",
      "Policy Gradient with Baseline ep 581 done. reward: -19.000000. reward running mean: -20.449289\n",
      "Policy Gradient with Baseline ep 582 done. reward: -19.000000. reward running mean: -20.434796\n",
      "Policy Gradient with Baseline ep 583 done. reward: -18.000000. reward running mean: -20.410449\n",
      "Policy Gradient with Baseline ep 584 done. reward: -20.000000. reward running mean: -20.406344\n",
      "Policy Gradient with Baseline ep 585 done. reward: -21.000000. reward running mean: -20.412281\n",
      "Policy Gradient with Baseline ep 586 done. reward: -21.000000. reward running mean: -20.418158\n",
      "Policy Gradient with Baseline ep 587 done. reward: -21.000000. reward running mean: -20.423976\n",
      "Policy Gradient with Baseline ep 588 done. reward: -20.000000. reward running mean: -20.419736\n",
      "Policy Gradient with Baseline ep 589 done. reward: -20.000000. reward running mean: -20.415539\n",
      "Policy Gradient with Baseline ep 590 done. reward: -21.000000. reward running mean: -20.421384\n",
      "Policy Gradient with Baseline ep 591 done. reward: -21.000000. reward running mean: -20.427170\n",
      "Policy Gradient with Baseline ep 592 done. reward: -19.000000. reward running mean: -20.412898\n",
      "Policy Gradient with Baseline ep 593 done. reward: -21.000000. reward running mean: -20.418769\n",
      "Policy Gradient with Baseline ep 594 done. reward: -19.000000. reward running mean: -20.404581\n",
      "Policy Gradient with Baseline ep 595 done. reward: -19.000000. reward running mean: -20.390536\n",
      "Policy Gradient with Baseline ep 596 done. reward: -21.000000. reward running mean: -20.396630\n",
      "Policy Gradient with Baseline ep 597 done. reward: -21.000000. reward running mean: -20.402664\n",
      "Policy Gradient with Baseline ep 598 done. reward: -20.000000. reward running mean: -20.398637\n",
      "Policy Gradient with Baseline ep 599 done. reward: -20.000000. reward running mean: -20.394651\n",
      "Policy Gradient with Baseline ep 600 done. reward: -21.000000. reward running mean: -20.400704\n",
      "ep 600: model saving...\n",
      "Policy Gradient with Baseline ep 601 done. reward: -21.000000. reward running mean: -20.406697\n",
      "Policy Gradient with Baseline ep 602 done. reward: -20.000000. reward running mean: -20.402630\n",
      "Policy Gradient with Baseline ep 603 done. reward: -21.000000. reward running mean: -20.408604\n",
      "Policy Gradient with Baseline ep 604 done. reward: -20.000000. reward running mean: -20.404518\n",
      "Policy Gradient with Baseline ep 605 done. reward: -19.000000. reward running mean: -20.390473\n",
      "Policy Gradient with Baseline ep 606 done. reward: -21.000000. reward running mean: -20.396568\n",
      "Policy Gradient with Baseline ep 607 done. reward: -20.000000. reward running mean: -20.392603\n",
      "Policy Gradient with Baseline ep 608 done. reward: -21.000000. reward running mean: -20.398676\n",
      "Policy Gradient with Baseline ep 609 done. reward: -21.000000. reward running mean: -20.404690\n",
      "Policy Gradient with Baseline ep 610 done. reward: -19.000000. reward running mean: -20.390643\n",
      "Policy Gradient with Baseline ep 611 done. reward: -21.000000. reward running mean: -20.396736\n",
      "Policy Gradient with Baseline ep 612 done. reward: -21.000000. reward running mean: -20.402769\n",
      "Policy Gradient with Baseline ep 613 done. reward: -20.000000. reward running mean: -20.398741\n",
      "Policy Gradient with Baseline ep 614 done. reward: -21.000000. reward running mean: -20.404754\n",
      "Policy Gradient with Baseline ep 615 done. reward: -21.000000. reward running mean: -20.410706\n",
      "Policy Gradient with Baseline ep 616 done. reward: -21.000000. reward running mean: -20.416599\n",
      "Policy Gradient with Baseline ep 617 done. reward: -18.000000. reward running mean: -20.392433\n",
      "Policy Gradient with Baseline ep 618 done. reward: -21.000000. reward running mean: -20.398509\n",
      "Policy Gradient with Baseline ep 619 done. reward: -21.000000. reward running mean: -20.404524\n",
      "Policy Gradient with Baseline ep 620 done. reward: -21.000000. reward running mean: -20.410479\n",
      "Policy Gradient with Baseline ep 621 done. reward: -21.000000. reward running mean: -20.416374\n",
      "Policy Gradient with Baseline ep 622 done. reward: -21.000000. reward running mean: -20.422210\n",
      "Policy Gradient with Baseline ep 623 done. reward: -21.000000. reward running mean: -20.427988\n",
      "Policy Gradient with Baseline ep 624 done. reward: -20.000000. reward running mean: -20.423708\n",
      "Policy Gradient with Baseline ep 625 done. reward: -21.000000. reward running mean: -20.429471\n",
      "Policy Gradient with Baseline ep 626 done. reward: -20.000000. reward running mean: -20.425176\n",
      "Policy Gradient with Baseline ep 627 done. reward: -21.000000. reward running mean: -20.430925\n",
      "Policy Gradient with Baseline ep 628 done. reward: -19.000000. reward running mean: -20.416615\n",
      "Policy Gradient with Baseline ep 629 done. reward: -21.000000. reward running mean: -20.422449\n",
      "Policy Gradient with Baseline ep 630 done. reward: -21.000000. reward running mean: -20.428225\n",
      "Policy Gradient with Baseline ep 631 done. reward: -21.000000. reward running mean: -20.433942\n",
      "Policy Gradient with Baseline ep 632 done. reward: -21.000000. reward running mean: -20.439603\n",
      "Policy Gradient with Baseline ep 633 done. reward: -19.000000. reward running mean: -20.425207\n",
      "Policy Gradient with Baseline ep 634 done. reward: -21.000000. reward running mean: -20.430955\n",
      "Policy Gradient with Baseline ep 635 done. reward: -19.000000. reward running mean: -20.416645\n",
      "Policy Gradient with Baseline ep 636 done. reward: -19.000000. reward running mean: -20.402479\n",
      "Policy Gradient with Baseline ep 637 done. reward: -21.000000. reward running mean: -20.408454\n",
      "Policy Gradient with Baseline ep 638 done. reward: -21.000000. reward running mean: -20.414370\n",
      "Policy Gradient with Baseline ep 639 done. reward: -21.000000. reward running mean: -20.420226\n",
      "Policy Gradient with Baseline ep 640 done. reward: -21.000000. reward running mean: -20.426024\n"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "for i_episode in count(1):\n",
    "    state = env.reset()\n",
    "    prev_x = None\n",
    "    for t in range(10000):\n",
    "        if render: env.render()\n",
    "        cur_x = prepro(state)\n",
    "        x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "        prev_x = cur_x\n",
    "        action = policy.select_action(x)\n",
    "        action_env = action + 2\n",
    "        state, reward, done, _ = env.step(action_env)\n",
    "        reward_sum += reward\n",
    "\n",
    "        policy.rewards.append(reward)\n",
    "        if done:\n",
    "            # tracking log\n",
    "            running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "            print('Policy Gradient with Baseline ep %03d done. reward: %f. reward running mean: %f' % (i_episode, reward_sum, running_reward))\n",
    "            reward_sum = 0\n",
    "            break\n",
    "\n",
    "\n",
    "    # use policy gradient update model weights\n",
    "    if i_episode % args[\"batch_size\"] == 0 and test == False:\n",
    "        finish_episode()\n",
    "\n",
    "    # Save model in every 50 episode\n",
    "    if i_episode % 50 == 0 and test == False:\n",
    "        print('ep %d: model saving...' % (i_episode))\n",
    "        torch.save(policy.state_dict(), 'pgb_params.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
