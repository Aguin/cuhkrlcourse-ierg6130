{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"gamma\": 0.99,\n",
    "    \"seed\": 543,\n",
    "    \"render\": False,\n",
    "    \"log_interval\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f76a73634b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(args[\"seed\"])\n",
    "torch.manual_seed(args[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + args[\"gamma\"] * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tLast reward: 26.00\tAverage reward: 16.00\n",
      "Episode 20\tLast reward: 16.00\tAverage reward: 14.85\n",
      "Episode 30\tLast reward: 49.00\tAverage reward: 20.77\n",
      "Episode 40\tLast reward: 45.00\tAverage reward: 27.37\n",
      "Episode 50\tLast reward: 44.00\tAverage reward: 30.80\n",
      "Episode 60\tLast reward: 111.00\tAverage reward: 42.69\n",
      "Episode 70\tLast reward: 131.00\tAverage reward: 70.39\n",
      "Episode 80\tLast reward: 87.00\tAverage reward: 76.68\n",
      "Episode 90\tLast reward: 97.00\tAverage reward: 96.58\n",
      "Episode 100\tLast reward: 87.00\tAverage reward: 95.25\n",
      "Episode 110\tLast reward: 355.00\tAverage reward: 111.75\n",
      "Episode 120\tLast reward: 110.00\tAverage reward: 120.82\n",
      "Episode 130\tLast reward: 106.00\tAverage reward: 140.40\n",
      "Episode 140\tLast reward: 158.00\tAverage reward: 139.32\n",
      "Episode 150\tLast reward: 65.00\tAverage reward: 149.50\n",
      "Episode 160\tLast reward: 116.00\tAverage reward: 150.58\n",
      "Episode 170\tLast reward: 207.00\tAverage reward: 191.71\n",
      "Episode 180\tLast reward: 93.00\tAverage reward: 171.79\n",
      "Episode 190\tLast reward: 33.00\tAverage reward: 122.50\n",
      "Episode 200\tLast reward: 106.00\tAverage reward: 102.34\n",
      "Episode 210\tLast reward: 123.00\tAverage reward: 112.04\n",
      "Episode 220\tLast reward: 120.00\tAverage reward: 123.19\n",
      "Episode 230\tLast reward: 95.00\tAverage reward: 121.20\n",
      "Episode 240\tLast reward: 125.00\tAverage reward: 117.17\n",
      "Episode 250\tLast reward: 352.00\tAverage reward: 163.49\n",
      "Episode 260\tLast reward: 102.00\tAverage reward: 148.51\n",
      "Episode 270\tLast reward: 86.00\tAverage reward: 136.66\n",
      "Episode 280\tLast reward: 108.00\tAverage reward: 121.70\n",
      "Episode 290\tLast reward: 78.00\tAverage reward: 104.88\n",
      "Episode 300\tLast reward: 90.00\tAverage reward: 97.88\n",
      "Episode 310\tLast reward: 142.00\tAverage reward: 104.91\n",
      "Episode 320\tLast reward: 130.00\tAverage reward: 126.44\n",
      "Episode 330\tLast reward: 203.00\tAverage reward: 130.95\n",
      "Episode 340\tLast reward: 263.00\tAverage reward: 204.25\n",
      "Episode 350\tLast reward: 500.00\tAverage reward: 277.07\n",
      "Episode 360\tLast reward: 243.00\tAverage reward: 291.12\n",
      "Episode 370\tLast reward: 149.00\tAverage reward: 252.35\n",
      "Episode 380\tLast reward: 89.00\tAverage reward: 198.71\n",
      "Episode 390\tLast reward: 99.00\tAverage reward: 159.79\n",
      "Episode 400\tLast reward: 18.00\tAverage reward: 122.33\n",
      "Episode 410\tLast reward: 69.00\tAverage reward: 103.65\n",
      "Episode 420\tLast reward: 65.00\tAverage reward: 82.33\n",
      "Episode 430\tLast reward: 17.00\tAverage reward: 61.30\n",
      "Episode 440\tLast reward: 30.00\tAverage reward: 53.72\n",
      "Episode 450\tLast reward: 134.00\tAverage reward: 64.62\n",
      "Episode 460\tLast reward: 147.00\tAverage reward: 95.19\n",
      "Episode 470\tLast reward: 500.00\tAverage reward: 217.14\n",
      "Episode 480\tLast reward: 500.00\tAverage reward: 330.64\n",
      "Episode 490\tLast reward: 319.00\tAverage reward: 356.19\n",
      "Episode 500\tLast reward: 486.00\tAverage reward: 401.99\n",
      "Episode 510\tLast reward: 500.00\tAverage reward: 389.01\n",
      "Episode 520\tLast reward: 269.00\tAverage reward: 379.67\n",
      "Episode 530\tLast reward: 189.00\tAverage reward: 317.59\n",
      "Episode 540\tLast reward: 214.00\tAverage reward: 275.04\n",
      "Episode 550\tLast reward: 146.00\tAverage reward: 223.14\n",
      "Episode 560\tLast reward: 117.00\tAverage reward: 183.22\n",
      "Episode 570\tLast reward: 189.00\tAverage reward: 175.98\n",
      "Episode 580\tLast reward: 22.00\tAverage reward: 151.61\n",
      "Episode 590\tLast reward: 42.00\tAverage reward: 126.23\n",
      "Episode 600\tLast reward: 23.00\tAverage reward: 102.88\n",
      "Episode 610\tLast reward: 35.00\tAverage reward: 86.76\n",
      "Episode 620\tLast reward: 107.00\tAverage reward: 95.30\n",
      "Episode 630\tLast reward: 120.00\tAverage reward: 105.80\n",
      "Episode 640\tLast reward: 39.00\tAverage reward: 103.91\n",
      "Episode 650\tLast reward: 127.00\tAverage reward: 110.98\n",
      "Episode 660\tLast reward: 135.00\tAverage reward: 123.42\n",
      "Episode 670\tLast reward: 177.00\tAverage reward: 136.48\n",
      "Episode 680\tLast reward: 121.00\tAverage reward: 140.42\n",
      "Episode 690\tLast reward: 122.00\tAverage reward: 134.85\n",
      "Episode 700\tLast reward: 84.00\tAverage reward: 124.07\n",
      "Episode 710\tLast reward: 132.00\tAverage reward: 108.78\n",
      "Episode 720\tLast reward: 121.00\tAverage reward: 105.63\n",
      "Episode 730\tLast reward: 54.00\tAverage reward: 104.46\n",
      "Episode 740\tLast reward: 99.00\tAverage reward: 102.80\n",
      "Episode 750\tLast reward: 118.00\tAverage reward: 100.05\n",
      "Episode 760\tLast reward: 129.00\tAverage reward: 109.76\n",
      "Episode 770\tLast reward: 125.00\tAverage reward: 110.14\n",
      "Episode 780\tLast reward: 124.00\tAverage reward: 117.97\n",
      "Episode 790\tLast reward: 154.00\tAverage reward: 129.36\n",
      "Episode 800\tLast reward: 127.00\tAverage reward: 134.33\n",
      "Episode 810\tLast reward: 139.00\tAverage reward: 137.57\n",
      "Episode 820\tLast reward: 141.00\tAverage reward: 138.75\n",
      "Episode 830\tLast reward: 144.00\tAverage reward: 142.53\n",
      "Episode 840\tLast reward: 194.00\tAverage reward: 156.10\n",
      "Episode 850\tLast reward: 224.00\tAverage reward: 222.89\n",
      "Episode 860\tLast reward: 500.00\tAverage reward: 334.08\n",
      "Episode 870\tLast reward: 500.00\tAverage reward: 400.66\n",
      "Episode 880\tLast reward: 500.00\tAverage reward: 440.52\n",
      "Episode 890\tLast reward: 500.00\tAverage reward: 464.39\n",
      "Solved! Running reward is now 475.33293447664164 and the last episode runs to 500 time steps!\n"
     ]
    }
   ],
   "source": [
    "running_reward = 10\n",
    "for i_episode in count(1):\n",
    "    state, ep_reward = env.reset(), 0\n",
    "    for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "        action = select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if args[\"render\"]:\n",
    "            env.render()\n",
    "        policy.rewards.append(reward)\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    finish_episode()\n",
    "    if i_episode % args[\"log_interval\"] == 0:\n",
    "        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "              i_episode, ep_reward, running_reward))\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
